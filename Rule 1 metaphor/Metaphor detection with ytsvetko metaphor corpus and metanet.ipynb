{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at https://github.com/ytsvetko/metaphor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##all the imports...\n",
    "#%pylab inline\n",
    "import re\n",
    "import math\n",
    "import string\n",
    "import nltk\n",
    "import pprint\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "sent_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "from collections import Counter\n",
    "from __future__ import division\n",
    "from nltk.collocations import *\n",
    "import string, random\n",
    "from nltk.corpus import brown\n",
    "from nltk.collocations import *\n",
    "from string import punctuation\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk import bigrams\n",
    "from nltk import collocations\n",
    "from nltk import trigrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start by loading the .csv files and setting the columns.\n",
    "### For these .csv files, downloaded from https://github.com/ytsvetko/metaphor inputs and saved as .csv with the first row added as \"sample\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>angry welt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bald assertion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bare outline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>black humor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blind alley</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           sample\n",
       "0      angry welt\n",
       "1  bald assertion\n",
       "2    bare outline\n",
       "3     black humor\n",
       "4     blind alley"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_anmet = pd.read_csv(\"an_mets.csv\", low_memory=False)\n",
    "df_anmet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_anmet = pd.read_csv(\"an_mets.csv\", low_memory=False)\n",
    "df_anmet['metaphor'] = 1\n",
    "df_anmet['an'] = 1\n",
    "df_anmet['svo'] = 0\n",
    "df_anmet['metanet'] = 0\n",
    "df_anmet.head()\n",
    "len(df_anmet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_annonmet = pd.read_csv(\"an_nonmets.csv\", low_memory=False)\n",
    "df_annonmet['metaphor'] = 0\n",
    "df_annonmet['an'] = 1\n",
    "df_annonmet['svo'] = 0\n",
    "df_annonmet['metanet'] = 0\n",
    "df_annonmet.head()\n",
    "len(df_annonmet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_svomet = pd.read_csv(\"svo_mets.csv\", low_memory=False)\n",
    "df_svomet['metaphor'] = 1\n",
    "df_svomet['an'] = 0\n",
    "df_svomet['svo'] = 1\n",
    "df_svomet['metanet'] = 0\n",
    "df_svomet.head()\n",
    "len(df_svomet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_svononmet = pd.read_csv(\"svo_nonmets.csv\", low_memory=False)\n",
    "df_svononmet['metaphor'] = 0\n",
    "df_svononmet['an'] = 0\n",
    "df_svononmet['svo'] = 1\n",
    "df_svononmet['metanet'] = 0\n",
    "df_svononmet.head()\n",
    "len(df_svononmet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>metaphor</th>\n",
       "      <th>an</th>\n",
       "      <th>svo</th>\n",
       "      <th>metanet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ability to evaluate government is ability to see</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ability to evaluate is ability to see</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ability to know is ability to see</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abusive political leaders are physical bullies</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>accepting is swallowing</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sample  metaphor  an  svo  \\\n",
       "0  ability to evaluate government is ability to see         1   0    0   \n",
       "1             ability to evaluate is ability to see         1   0    0   \n",
       "2                 ability to know is ability to see         1   0    0   \n",
       "3    abusive political leaders are physical bullies         1   0    0   \n",
       "4                           accepting is swallowing         1   0    0   \n",
       "\n",
       "   metanet  \n",
       "0        1  \n",
       "1        1  \n",
       "2        1  \n",
       "3        1  \n",
       "4        1  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metanet = pd.read_csv(\"metanet.csv\", low_memory=False)\n",
    "df_metanet['metaphor'] = 1\n",
    "df_metanet['an'] = 0\n",
    "df_metanet['svo'] = 0\n",
    "df_metanet['metanet'] = 1\n",
    "df_metanet.head()\n",
    "# len(df_metanet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>metaphor</th>\n",
       "      <th>an</th>\n",
       "      <th>svo</th>\n",
       "      <th>metanet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>conversation turn subject</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>resumption bring relief</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>economy move direction</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>service meet expectation</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>material live dream</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>unemployment stand *none</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>action talk *none</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>statement sit *none</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>income fall *none</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>silence speak volume</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>car decide *none</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>battery die *none</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>accident wait *none</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>Texan break record</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>temperature break *number</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>insurance cover care</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>state cut spending</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>Hawaii kill  proposal</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>electronics drive innovation</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>teenager wear attitude</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>*pronoun catch flight</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>envy eat *none</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>data point pain</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>advertiser pull ad</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>*pronoun close deal</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>police close investigation</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>bicycle suffer damage</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>Tori throw tantrum</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>fortune smile *pronoun</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>excitement fill street</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>analysis of social problems is diagnosis of af...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>analyzing is dissecting</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>anger is fire</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>anger is heat</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>anger is insanity</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>anger is pressure in a container</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>anger is the heat of fluid in a container</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>argument is physical combat</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>argument is war</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>arithmetic is object construction</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>assessing is measuring</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>assistance is support</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>attaining control is gaining a possession</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>attributes are entities</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>attributes are possessions</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>attributes of government are entities</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>augmenting economic assets is creating objects</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>bad is stinky</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>becoming impoverished is moving downwards</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>being alive is being physically at this location</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>being good is being upright</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>being immoral is being low</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>being impoverished is being at a low location</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>being impoverished is being in a bounded region</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>being in a high social class is being high on ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>being in a low social class is being low on a ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>being in a middle class is being in the middle...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>being in a state is being at a point on a line...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>being poised to know is being positioned to se...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>being wealthy is being at a high location</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sample  metaphor  an  svo  \\\n",
       "200                          conversation turn subject         1   0    1   \n",
       "201                            resumption bring relief         1   0    1   \n",
       "202                             economy move direction         1   0    1   \n",
       "203                           service meet expectation         1   0    1   \n",
       "204                                material live dream         1   0    1   \n",
       "205                           unemployment stand *none         1   0    1   \n",
       "206                                  action talk *none         1   0    1   \n",
       "207                                statement sit *none         1   0    1   \n",
       "208                                  income fall *none         1   0    1   \n",
       "209                               silence speak volume         1   0    1   \n",
       "210                                   car decide *none         1   0    1   \n",
       "211                                  battery die *none         1   0    1   \n",
       "212                                accident wait *none         1   0    1   \n",
       "213                                 Texan break record         1   0    1   \n",
       "214                          temperature break *number         1   0    1   \n",
       "215                               insurance cover care         1   0    1   \n",
       "216                                 state cut spending         1   0    1   \n",
       "217                              Hawaii kill  proposal         1   0    1   \n",
       "218                       electronics drive innovation         1   0    1   \n",
       "219                             teenager wear attitude         1   0    1   \n",
       "220                              *pronoun catch flight         1   0    1   \n",
       "221                                     envy eat *none         1   0    1   \n",
       "222                                    data point pain         1   0    1   \n",
       "223                                 advertiser pull ad         1   0    1   \n",
       "224                                *pronoun close deal         1   0    1   \n",
       "225                         police close investigation         1   0    1   \n",
       "226                              bicycle suffer damage         1   0    1   \n",
       "227                                 Tori throw tantrum         1   0    1   \n",
       "228                             fortune smile *pronoun         1   0    1   \n",
       "229                             excitement fill street         1   0    1   \n",
       "..                                                 ...       ...  ..  ...   \n",
       "470  analysis of social problems is diagnosis of af...         1   0    0   \n",
       "471                            analyzing is dissecting         1   0    0   \n",
       "472                                      anger is fire         1   0    0   \n",
       "473                                      anger is heat         1   0    0   \n",
       "474                                  anger is insanity         1   0    0   \n",
       "475                   anger is pressure in a container         1   0    0   \n",
       "476          anger is the heat of fluid in a container         1   0    0   \n",
       "477                        argument is physical combat         1   0    0   \n",
       "478                                    argument is war         1   0    0   \n",
       "479                  arithmetic is object construction         1   0    0   \n",
       "480                             assessing is measuring         1   0    0   \n",
       "481                              assistance is support         1   0    0   \n",
       "482          attaining control is gaining a possession         1   0    0   \n",
       "483                            attributes are entities         1   0    0   \n",
       "484                         attributes are possessions         1   0    0   \n",
       "485              attributes of government are entities         1   0    0   \n",
       "486     augmenting economic assets is creating objects         1   0    0   \n",
       "487                                      bad is stinky         1   0    0   \n",
       "488          becoming impoverished is moving downwards         1   0    0   \n",
       "489   being alive is being physically at this location         1   0    0   \n",
       "490                        being good is being upright         1   0    0   \n",
       "491                         being immoral is being low         1   0    0   \n",
       "492      being impoverished is being at a low location         1   0    0   \n",
       "493    being impoverished is being in a bounded region         1   0    0   \n",
       "494  being in a high social class is being high on ...         1   0    0   \n",
       "495  being in a low social class is being low on a ...         1   0    0   \n",
       "496  being in a middle class is being in the middle...         1   0    0   \n",
       "497  being in a state is being at a point on a line...         1   0    0   \n",
       "498  being poised to know is being positioned to se...         1   0    0   \n",
       "499          being wealthy is being at a high location         1   0    0   \n",
       "\n",
       "     metanet  \n",
       "200        0  \n",
       "201        0  \n",
       "202        0  \n",
       "203        0  \n",
       "204        0  \n",
       "205        0  \n",
       "206        0  \n",
       "207        0  \n",
       "208        0  \n",
       "209        0  \n",
       "210        0  \n",
       "211        0  \n",
       "212        0  \n",
       "213        0  \n",
       "214        0  \n",
       "215        0  \n",
       "216        0  \n",
       "217        0  \n",
       "218        0  \n",
       "219        0  \n",
       "220        0  \n",
       "221        0  \n",
       "222        0  \n",
       "223        0  \n",
       "224        0  \n",
       "225        0  \n",
       "226        0  \n",
       "227        0  \n",
       "228        0  \n",
       "229        0  \n",
       "..       ...  \n",
       "470        1  \n",
       "471        1  \n",
       "472        1  \n",
       "473        1  \n",
       "474        1  \n",
       "475        1  \n",
       "476        1  \n",
       "477        1  \n",
       "478        1  \n",
       "479        1  \n",
       "480        1  \n",
       "481        1  \n",
       "482        1  \n",
       "483        1  \n",
       "484        1  \n",
       "485        1  \n",
       "486        1  \n",
       "487        1  \n",
       "488        1  \n",
       "489        1  \n",
       "490        1  \n",
       "491        1  \n",
       "492        1  \n",
       "493        1  \n",
       "494        1  \n",
       "495        1  \n",
       "496        1  \n",
       "497        1  \n",
       "498        1  \n",
       "499        1  \n",
       "\n",
       "[300 rows x 5 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#combine into one df\n",
    "frames = [df_anmet, df_annonmet, df_svomet, df_svononmet, df_metanet]\n",
    "df_combo = pd.concat(frames)\n",
    "df_combo.reset_index(drop=True, inplace=True)\n",
    "df_combo.shape\n",
    "# df_combo.replace(to_replace='none', value=\"\")\n",
    "# #         tokenizer = RegexpTokenizer(r'\\w+')\n",
    "# #         word_list = tokenizer.tokenize(line)\n",
    "# #         filtered_words = [word for word in word_list if word not in stopwords.words('english')]\n",
    "# df_combo.replace(to_replace='is', value=\"\")\n",
    "stop = ['*none', '.', 'is']\n",
    "df_combo['sample'].apply(lambda x: [item for item in x if item not in stop])\n",
    "df_combo[200:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Break down into training, dev, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1606"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_index = np.random.permutation(df_combo.index)\n",
    "df_combo.ix[random_index, ['sample', 'metaphor', 'an', 'svo', 'metanet']]\n",
    "df_shuffled = df_combo.ix[random_index, ['sample', 'metaphor', 'an', 'svo', 'metanet']]\n",
    "df_shuffled.reset_index(drop=True, inplace=True)\n",
    "len(df_shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 1606\n",
      "Columns: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 5)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows, columns = df_shuffled.shape\n",
    "print(\"Rows:\", rows)\n",
    "print(\"Columns:\", columns)\n",
    "#train_size = round(rows*.6)\n",
    "train_size = round(rows*.9)\n",
    "#dev_size   = round(rows*.2)\n",
    "dev_size   = round(rows*.1)\n",
    "df_train = df_shuffled.loc[:train_size]\n",
    "df_train.shape\n",
    "df_dev = df_shuffled.loc[train_size:dev_size+train_size].reset_index(drop=True)\n",
    "df_dev.shape\n",
    "df_test = df_shuffled.loc[dev_size+train_size:].reset_index(drop=True)\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer\n",
    "### Use count vecotrizer with df = 3 for unigram and bigrams to predict dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = list(set(brown.words()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# vec = CountVectorizer(ngram_range=(1, 3), token_pattern=r'\\b\\w+\\b', analyzer=u'word', min_df=1, vocabulary=vocab)\n",
    "# df_train = df_train.fillna(\"\")\n",
    "# df_dev = df_dev.fillna(\"\")\n",
    "# df_test = df_test.fillna(\"\")\n",
    "\n",
    "\n",
    "vec = TfidfVectorizer(ngram_range=(2, 3), token_pattern=r'\\b\\w+\\b', analyzer=u'char', min_df=1)\n",
    "df_train = df_train.fillna(\"\")\n",
    "df_dev = df_dev.fillna(\"\")\n",
    "df_test = df_test.fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arr_train_feature_sparse = vec.fit_transform(df_train['sample'])\n",
    "arr_train_feature_sparse\n",
    "arr_train_feature = arr_train_feature_sparse.toarray()\n",
    "feature_labels = vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arr_dev_feature_sparse = vec.transform(df_dev[\"sample\"])\n",
    "arr_dev_feature = arr_dev_feature_sparse.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80124223602484468"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor']) #defining features (from reviews) and passing in Category label\n",
    "logreg_predictions = logreg_model.predict(arr_dev_feature)\n",
    "accuracy_score(df_dev['metaphor'], logreg_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sayan/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:5: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>s</th>\n",
       "      <td>56.563381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>54.526603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>52.336908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>51.273717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ti</th>\n",
       "      <td>50.196289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>50.193332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on</th>\n",
       "      <td>49.803135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>45.391184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>45.139044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ng</th>\n",
       "      <td>42.342324</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        counts\n",
       "s    56.563381\n",
       "in   54.526603\n",
       " i   52.336908\n",
       "is   51.273717\n",
       "ti   50.196289\n",
       " a   50.193332\n",
       "on   49.803135\n",
       "is   45.391184\n",
       " is  45.139044\n",
       "ng   42.342324"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_sum = arr_train_feature.sum(axis=0)   #sum the counts of each feature\n",
    "\n",
    "df_feature_sum = pd.DataFrame({'counts': feature_sum})\n",
    "df_feature_sum.index = vec.get_feature_names()\n",
    "df_feature_sum.sort('counts', ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## make a file of sample sentences to practice on\n",
    "## loop through sentence and .split()\n",
    "## use nltk bigrams\n",
    "##\n",
    "## put into pandas as df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jamal', 'pig', 'dinner']\n"
     ]
    }
   ],
   "source": [
    "word_list = 'Jamal was a pig at dinner'.split()\n",
    "filtered_words = [word for word in word_list if word not in stopwords.words('english')]\n",
    "filtered_words\n",
    "testmeta = list(trigrams(filtered_words))\n",
    "testmeta\n",
    "testmeta1 = [' '.join(x) for x in testmeta]\n",
    "testmeta1\n",
    "df_test = pd.DataFrame(testmeta1)\n",
    "df_test\n",
    "df_test.columns = ['sample']\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_test_feature_sparse = vec.transform(df_test['sample'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "logreg_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jamal pig dinner</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0\n",
       "0  Jamal pig dinner"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.DataFrame(testmeta1)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"test_meta_sent.txt\", \"r\") as testset:\n",
    "    filtered_words = []\n",
    "    testmeta = []\n",
    "    testmeta1 = []\n",
    "    word_list = ''\n",
    "    for line in testset:\n",
    "#         word_list = line.split()\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        word_list = tokenizer.tokenize(line)\n",
    "        filtered_words = [word for word in word_list if word not in stopwords.words('english')]\n",
    "#         testmeta1.append(' '.join(filtered_words))\n",
    "        testmeta1.append(' '.join(x) for x in bigrams(filtered_words))\n",
    "#         testmeta1.append(' '.join(x) for x in filtered_words)\n",
    "# print(testmeta1)\n",
    "\n",
    "df_test =pd.DataFrame(testmeta1)\n",
    "df_test\n",
    "df_test = df_test.fillna(\"\")\n",
    "df_test\n",
    "# df_test.columns = ['sample']\n",
    "# df_test = pd.DataFrame(testmeta1)\n",
    "# df_test\n",
    "df_test.columns = ['sample0', 'sample1', 'sample2', 'sample3', 'sample4', 'sample5', 'sample6']\n",
    "# df_test = df_test.fillna(\"\")\n",
    "# df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-29cc55d9702a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogreg_predictions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# df_test['metaphor0'] = logreg_predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sample0'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'metaphor0'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sayan/anaconda/lib/python3.5/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__nonzero__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    890\u001b[0m         raise ValueError(\"The truth value of a {0} is ambiguous. \"\n\u001b[1;32m    891\u001b[0m                          \u001b[0;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m                          .format(self.__class__.__name__))\n\u001b[0m\u001b[1;32m    893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m     \u001b[0m__bool__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "arr_test_feature_sparse = vec.transform(df_test['sample0'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "# df_test['metaphor0'] = logreg_predictions\n",
    "if df_test['sample0'] == '':\n",
    "    df_test['metaphor0'] = 0\n",
    "else:\n",
    "    df_test['metaphor0'] = logreg_predictions\n",
    "\n",
    "arr_test_feature_sparse = vec.transform(df_test['sample1'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "# df_test['metaphor1'] = logreg_predictions\n",
    "if df_test['sample1'] == '':\n",
    "    df_test['metaphor1'] = 0\n",
    "else:\n",
    "    df_test['metaphor1'] = logreg_predictions\n",
    "\n",
    "arr_test_feature_sparse = vec.transform(df_test['sample2'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "# df_test['metaphor2'] = logreg_predictions\n",
    "if df_test['sample2'] == '':\n",
    "    df_test['metaphor2'] = 0\n",
    "else:\n",
    "    df_test['metaphor2'] = logreg_predictions\n",
    "\n",
    "arr_test_feature_sparse = vec.transform(df_test['sample3'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "# df_test['metaphor3'] = logreg_predictions\n",
    "if df_test['sample3'] == '':\n",
    "    df_test['metaphor3'] = 0\n",
    "else:\n",
    "    df_test['metaphor3'] = logreg_predictions\n",
    "\n",
    "arr_test_feature_sparse = vec.transform(df_test['sample4'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "# df_test['metaphor4'] = logreg_predictions\n",
    "if df_test['sample4'] == '':\n",
    "    df_test['metaphor4'] = 0\n",
    "else:\n",
    "    df_test['metaphor4'] = logreg_predictions\n",
    "\n",
    "arr_test_feature_sparse = vec.transform(df_test['sample5'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "# df_test['metaphor5'] = logreg_predictions\n",
    "if df_test['sample5'] == '':\n",
    "    df_test['metaphor5'] = 0\n",
    "else:\n",
    "    df_test['metaphor5'] = logreg_predictions\n",
    "\n",
    "arr_test_feature_sparse = vec.transform(df_test['sample6'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "# df_test['metaphor6'] = logreg_predictions\n",
    "if df_test['sample6'] == '':\n",
    "    df_test['metaphor6'] = 0\n",
    "else:\n",
    "    df_test['metaphor6'] = logreg_predictions\n",
    "\n",
    "df_test['sum_metaphor'] = df_test['metaphor0'] + df_test['metaphor1'] + df_test['metaphor2'] + df_test['metaphor3'] +df_test['metaphor4'] +df_test['metaphor5'] + df_test['metaphor6'] \n",
    "\n",
    "# def count_metaphors():\n",
    "#     if sum(df_test['sum_metaphor']) >= 1:\n",
    "#         df_test['sum_metaphor'] = 1\n",
    "#     else:\n",
    "#         df_test['sum_metaphor'] = 0\n",
    "# count_metaphors()\n",
    "\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      True\n",
      "1      True\n",
      "2      True\n",
      "3      True\n",
      "4     False\n",
      "5      True\n",
      "6     False\n",
      "7      True\n",
      "8     False\n",
      "9      True\n",
      "10     True\n",
      "11     True\n",
      "12     True\n",
      "13     True\n",
      "14     True\n",
      "15    False\n",
      "16    False\n",
      "17     True\n",
      "18    False\n",
      "19    False\n",
      "20     True\n",
      "21     True\n",
      "22     True\n",
      "23     True\n",
      "24    False\n",
      "25     True\n",
      "26    False\n",
      "27    False\n",
      "28     True\n",
      "29     True\n",
      "      ...  \n",
      "41     True\n",
      "42     True\n",
      "43     True\n",
      "44     True\n",
      "45     True\n",
      "46     True\n",
      "47    False\n",
      "48    False\n",
      "49     True\n",
      "50     True\n",
      "51    False\n",
      "52     True\n",
      "53     True\n",
      "54     True\n",
      "55     True\n",
      "56     True\n",
      "57     True\n",
      "58    False\n",
      "59     True\n",
      "60     True\n",
      "61     True\n",
      "62    False\n",
      "63    False\n",
      "64    False\n",
      "65     True\n",
      "66    False\n",
      "67     True\n",
      "68     True\n",
      "69    False\n",
      "70    False\n",
      "Name: sample3, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "print(df_test['sample3'] == '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4     10\n",
       "5      0\n",
       "6     13\n",
       "7      0\n",
       "8      9\n",
       "9      0\n",
       "10     0\n",
       "11     0\n",
       "12     0\n",
       "13     0\n",
       "14     0\n",
       "15    13\n",
       "16    12\n",
       "17     0\n",
       "18    13\n",
       "19    14\n",
       "20     0\n",
       "21     0\n",
       "22     0\n",
       "23     0\n",
       "24    11\n",
       "25     0\n",
       "26    14\n",
       "27    13\n",
       "28     0\n",
       "29     0\n",
       "      ..\n",
       "41     0\n",
       "42     0\n",
       "43     0\n",
       "44     0\n",
       "45     0\n",
       "46     0\n",
       "47    13\n",
       "48    10\n",
       "49     0\n",
       "50     0\n",
       "51    11\n",
       "52     0\n",
       "53     0\n",
       "54     0\n",
       "55     0\n",
       "56     0\n",
       "57     0\n",
       "58    11\n",
       "59     0\n",
       "60     0\n",
       "61     0\n",
       "62    12\n",
       "63    17\n",
       "64    14\n",
       "65     0\n",
       "66    10\n",
       "67     0\n",
       "68     0\n",
       "69    11\n",
       "70    14\n",
       "Name: sample3, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['sample3'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"an_nonmets.txt\", \"r\") as testset:\n",
    "    filtered_words = []\n",
    "    testmeta = []\n",
    "    testmeta1 = []\n",
    "    word_list = ''\n",
    "    for line in testset:\n",
    "        word_list = line.split()\n",
    "        filtered_words = [word for word in word_list if word not in stopwords.words('english')]\n",
    "        testmeta1.append(' '.join(x) for x in bigrams(filtered_words))\n",
    "df_test = pd.DataFrame(testmeta1)\n",
    "df_test\n",
    "df_test.columns = ['sample0']\n",
    "df_test = df_test.fillna(\"\")\n",
    "df_test\n",
    "\n",
    "arr_test_feature_sparse = vec.transform(df_test['sample0'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "df_test['metaphor0'] = logreg_predictions\n",
    "\n",
    "sum(df_test['metaphor0'])\n",
    "#len(df_test['metaphor0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Scientists created â€œatlas</td>\n",
       "      <td>created â€œatlas brainâ€</td>\n",
       "      <td>â€œatlas brainâ€ reveals</td>\n",
       "      <td>brainâ€ reveals meanings</td>\n",
       "      <td>reveals meanings words</td>\n",
       "      <td>meanings words arranged</td>\n",
       "      <td>words arranged across</td>\n",
       "      <td>arranged across different</td>\n",
       "      <td>across different regions</td>\n",
       "      <td>different regions organ.</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Like colourful quilt</td>\n",
       "      <td>colourful quilt laid</td>\n",
       "      <td>quilt laid cortex,</td>\n",
       "      <td>laid cortex, atlas</td>\n",
       "      <td>cortex, atlas displays</td>\n",
       "      <td>atlas displays rainbow</td>\n",
       "      <td>displays rainbow hues</td>\n",
       "      <td>rainbow hues individual</td>\n",
       "      <td>hues individual words</td>\n",
       "      <td>individual words concepts</td>\n",
       "      <td>...</td>\n",
       "      <td>concepts convey grouped</td>\n",
       "      <td>convey grouped together</td>\n",
       "      <td>grouped together clumps</td>\n",
       "      <td>together clumps white</td>\n",
       "      <td>clumps white matter.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>â€œOur goal build</td>\n",
       "      <td>goal build giant</td>\n",
       "      <td>build giant atlas</td>\n",
       "      <td>giant atlas shows</td>\n",
       "      <td>atlas shows one</td>\n",
       "      <td>shows one specific</td>\n",
       "      <td>one specific aspect</td>\n",
       "      <td>specific aspect language</td>\n",
       "      <td>aspect language represented</td>\n",
       "      <td>language represented brain,</td>\n",
       "      <td>...</td>\n",
       "      <td>brain, case semantics,</td>\n",
       "      <td>case semantics, meanings</td>\n",
       "      <td>semantics, meanings words,â€</td>\n",
       "      <td>meanings words,â€ said</td>\n",
       "      <td>words,â€ said Jack</td>\n",
       "      <td>said Jack Gallant,</td>\n",
       "      <td>Jack Gallant, neuroscientist</td>\n",
       "      <td>Gallant, neuroscientist University</td>\n",
       "      <td>neuroscientist University California,</td>\n",
       "      <td>University California, Berkeley.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No single brain</td>\n",
       "      <td>single brain region</td>\n",
       "      <td>brain region holds</td>\n",
       "      <td>region holds one</td>\n",
       "      <td>holds one word</td>\n",
       "      <td>one word concept.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A single brain</td>\n",
       "      <td>single brain spot</td>\n",
       "      <td>brain spot associated</td>\n",
       "      <td>spot associated number</td>\n",
       "      <td>associated number related</td>\n",
       "      <td>number related words.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>And single word</td>\n",
       "      <td>single word lights</td>\n",
       "      <td>word lights many</td>\n",
       "      <td>lights many different</td>\n",
       "      <td>many different brain</td>\n",
       "      <td>different brain spots.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Together make networks</td>\n",
       "      <td>make networks represent</td>\n",
       "      <td>networks represent meanings</td>\n",
       "      <td>represent meanings word</td>\n",
       "      <td>meanings word use:</td>\n",
       "      <td>word use: life</td>\n",
       "      <td>use: life love;</td>\n",
       "      <td>life love; death</td>\n",
       "      <td>love; death taxes;</td>\n",
       "      <td>death taxes; clouds,</td>\n",
       "      <td>...</td>\n",
       "      <td>clouds, Florida bra.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>All light networks.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Described â€œtour de</td>\n",
       "      <td>â€œtour de forceâ€</td>\n",
       "      <td>de forceâ€ one</td>\n",
       "      <td>forceâ€ one researcher</td>\n",
       "      <td>one researcher involved</td>\n",
       "      <td>researcher involved study,</td>\n",
       "      <td>involved study, atlas</td>\n",
       "      <td>study, atlas demonstrates</td>\n",
       "      <td>atlas demonstrates modern</td>\n",
       "      <td>demonstrates modern imaging</td>\n",
       "      <td>...</td>\n",
       "      <td>imaging transform knowledge</td>\n",
       "      <td>transform knowledge brain</td>\n",
       "      <td>knowledge brain performs</td>\n",
       "      <td>brain performs important</td>\n",
       "      <td>performs important tasks.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>With advances, technology</td>\n",
       "      <td>advances, technology could</td>\n",
       "      <td>technology could profound</td>\n",
       "      <td>could profound impact</td>\n",
       "      <td>profound impact medicine</td>\n",
       "      <td>impact medicine fields.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>â€œIt possible approach</td>\n",
       "      <td>possible approach could</td>\n",
       "      <td>approach could used</td>\n",
       "      <td>could used decode</td>\n",
       "      <td>used decode information</td>\n",
       "      <td>decode information words</td>\n",
       "      <td>information words person</td>\n",
       "      <td>words person hearing,</td>\n",
       "      <td>person hearing, reading,</td>\n",
       "      <td>hearing, reading, possibly</td>\n",
       "      <td>...</td>\n",
       "      <td>possibly even thinking,â€</td>\n",
       "      <td>even thinking,â€ said</td>\n",
       "      <td>thinking,â€ said Alexander</td>\n",
       "      <td>said Alexander Huth,</td>\n",
       "      <td>Alexander Huth, first</td>\n",
       "      <td>Huth, first author</td>\n",
       "      <td>first author study.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>One potential use</td>\n",
       "      <td>potential use would</td>\n",
       "      <td>use would language</td>\n",
       "      <td>would language decoder</td>\n",
       "      <td>language decoder could</td>\n",
       "      <td>decoder could allow</td>\n",
       "      <td>could allow people</td>\n",
       "      <td>allow people silenced</td>\n",
       "      <td>people silenced motor</td>\n",
       "      <td>silenced motor neurone</td>\n",
       "      <td>...</td>\n",
       "      <td>neurone disease locked-in</td>\n",
       "      <td>disease locked-in syndrome</td>\n",
       "      <td>locked-in syndrome speak</td>\n",
       "      <td>syndrome speak computer.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>To create atlas,</td>\n",
       "      <td>create atlas, scientists</td>\n",
       "      <td>atlas, scientists recorded</td>\n",
       "      <td>scientists recorded peopleâ€™s</td>\n",
       "      <td>recorded peopleâ€™s brain</td>\n",
       "      <td>peopleâ€™s brain activity</td>\n",
       "      <td>brain activity listened</td>\n",
       "      <td>activity listened stories</td>\n",
       "      <td>listened stories read</td>\n",
       "      <td>stories read The</td>\n",
       "      <td>...</td>\n",
       "      <td>The Moth Radio</td>\n",
       "      <td>Moth Radio Hour,</td>\n",
       "      <td>Radio Hour, US</td>\n",
       "      <td>Hour, US radio</td>\n",
       "      <td>US radio show.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>They matched transcripts</td>\n",
       "      <td>matched transcripts stories</td>\n",
       "      <td>transcripts stories brain</td>\n",
       "      <td>stories brain activity</td>\n",
       "      <td>brain activity data</td>\n",
       "      <td>activity data show</td>\n",
       "      <td>data show groups</td>\n",
       "      <td>show groups related</td>\n",
       "      <td>groups related words</td>\n",
       "      <td>related words triggered</td>\n",
       "      <td>...</td>\n",
       "      <td>triggered neural responses</td>\n",
       "      <td>neural responses 50,000</td>\n",
       "      <td>responses 50,000 80,000</td>\n",
       "      <td>50,000 80,000 pea-sized</td>\n",
       "      <td>80,000 pea-sized spots</td>\n",
       "      <td>pea-sized spots cerebral</td>\n",
       "      <td>spots cerebral cortex.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Huth used stories</td>\n",
       "      <td>used stories The</td>\n",
       "      <td>stories The Moth</td>\n",
       "      <td>The Moth Radio</td>\n",
       "      <td>Moth Radio Hour</td>\n",
       "      <td>Radio Hour short</td>\n",
       "      <td>Hour short compelling.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>The enthralling stories,</td>\n",
       "      <td>enthralling stories, confident</td>\n",
       "      <td>stories, confident scientists</td>\n",
       "      <td>confident scientists could</td>\n",
       "      <td>scientists could people</td>\n",
       "      <td>could people scanned</td>\n",
       "      <td>people scanned focusing</td>\n",
       "      <td>scanned focusing words</td>\n",
       "      <td>focusing words drifting</td>\n",
       "      <td>words drifting off.</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Seven people listened</td>\n",
       "      <td>people listened two</td>\n",
       "      <td>listened two hours</td>\n",
       "      <td>two hours stories</td>\n",
       "      <td>hours stories each.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Per person, amounted</td>\n",
       "      <td>person, amounted hearing</td>\n",
       "      <td>amounted hearing roughly</td>\n",
       "      <td>hearing roughly 25,000</td>\n",
       "      <td>roughly 25,000 words-</td>\n",
       "      <td>25,000 words- 3,000</td>\n",
       "      <td>words- 3,000 different</td>\n",
       "      <td>3,000 different words</td>\n",
       "      <td>different words -</td>\n",
       "      <td>words - lay</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>The atlas shows</td>\n",
       "      <td>atlas shows words</td>\n",
       "      <td>shows words related</td>\n",
       "      <td>words related terms</td>\n",
       "      <td>related terms exercise</td>\n",
       "      <td>terms exercise regions</td>\n",
       "      <td>exercise regions brain.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>For example, left-hand</td>\n",
       "      <td>example, left-hand side</td>\n",
       "      <td>left-hand side brain,</td>\n",
       "      <td>side brain, ear,</td>\n",
       "      <td>brain, ear, one</td>\n",
       "      <td>ear, one tiny</td>\n",
       "      <td>one tiny regions</td>\n",
       "      <td>tiny regions represents</td>\n",
       "      <td>regions represents word</td>\n",
       "      <td>represents word â€œvictimâ€.</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>The region responds</td>\n",
       "      <td>region responds â€œkilledâ€,</td>\n",
       "      <td>responds â€œkilledâ€, â€œconvictedâ€,</td>\n",
       "      <td>â€œkilledâ€, â€œconvictedâ€, â€œmurderedâ€</td>\n",
       "      <td>â€œconvictedâ€, â€œmurderedâ€ â€œconfessedâ€.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>On brainâ€™s right-hand</td>\n",
       "      <td>brainâ€™s right-hand side,</td>\n",
       "      <td>right-hand side, near</td>\n",
       "      <td>side, near top</td>\n",
       "      <td>near top head,</td>\n",
       "      <td>top head, one</td>\n",
       "      <td>head, one brain</td>\n",
       "      <td>one brain spots</td>\n",
       "      <td>brain spots activated</td>\n",
       "      <td>spots activated family</td>\n",
       "      <td>...</td>\n",
       "      <td>family terms: â€œwifeâ€,</td>\n",
       "      <td>terms: â€œwifeâ€, â€œhusbandâ€,</td>\n",
       "      <td>â€œwifeâ€, â€œhusbandâ€, â€œchildrenâ€,</td>\n",
       "      <td>â€œhusbandâ€, â€œchildrenâ€, â€œparentsâ€.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Each word represented</td>\n",
       "      <td>word represented one</td>\n",
       "      <td>represented one spot</td>\n",
       "      <td>one spot words</td>\n",
       "      <td>spot words tend</td>\n",
       "      <td>words tend several</td>\n",
       "      <td>tend several meanings.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>One part brain,</td>\n",
       "      <td>part brain, example,</td>\n",
       "      <td>brain, example, reliably</td>\n",
       "      <td>example, reliably responds</td>\n",
       "      <td>reliably responds word</td>\n",
       "      <td>responds word â€œtopâ€,</td>\n",
       "      <td>word â€œtopâ€, along</td>\n",
       "      <td>â€œtopâ€, along words</td>\n",
       "      <td>along words describe</td>\n",
       "      <td>words describe clothing.</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>But word â€œtopâ€</td>\n",
       "      <td>word â€œtopâ€ activates</td>\n",
       "      <td>â€œtopâ€ activates many</td>\n",
       "      <td>activates many regions.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>One responds numbers</td>\n",
       "      <td>responds numbers measurements,</td>\n",
       "      <td>numbers measurements, another</td>\n",
       "      <td>measurements, another buildings</td>\n",
       "      <td>another buildings places.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>The scientists created</td>\n",
       "      <td>scientists created interactive</td>\n",
       "      <td>created interactive website</td>\n",
       "      <td>interactive website public</td>\n",
       "      <td>website public explore</td>\n",
       "      <td>public explore brain</td>\n",
       "      <td>explore brain atlas.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Strikingly, brain atlases</td>\n",
       "      <td>brain atlases similar</td>\n",
       "      <td>atlases similar participants,</td>\n",
       "      <td>similar participants, suggesting</td>\n",
       "      <td>participants, suggesting brains</td>\n",
       "      <td>suggesting brains organised</td>\n",
       "      <td>brains organised meanings</td>\n",
       "      <td>organised meanings words</td>\n",
       "      <td>meanings words way.</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>The scientists scanned</td>\n",
       "      <td>scientists scanned five</td>\n",
       "      <td>scanned five men</td>\n",
       "      <td>five men two</td>\n",
       "      <td>men two women,</td>\n",
       "      <td>two women, however.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>All native English</td>\n",
       "      <td>native English speakers,</td>\n",
       "      <td>English speakers, two</td>\n",
       "      <td>speakers, two authors</td>\n",
       "      <td>two authors study</td>\n",
       "      <td>authors study published</td>\n",
       "      <td>study published Nature.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>It highly possible</td>\n",
       "      <td>highly possible people</td>\n",
       "      <td>possible people different</td>\n",
       "      <td>people different backgrounds</td>\n",
       "      <td>different backgrounds cultures</td>\n",
       "      <td>backgrounds cultures different</td>\n",
       "      <td>cultures different semantic</td>\n",
       "      <td>different semantic brain</td>\n",
       "      <td>semantic brain atlases.</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Armed atlas, researchers</td>\n",
       "      <td>atlas, researchers piece</td>\n",
       "      <td>researchers piece together</td>\n",
       "      <td>piece together brain</td>\n",
       "      <td>together brain networks</td>\n",
       "      <td>brain networks represent</td>\n",
       "      <td>networks represent wildly</td>\n",
       "      <td>represent wildly different</td>\n",
       "      <td>wildly different concepts,</td>\n",
       "      <td>different concepts, numbers</td>\n",
       "      <td>...</td>\n",
       "      <td>numbers murder religion.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>â€œThe idea murder</td>\n",
       "      <td>idea murder represented</td>\n",
       "      <td>murder represented lot</td>\n",
       "      <td>represented lot brain,â€</td>\n",
       "      <td>lot brain,â€ Gallant</td>\n",
       "      <td>brain,â€ Gallant said.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Using haul data,</td>\n",
       "      <td>haul data, group</td>\n",
       "      <td>data, group begun</td>\n",
       "      <td>group begun work</td>\n",
       "      <td>begun work new</td>\n",
       "      <td>work new atlases</td>\n",
       "      <td>new atlases show</td>\n",
       "      <td>atlases show brain</td>\n",
       "      <td>show brain holds</td>\n",
       "      <td>brain holds information</td>\n",
       "      <td>...</td>\n",
       "      <td>information aspects language,</td>\n",
       "      <td>aspects language, phonemes</td>\n",
       "      <td>language, phonemes syntax.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>A brain atlas</td>\n",
       "      <td>brain atlas narrative</td>\n",
       "      <td>atlas narrative structure</td>\n",
       "      <td>narrative structure far</td>\n",
       "      <td>structure far proved</td>\n",
       "      <td>far proved elusive,</td>\n",
       "      <td>proved elusive, however.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>â€œEvery time come</td>\n",
       "      <td>time come set</td>\n",
       "      <td>come set narrative</td>\n",
       "      <td>set narrative features,</td>\n",
       "      <td>narrative features, get</td>\n",
       "      <td>features, get told</td>\n",
       "      <td>get told arenâ€™t</td>\n",
       "      <td>told arenâ€™t right</td>\n",
       "      <td>arenâ€™t right set</td>\n",
       "      <td>right set narrative</td>\n",
       "      <td>...</td>\n",
       "      <td>narrative features,â€ said</td>\n",
       "      <td>features,â€ said Gallant.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Uri Hasson, neuroscientist</td>\n",
       "      <td>Hasson, neuroscientist Princeton</td>\n",
       "      <td>neuroscientist Princeton University,</td>\n",
       "      <td>Princeton University, praised</td>\n",
       "      <td>University, praised work.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Unlike many studies</td>\n",
       "      <td>many studies looked</td>\n",
       "      <td>studies looked brain</td>\n",
       "      <td>looked brain activity</td>\n",
       "      <td>brain activity isolated</td>\n",
       "      <td>activity isolated word</td>\n",
       "      <td>isolated word sentence</td>\n",
       "      <td>word sentence spoken,</td>\n",
       "      <td>sentence spoken, Gallantâ€™s</td>\n",
       "      <td>spoken, Gallantâ€™s team</td>\n",
       "      <td>...</td>\n",
       "      <td>team shed light</td>\n",
       "      <td>shed light brain</td>\n",
       "      <td>light brain worked</td>\n",
       "      <td>brain worked real-world</td>\n",
       "      <td>worked real-world scenario,</td>\n",
       "      <td>real-world scenario, said.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>The next step,</td>\n",
       "      <td>next step, said,</td>\n",
       "      <td>step, said, create</td>\n",
       "      <td>said, create comprehensive</td>\n",
       "      <td>create comprehensive precise</td>\n",
       "      <td>comprehensive precise semantic</td>\n",
       "      <td>precise semantic brain</td>\n",
       "      <td>semantic brain atlas.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Ultimately, Hasson believes</td>\n",
       "      <td>Hasson believes possible</td>\n",
       "      <td>believes possible reconstruct</td>\n",
       "      <td>possible reconstruct words</td>\n",
       "      <td>reconstruct words person</td>\n",
       "      <td>words person thinking</td>\n",
       "      <td>person thinking brain</td>\n",
       "      <td>thinking brain activity.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>The ethical implications</td>\n",
       "      <td>ethical implications enormous.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>One benign use</td>\n",
       "      <td>benign use would</td>\n",
       "      <td>use would see</td>\n",
       "      <td>would see brain</td>\n",
       "      <td>see brain activity</td>\n",
       "      <td>brain activity used</td>\n",
       "      <td>activity used assess</td>\n",
       "      <td>used assess whether</td>\n",
       "      <td>assess whether political</td>\n",
       "      <td>whether political messages</td>\n",
       "      <td>...</td>\n",
       "      <td>messages effectively communicated</td>\n",
       "      <td>effectively communicated public.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>â€œThere many implications,</td>\n",
       "      <td>many implications, barely</td>\n",
       "      <td>implications, barely touching</td>\n",
       "      <td>barely touching surface,â€</td>\n",
       "      <td>touching surface,â€ said.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Lorraine Tyler, cognitive</td>\n",
       "      <td>Tyler, cognitive neuroscientist</td>\n",
       "      <td>cognitive neuroscientist head</td>\n",
       "      <td>neuroscientist head Centre</td>\n",
       "      <td>head Centre Speech,</td>\n",
       "      <td>Centre Speech, Language</td>\n",
       "      <td>Speech, Language Brain</td>\n",
       "      <td>Language Brain Cambridge</td>\n",
       "      <td>Brain Cambridge University</td>\n",
       "      <td>Cambridge University said</td>\n",
       "      <td>...</td>\n",
       "      <td>said research â€œtour</td>\n",
       "      <td>research â€œtour de</td>\n",
       "      <td>â€œtour de force</td>\n",
       "      <td>de force scope</td>\n",
       "      <td>force scope methodsâ€.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>But brain atlas</td>\n",
       "      <td>brain atlas current</td>\n",
       "      <td>atlas current form</td>\n",
       "      <td>current form capture</td>\n",
       "      <td>form capture fine</td>\n",
       "      <td>capture fine differences</td>\n",
       "      <td>fine differences word</td>\n",
       "      <td>differences word meanings.</td>\n",
       "      <td>word meanings. Take</td>\n",
       "      <td>meanings. Take word</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>It member many</td>\n",
       "      <td>member many different</td>\n",
       "      <td>many different groups,</td>\n",
       "      <td>different groups, says</td>\n",
       "      <td>groups, says Tyler.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>â€œIt something eat</td>\n",
       "      <td>something eat off,</td>\n",
       "      <td>eat off, things</td>\n",
       "      <td>off, things made</td>\n",
       "      <td>things made wood,</td>\n",
       "      <td>made wood, things</td>\n",
       "      <td>wood, things heavy,</td>\n",
       "      <td>things heavy, things</td>\n",
       "      <td>heavy, things four</td>\n",
       "      <td>things four legs,</td>\n",
       "      <td>...</td>\n",
       "      <td>legs, non-animate objects,</td>\n",
       "      <td>non-animate objects, on.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>This kind detailed</td>\n",
       "      <td>kind detailed semantic</td>\n",
       "      <td>detailed semantic information</td>\n",
       "      <td>semantic information enables</td>\n",
       "      <td>information enables words</td>\n",
       "      <td>enables words used</td>\n",
       "      <td>words used flexibly</td>\n",
       "      <td>used flexibly lost</td>\n",
       "      <td>flexibly lost analysis,â€</td>\n",
       "      <td>lost analysis,â€ said.</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>â€œWhile research path-breaking</td>\n",
       "      <td>research path-breaking scope,</td>\n",
       "      <td>path-breaking scope, still</td>\n",
       "      <td>scope, still lot</td>\n",
       "      <td>still lot learn</td>\n",
       "      <td>lot learn semantics</td>\n",
       "      <td>learn semantics represented</td>\n",
       "      <td>semantics represented brain.â€</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               0                                 1   \\\n",
       "0       Scientists created â€œatlas             created â€œatlas brainâ€   \n",
       "1            Like colourful quilt              colourful quilt laid   \n",
       "2                 â€œOur goal build                  goal build giant   \n",
       "3                 No single brain               single brain region   \n",
       "4                  A single brain                 single brain spot   \n",
       "5                 And single word                single word lights   \n",
       "6          Together make networks           make networks represent   \n",
       "7             All light networks.                              None   \n",
       "8              Described â€œtour de                   â€œtour de forceâ€   \n",
       "9       With advances, technology        advances, technology could   \n",
       "10          â€œIt possible approach           possible approach could   \n",
       "11              One potential use               potential use would   \n",
       "12               To create atlas,          create atlas, scientists   \n",
       "13       They matched transcripts       matched transcripts stories   \n",
       "14              Huth used stories                  used stories The   \n",
       "15       The enthralling stories,    enthralling stories, confident   \n",
       "16          Seven people listened               people listened two   \n",
       "17           Per person, amounted          person, amounted hearing   \n",
       "18                The atlas shows                 atlas shows words   \n",
       "19         For example, left-hand           example, left-hand side   \n",
       "20            The region responds         region responds â€œkilledâ€,   \n",
       "21          On brainâ€™s right-hand          brainâ€™s right-hand side,   \n",
       "22          Each word represented              word represented one   \n",
       "23                One part brain,              part brain, example,   \n",
       "24                 But word â€œtopâ€              word â€œtopâ€ activates   \n",
       "25           One responds numbers    responds numbers measurements,   \n",
       "26         The scientists created    scientists created interactive   \n",
       "27      Strikingly, brain atlases             brain atlases similar   \n",
       "28         The scientists scanned           scientists scanned five   \n",
       "29             All native English          native English speakers,   \n",
       "30             It highly possible            highly possible people   \n",
       "31       Armed atlas, researchers          atlas, researchers piece   \n",
       "32               â€œThe idea murder           idea murder represented   \n",
       "33               Using haul data,                  haul data, group   \n",
       "34                  A brain atlas             brain atlas narrative   \n",
       "35               â€œEvery time come                     time come set   \n",
       "36     Uri Hasson, neuroscientist  Hasson, neuroscientist Princeton   \n",
       "37            Unlike many studies               many studies looked   \n",
       "38                 The next step,                  next step, said,   \n",
       "39    Ultimately, Hasson believes          Hasson believes possible   \n",
       "40       The ethical implications    ethical implications enormous.   \n",
       "41                 One benign use                  benign use would   \n",
       "42      â€œThere many implications,         many implications, barely   \n",
       "43      Lorraine Tyler, cognitive   Tyler, cognitive neuroscientist   \n",
       "44                But brain atlas               brain atlas current   \n",
       "45                 It member many             member many different   \n",
       "46              â€œIt something eat                something eat off,   \n",
       "47             This kind detailed            kind detailed semantic   \n",
       "48  â€œWhile research path-breaking     research path-breaking scope,   \n",
       "\n",
       "                                      2                                  3   \\\n",
       "0                  â€œatlas brainâ€ reveals            brainâ€ reveals meanings   \n",
       "1                     quilt laid cortex,                 laid cortex, atlas   \n",
       "2                      build giant atlas                  giant atlas shows   \n",
       "3                     brain region holds                   region holds one   \n",
       "4                  brain spot associated             spot associated number   \n",
       "5                       word lights many              lights many different   \n",
       "6            networks represent meanings            represent meanings word   \n",
       "7                                   None                               None   \n",
       "8                          de forceâ€ one              forceâ€ one researcher   \n",
       "9              technology could profound              could profound impact   \n",
       "10                   approach could used                  could used decode   \n",
       "11                    use would language             would language decoder   \n",
       "12            atlas, scientists recorded       scientists recorded peopleâ€™s   \n",
       "13             transcripts stories brain             stories brain activity   \n",
       "14                      stories The Moth                     The Moth Radio   \n",
       "15         stories, confident scientists         confident scientists could   \n",
       "16                    listened two hours                  two hours stories   \n",
       "17              amounted hearing roughly             hearing roughly 25,000   \n",
       "18                   shows words related                words related terms   \n",
       "19                 left-hand side brain,                   side brain, ear,   \n",
       "20       responds â€œkilledâ€, â€œconvictedâ€,  â€œkilledâ€, â€œconvictedâ€, â€œmurderedâ€   \n",
       "21                 right-hand side, near                     side, near top   \n",
       "22                  represented one spot                     one spot words   \n",
       "23              brain, example, reliably         example, reliably responds   \n",
       "24                  â€œtopâ€ activates many            activates many regions.   \n",
       "25         numbers measurements, another    measurements, another buildings   \n",
       "26           created interactive website         interactive website public   \n",
       "27         atlases similar participants,   similar participants, suggesting   \n",
       "28                      scanned five men                       five men two   \n",
       "29                 English speakers, two              speakers, two authors   \n",
       "30             possible people different       people different backgrounds   \n",
       "31            researchers piece together               piece together brain   \n",
       "32                murder represented lot            represented lot brain,â€   \n",
       "33                     data, group begun                   group begun work   \n",
       "34             atlas narrative structure            narrative structure far   \n",
       "35                    come set narrative            set narrative features,   \n",
       "36  neuroscientist Princeton University,      Princeton University, praised   \n",
       "37                  studies looked brain              looked brain activity   \n",
       "38                    step, said, create         said, create comprehensive   \n",
       "39         believes possible reconstruct         possible reconstruct words   \n",
       "40                                  None                               None   \n",
       "41                         use would see                    would see brain   \n",
       "42         implications, barely touching          barely touching surface,â€   \n",
       "43         cognitive neuroscientist head         neuroscientist head Centre   \n",
       "44                    atlas current form               current form capture   \n",
       "45                many different groups,             different groups, says   \n",
       "46                       eat off, things                   off, things made   \n",
       "47         detailed semantic information       semantic information enables   \n",
       "48            path-breaking scope, still                   scope, still lot   \n",
       "\n",
       "                                      4                               5   \\\n",
       "0                 reveals meanings words         meanings words arranged   \n",
       "1                 cortex, atlas displays          atlas displays rainbow   \n",
       "2                        atlas shows one              shows one specific   \n",
       "3                         holds one word               one word concept.   \n",
       "4              associated number related           number related words.   \n",
       "5                   many different brain          different brain spots.   \n",
       "6                     meanings word use:                  word use: life   \n",
       "7                                   None                            None   \n",
       "8                one researcher involved      researcher involved study,   \n",
       "9               profound impact medicine         impact medicine fields.   \n",
       "10               used decode information        decode information words   \n",
       "11                language decoder could             decoder could allow   \n",
       "12               recorded peopleâ€™s brain         peopleâ€™s brain activity   \n",
       "13                   brain activity data              activity data show   \n",
       "14                       Moth Radio Hour                Radio Hour short   \n",
       "15               scientists could people            could people scanned   \n",
       "16                   hours stories each.                            None   \n",
       "17                 roughly 25,000 words-             25,000 words- 3,000   \n",
       "18                related terms exercise          terms exercise regions   \n",
       "19                       brain, ear, one                   ear, one tiny   \n",
       "20  â€œconvictedâ€, â€œmurderedâ€ â€œconfessedâ€.                            None   \n",
       "21                        near top head,                   top head, one   \n",
       "22                       spot words tend              words tend several   \n",
       "23                reliably responds word            responds word â€œtopâ€,   \n",
       "24                                  None                            None   \n",
       "25             another buildings places.                            None   \n",
       "26                website public explore            public explore brain   \n",
       "27       participants, suggesting brains     suggesting brains organised   \n",
       "28                        men two women,             two women, however.   \n",
       "29                     two authors study         authors study published   \n",
       "30        different backgrounds cultures  backgrounds cultures different   \n",
       "31               together brain networks        brain networks represent   \n",
       "32                   lot brain,â€ Gallant           brain,â€ Gallant said.   \n",
       "33                        begun work new                work new atlases   \n",
       "34                  structure far proved             far proved elusive,   \n",
       "35               narrative features, get              features, get told   \n",
       "36             University, praised work.                            None   \n",
       "37               brain activity isolated          activity isolated word   \n",
       "38          create comprehensive precise  comprehensive precise semantic   \n",
       "39              reconstruct words person           words person thinking   \n",
       "40                                  None                            None   \n",
       "41                    see brain activity             brain activity used   \n",
       "42              touching surface,â€ said.                            None   \n",
       "43                   head Centre Speech,         Centre Speech, Language   \n",
       "44                     form capture fine        capture fine differences   \n",
       "45                   groups, says Tyler.                            None   \n",
       "46                     things made wood,               made wood, things   \n",
       "47             information enables words              enables words used   \n",
       "48                       still lot learn             lot learn semantics   \n",
       "\n",
       "                             6                              7   \\\n",
       "0         words arranged across      arranged across different   \n",
       "1         displays rainbow hues        rainbow hues individual   \n",
       "2           one specific aspect       specific aspect language   \n",
       "3                          None                           None   \n",
       "4                          None                           None   \n",
       "5                          None                           None   \n",
       "6               use: life love;               life love; death   \n",
       "7                          None                           None   \n",
       "8         involved study, atlas      study, atlas demonstrates   \n",
       "9                          None                           None   \n",
       "10     information words person          words person hearing,   \n",
       "11           could allow people          allow people silenced   \n",
       "12      brain activity listened      activity listened stories   \n",
       "13             data show groups            show groups related   \n",
       "14       Hour short compelling.                           None   \n",
       "15      people scanned focusing         scanned focusing words   \n",
       "16                         None                           None   \n",
       "17       words- 3,000 different          3,000 different words   \n",
       "18      exercise regions brain.                           None   \n",
       "19             one tiny regions        tiny regions represents   \n",
       "20                         None                           None   \n",
       "21              head, one brain                one brain spots   \n",
       "22       tend several meanings.                           None   \n",
       "23            word â€œtopâ€, along             â€œtopâ€, along words   \n",
       "24                         None                           None   \n",
       "25                         None                           None   \n",
       "26         explore brain atlas.                           None   \n",
       "27    brains organised meanings       organised meanings words   \n",
       "28                         None                           None   \n",
       "29      study published Nature.                           None   \n",
       "30  cultures different semantic       different semantic brain   \n",
       "31    networks represent wildly     represent wildly different   \n",
       "32                         None                           None   \n",
       "33             new atlases show             atlases show brain   \n",
       "34     proved elusive, however.                           None   \n",
       "35              get told arenâ€™t              told arenâ€™t right   \n",
       "36                         None                           None   \n",
       "37       isolated word sentence          word sentence spoken,   \n",
       "38       precise semantic brain          semantic brain atlas.   \n",
       "39        person thinking brain       thinking brain activity.   \n",
       "40                         None                           None   \n",
       "41         activity used assess            used assess whether   \n",
       "42                         None                           None   \n",
       "43       Speech, Language Brain       Language Brain Cambridge   \n",
       "44        fine differences word     differences word meanings.   \n",
       "45                         None                           None   \n",
       "46          wood, things heavy,           things heavy, things   \n",
       "47          words used flexibly             used flexibly lost   \n",
       "48  learn semantics represented  semantics represented brain.â€   \n",
       "\n",
       "                             8                            9   \\\n",
       "0      across different regions     different regions organ.   \n",
       "1         hues individual words    individual words concepts   \n",
       "2   aspect language represented  language represented brain,   \n",
       "3                          None                         None   \n",
       "4                          None                         None   \n",
       "5                          None                         None   \n",
       "6            love; death taxes;         death taxes; clouds,   \n",
       "7                          None                         None   \n",
       "8     atlas demonstrates modern  demonstrates modern imaging   \n",
       "9                          None                         None   \n",
       "10     person hearing, reading,   hearing, reading, possibly   \n",
       "11        people silenced motor       silenced motor neurone   \n",
       "12        listened stories read             stories read The   \n",
       "13         groups related words      related words triggered   \n",
       "14                         None                         None   \n",
       "15      focusing words drifting          words drifting off.   \n",
       "16                         None                         None   \n",
       "17            different words -                  words - lay   \n",
       "18                         None                         None   \n",
       "19      regions represents word    represents word â€œvictimâ€.   \n",
       "20                         None                         None   \n",
       "21        brain spots activated       spots activated family   \n",
       "22                         None                         None   \n",
       "23         along words describe     words describe clothing.   \n",
       "24                         None                         None   \n",
       "25                         None                         None   \n",
       "26                         None                         None   \n",
       "27          meanings words way.                         None   \n",
       "28                         None                         None   \n",
       "29                         None                         None   \n",
       "30      semantic brain atlases.                         None   \n",
       "31   wildly different concepts,  different concepts, numbers   \n",
       "32                         None                         None   \n",
       "33             show brain holds      brain holds information   \n",
       "34                         None                         None   \n",
       "35             arenâ€™t right set          right set narrative   \n",
       "36                         None                         None   \n",
       "37   sentence spoken, Gallantâ€™s       spoken, Gallantâ€™s team   \n",
       "38                         None                         None   \n",
       "39                         None                         None   \n",
       "40                         None                         None   \n",
       "41     assess whether political   whether political messages   \n",
       "42                         None                         None   \n",
       "43   Brain Cambridge University    Cambridge University said   \n",
       "44          word meanings. Take          meanings. Take word   \n",
       "45                         None                         None   \n",
       "46           heavy, things four            things four legs,   \n",
       "47     flexibly lost analysis,â€        lost analysis,â€ said.   \n",
       "48                         None                         None   \n",
       "\n",
       "                  ...                                                11  \\\n",
       "0                 ...                                              None   \n",
       "1                 ...                           concepts convey grouped   \n",
       "2                 ...                            brain, case semantics,   \n",
       "3                 ...                                              None   \n",
       "4                 ...                                              None   \n",
       "5                 ...                                              None   \n",
       "6                 ...                              clouds, Florida bra.   \n",
       "7                 ...                                              None   \n",
       "8                 ...                       imaging transform knowledge   \n",
       "9                 ...                                              None   \n",
       "10                ...                          possibly even thinking,â€   \n",
       "11                ...                         neurone disease locked-in   \n",
       "12                ...                                    The Moth Radio   \n",
       "13                ...                        triggered neural responses   \n",
       "14                ...                                              None   \n",
       "15                ...                                              None   \n",
       "16                ...                                              None   \n",
       "17                ...                                              None   \n",
       "18                ...                                              None   \n",
       "19                ...                                              None   \n",
       "20                ...                                              None   \n",
       "21                ...                             family terms: â€œwifeâ€,   \n",
       "22                ...                                              None   \n",
       "23                ...                                              None   \n",
       "24                ...                                              None   \n",
       "25                ...                                              None   \n",
       "26                ...                                              None   \n",
       "27                ...                                              None   \n",
       "28                ...                                              None   \n",
       "29                ...                                              None   \n",
       "30                ...                                              None   \n",
       "31                ...                          numbers murder religion.   \n",
       "32                ...                                              None   \n",
       "33                ...                     information aspects language,   \n",
       "34                ...                                              None   \n",
       "35                ...                         narrative features,â€ said   \n",
       "36                ...                                              None   \n",
       "37                ...                                   team shed light   \n",
       "38                ...                                              None   \n",
       "39                ...                                              None   \n",
       "40                ...                                              None   \n",
       "41                ...                 messages effectively communicated   \n",
       "42                ...                                              None   \n",
       "43                ...                               said research â€œtour   \n",
       "44                ...                                              None   \n",
       "45                ...                                              None   \n",
       "46                ...                        legs, non-animate objects,   \n",
       "47                ...                                              None   \n",
       "48                ...                                              None   \n",
       "\n",
       "                                  12                              13  \\\n",
       "0                               None                            None   \n",
       "1            convey grouped together         grouped together clumps   \n",
       "2           case semantics, meanings     semantics, meanings words,â€   \n",
       "3                               None                            None   \n",
       "4                               None                            None   \n",
       "5                               None                            None   \n",
       "6                               None                            None   \n",
       "7                               None                            None   \n",
       "8          transform knowledge brain        knowledge brain performs   \n",
       "9                               None                            None   \n",
       "10              even thinking,â€ said       thinking,â€ said Alexander   \n",
       "11        disease locked-in syndrome        locked-in syndrome speak   \n",
       "12                  Moth Radio Hour,                  Radio Hour, US   \n",
       "13           neural responses 50,000         responses 50,000 80,000   \n",
       "14                              None                            None   \n",
       "15                              None                            None   \n",
       "16                              None                            None   \n",
       "17                              None                            None   \n",
       "18                              None                            None   \n",
       "19                              None                            None   \n",
       "20                              None                            None   \n",
       "21         terms: â€œwifeâ€, â€œhusbandâ€,  â€œwifeâ€, â€œhusbandâ€, â€œchildrenâ€,   \n",
       "22                              None                            None   \n",
       "23                              None                            None   \n",
       "24                              None                            None   \n",
       "25                              None                            None   \n",
       "26                              None                            None   \n",
       "27                              None                            None   \n",
       "28                              None                            None   \n",
       "29                              None                            None   \n",
       "30                              None                            None   \n",
       "31                              None                            None   \n",
       "32                              None                            None   \n",
       "33        aspects language, phonemes      language, phonemes syntax.   \n",
       "34                              None                            None   \n",
       "35          features,â€ said Gallant.                            None   \n",
       "36                              None                            None   \n",
       "37                  shed light brain              light brain worked   \n",
       "38                              None                            None   \n",
       "39                              None                            None   \n",
       "40                              None                            None   \n",
       "41  effectively communicated public.                            None   \n",
       "42                              None                            None   \n",
       "43                 research â€œtour de                  â€œtour de force   \n",
       "44                              None                            None   \n",
       "45                              None                            None   \n",
       "46          non-animate objects, on.                            None   \n",
       "47                              None                            None   \n",
       "48                              None                            None   \n",
       "\n",
       "                                   14                           15  \\\n",
       "0                                None                         None   \n",
       "1               together clumps white         clumps white matter.   \n",
       "2               meanings words,â€ said            words,â€ said Jack   \n",
       "3                                None                         None   \n",
       "4                                None                         None   \n",
       "5                                None                         None   \n",
       "6                                None                         None   \n",
       "7                                None                         None   \n",
       "8            brain performs important    performs important tasks.   \n",
       "9                                None                         None   \n",
       "10               said Alexander Huth,        Alexander Huth, first   \n",
       "11           syndrome speak computer.                         None   \n",
       "12                     Hour, US radio               US radio show.   \n",
       "13            50,000 80,000 pea-sized       80,000 pea-sized spots   \n",
       "14                               None                         None   \n",
       "15                               None                         None   \n",
       "16                               None                         None   \n",
       "17                               None                         None   \n",
       "18                               None                         None   \n",
       "19                               None                         None   \n",
       "20                               None                         None   \n",
       "21  â€œhusbandâ€, â€œchildrenâ€, â€œparentsâ€.                         None   \n",
       "22                               None                         None   \n",
       "23                               None                         None   \n",
       "24                               None                         None   \n",
       "25                               None                         None   \n",
       "26                               None                         None   \n",
       "27                               None                         None   \n",
       "28                               None                         None   \n",
       "29                               None                         None   \n",
       "30                               None                         None   \n",
       "31                               None                         None   \n",
       "32                               None                         None   \n",
       "33                               None                         None   \n",
       "34                               None                         None   \n",
       "35                               None                         None   \n",
       "36                               None                         None   \n",
       "37            brain worked real-world  worked real-world scenario,   \n",
       "38                               None                         None   \n",
       "39                               None                         None   \n",
       "40                               None                         None   \n",
       "41                               None                         None   \n",
       "42                               None                         None   \n",
       "43                     de force scope        force scope methodsâ€.   \n",
       "44                               None                         None   \n",
       "45                               None                         None   \n",
       "46                               None                         None   \n",
       "47                               None                         None   \n",
       "48                               None                         None   \n",
       "\n",
       "                            16                            17  \\\n",
       "0                         None                          None   \n",
       "1                         None                          None   \n",
       "2           said Jack Gallant,  Jack Gallant, neuroscientist   \n",
       "3                         None                          None   \n",
       "4                         None                          None   \n",
       "5                         None                          None   \n",
       "6                         None                          None   \n",
       "7                         None                          None   \n",
       "8                         None                          None   \n",
       "9                         None                          None   \n",
       "10          Huth, first author           first author study.   \n",
       "11                        None                          None   \n",
       "12                        None                          None   \n",
       "13    pea-sized spots cerebral        spots cerebral cortex.   \n",
       "14                        None                          None   \n",
       "15                        None                          None   \n",
       "16                        None                          None   \n",
       "17                        None                          None   \n",
       "18                        None                          None   \n",
       "19                        None                          None   \n",
       "20                        None                          None   \n",
       "21                        None                          None   \n",
       "22                        None                          None   \n",
       "23                        None                          None   \n",
       "24                        None                          None   \n",
       "25                        None                          None   \n",
       "26                        None                          None   \n",
       "27                        None                          None   \n",
       "28                        None                          None   \n",
       "29                        None                          None   \n",
       "30                        None                          None   \n",
       "31                        None                          None   \n",
       "32                        None                          None   \n",
       "33                        None                          None   \n",
       "34                        None                          None   \n",
       "35                        None                          None   \n",
       "36                        None                          None   \n",
       "37  real-world scenario, said.                          None   \n",
       "38                        None                          None   \n",
       "39                        None                          None   \n",
       "40                        None                          None   \n",
       "41                        None                          None   \n",
       "42                        None                          None   \n",
       "43                        None                          None   \n",
       "44                        None                          None   \n",
       "45                        None                          None   \n",
       "46                        None                          None   \n",
       "47                        None                          None   \n",
       "48                        None                          None   \n",
       "\n",
       "                                    18                                     19  \\\n",
       "0                                 None                                   None   \n",
       "1                                 None                                   None   \n",
       "2   Gallant, neuroscientist University  neuroscientist University California,   \n",
       "3                                 None                                   None   \n",
       "4                                 None                                   None   \n",
       "5                                 None                                   None   \n",
       "6                                 None                                   None   \n",
       "7                                 None                                   None   \n",
       "8                                 None                                   None   \n",
       "9                                 None                                   None   \n",
       "10                                None                                   None   \n",
       "11                                None                                   None   \n",
       "12                                None                                   None   \n",
       "13                                None                                   None   \n",
       "14                                None                                   None   \n",
       "15                                None                                   None   \n",
       "16                                None                                   None   \n",
       "17                                None                                   None   \n",
       "18                                None                                   None   \n",
       "19                                None                                   None   \n",
       "20                                None                                   None   \n",
       "21                                None                                   None   \n",
       "22                                None                                   None   \n",
       "23                                None                                   None   \n",
       "24                                None                                   None   \n",
       "25                                None                                   None   \n",
       "26                                None                                   None   \n",
       "27                                None                                   None   \n",
       "28                                None                                   None   \n",
       "29                                None                                   None   \n",
       "30                                None                                   None   \n",
       "31                                None                                   None   \n",
       "32                                None                                   None   \n",
       "33                                None                                   None   \n",
       "34                                None                                   None   \n",
       "35                                None                                   None   \n",
       "36                                None                                   None   \n",
       "37                                None                                   None   \n",
       "38                                None                                   None   \n",
       "39                                None                                   None   \n",
       "40                                None                                   None   \n",
       "41                                None                                   None   \n",
       "42                                None                                   None   \n",
       "43                                None                                   None   \n",
       "44                                None                                   None   \n",
       "45                                None                                   None   \n",
       "46                                None                                   None   \n",
       "47                                None                                   None   \n",
       "48                                None                                   None   \n",
       "\n",
       "                                  20  \n",
       "0                               None  \n",
       "1                               None  \n",
       "2   University California, Berkeley.  \n",
       "3                               None  \n",
       "4                               None  \n",
       "5                               None  \n",
       "6                               None  \n",
       "7                               None  \n",
       "8                               None  \n",
       "9                               None  \n",
       "10                              None  \n",
       "11                              None  \n",
       "12                              None  \n",
       "13                              None  \n",
       "14                              None  \n",
       "15                              None  \n",
       "16                              None  \n",
       "17                              None  \n",
       "18                              None  \n",
       "19                              None  \n",
       "20                              None  \n",
       "21                              None  \n",
       "22                              None  \n",
       "23                              None  \n",
       "24                              None  \n",
       "25                              None  \n",
       "26                              None  \n",
       "27                              None  \n",
       "28                              None  \n",
       "29                              None  \n",
       "30                              None  \n",
       "31                              None  \n",
       "32                              None  \n",
       "33                              None  \n",
       "34                              None  \n",
       "35                              None  \n",
       "36                              None  \n",
       "37                              None  \n",
       "38                              None  \n",
       "39                              None  \n",
       "40                              None  \n",
       "41                              None  \n",
       "42                              None  \n",
       "43                              None  \n",
       "44                              None  \n",
       "45                              None  \n",
       "46                              None  \n",
       "47                              None  \n",
       "48                              None  \n",
       "\n",
       "[49 rows x 21 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"sciencearticle_line.txt\", \"r\") as testset:\n",
    "    filtered_words = []\n",
    "    testmeta = []\n",
    "    testmeta1 = []\n",
    "    word_list = ''\n",
    "    for line in testset:\n",
    "        word_list = line.split()\n",
    "        filtered_words = [word for word in word_list if word not in stopwords.words('english')]\n",
    "        testmeta1.append(' '.join(x) for x in trigrams(filtered_words))\n",
    "df_test = pd.DataFrame(testmeta1)\n",
    "# df_test = df_test.ix[:, 0:6]\n",
    "# df_test.columns = ['sample0', 'sample1', 'sample2', 'sample3', 'sample4', 'sample5', 'sample6']\n",
    "# df_test = df_test.fillna(\"\")\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'sample0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:4018)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.Int64HashTable.get_item (pandas/hashtable.c:6589)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: an integer is required",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/Users/sayan/anaconda/lib/python3.5/site-packages/pandas/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   1944\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1945\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1946\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:4154)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:4084)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sample0'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:4018)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.Int64HashTable.get_item (pandas/hashtable.c:6589)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: an integer is required",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-f6d4e028b462>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0marr_test_feature_sparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sample0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0marr_test_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr_test_feature_sparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlogreg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlogreg_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_train_feature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'metaphor'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlogreg_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogreg_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_test_feature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sayan/anaconda/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1995\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1996\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1997\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1999\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sayan/anaconda/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2002\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2003\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2004\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2006\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sayan/anaconda/lib/python3.5/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1348\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sayan/anaconda/lib/python3.5/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   3288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3289\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3290\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3291\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3292\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sayan/anaconda/lib/python3.5/site-packages/pandas/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   1945\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1946\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1947\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1949\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:4154)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:4084)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sample0'"
     ]
    }
   ],
   "source": [
    "arr_test_feature_sparse = vec.transform(df_test['sample0'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "df_test['metaphor0'] = logreg_predictions\n",
    "\n",
    "arr_test_feature_sparse = vec.transform(df_test['sample1'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "df_test['metaphor1'] = logreg_predictions\n",
    "\n",
    "arr_test_feature_sparse = vec.transform(df_test['sample2'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "df_test['metaphor2'] = logreg_predictions\n",
    "\n",
    "arr_test_feature_sparse = vec.transform(df_test['sample3'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "df_test['metaphor3'] = logreg_predictions\n",
    "\n",
    "arr_test_feature_sparse = vec.transform(df_test['sample4'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "df_test['metaphor4'] = logreg_predictions\n",
    "\n",
    "arr_test_feature_sparse = vec.transform(df_test['sample5'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "df_test['metaphor5'] = logreg_predictions\n",
    "\n",
    "arr_test_feature_sparse = vec.transform(df_test['sample6'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "df_test['metaphor6'] = logreg_predictions\n",
    "\n",
    "df_test['sum_metaphor'] = df_test['metaphor1'] + df_test['metaphor2'] + df_test['metaphor3'] +df_test['metaphor4'] +df_test['metaphor5'] \n",
    "\n",
    "if sum(df_test['sum_metaphor']) >= 1:\n",
    "    df_test['sum_metaphor'] = 1\n",
    "sum(df_test['sum_metaphor'])\n",
    "#len(df_test['sum_metaphor'])\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"sciencearticle_line.txt\", \"r\") as testset:\n",
    "    filtered_words = []\n",
    "    testmeta = []\n",
    "    testmeta1 = []\n",
    "    word_list = ''\n",
    "    for line in testset:\n",
    "        word_list = line.split()\n",
    "        filtered_words = ' '.join([word.lower() for word in word_list if word not in stopwords.words('english')])\n",
    "        testmeta1.append(filtered_words)\n",
    "df_test = pd.DataFrame(testmeta1)\n",
    "df_test\n",
    "# df_test = df_test.ix[:, 0:6]\n",
    "df_test.columns = ['sample0']\n",
    "df_test = df_test.fillna(\"\")\n",
    "# df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample0</th>\n",
       "      <th>metaphor0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>scientists created â€œatlas brainâ€ reveals meani...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>like colourful quilt laid cortex, atlas displa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>â€œour goal build giant atlas shows one specific...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no single brain region holds one word concept.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a single brain spot associated number related ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>and single word lights many different brain sp...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>together make networks represent meanings word...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>all light networks.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>described â€œtour de forceâ€ one researcher invol...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>with advances, technology could profound impac...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>â€œit possible approach could used decode inform...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>one potential use would language decoder could...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>to create atlas, scientists recorded peopleâ€™s ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>they matched transcripts stories brain activit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>huth used stories the moth radio hour short co...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>the enthralling stories, confident scientists ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>seven people listened two hours stories each.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>per person, amounted hearing roughly 25,000 wo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>the atlas shows words related terms exercise r...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>for example, left-hand side brain, ear, one ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>the region responds â€œkilledâ€, â€œconvictedâ€, â€œmu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>on brainâ€™s right-hand side, near top head, one...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>each word represented one spot words tend seve...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>one part brain, example, reliably responds wor...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>but word â€œtopâ€ activates many regions.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>one responds numbers measurements, another bui...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>the scientists created interactive website pub...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>strikingly, brain atlases similar participants...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>the scientists scanned five men two women, how...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>all native english speakers, two authors study...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>it highly possible people different background...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>armed atlas, researchers piece together brain ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>â€œthe idea murder represented lot brain,â€ galla...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>using haul data, group begun work new atlases ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>a brain atlas narrative structure far proved e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>â€œevery time come set narrative features, get t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>uri hasson, neuroscientist princeton universit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>unlike many studies looked brain activity isol...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>the next step, said, create comprehensive prec...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>ultimately, hasson believes possible reconstru...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>the ethical implications enormous.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>one benign use would see brain activity used a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>â€œthere many implications, barely touching surf...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>lorraine tyler, cognitive neuroscientist head ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>but brain atlas current form capture fine diff...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>it member many different groups, says tyler.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>â€œit something eat off, things made wood, thing...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>this kind detailed semantic information enable...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>â€œwhile research path-breaking scope, still lot...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sample0  metaphor0\n",
       "0   scientists created â€œatlas brainâ€ reveals meani...          1\n",
       "1   like colourful quilt laid cortex, atlas displa...          1\n",
       "2   â€œour goal build giant atlas shows one specific...          1\n",
       "3      no single brain region holds one word concept.          1\n",
       "4   a single brain spot associated number related ...          1\n",
       "5   and single word lights many different brain sp...          1\n",
       "6   together make networks represent meanings word...          1\n",
       "7                                 all light networks.          1\n",
       "8   described â€œtour de forceâ€ one researcher invol...          1\n",
       "9   with advances, technology could profound impac...          1\n",
       "10  â€œit possible approach could used decode inform...          1\n",
       "11  one potential use would language decoder could...          1\n",
       "12  to create atlas, scientists recorded peopleâ€™s ...          1\n",
       "13  they matched transcripts stories brain activit...          1\n",
       "14  huth used stories the moth radio hour short co...          1\n",
       "15  the enthralling stories, confident scientists ...          1\n",
       "16      seven people listened two hours stories each.          1\n",
       "17  per person, amounted hearing roughly 25,000 wo...          1\n",
       "18  the atlas shows words related terms exercise r...          1\n",
       "19  for example, left-hand side brain, ear, one ti...          1\n",
       "20  the region responds â€œkilledâ€, â€œconvictedâ€, â€œmu...          1\n",
       "21  on brainâ€™s right-hand side, near top head, one...          1\n",
       "22  each word represented one spot words tend seve...          1\n",
       "23  one part brain, example, reliably responds wor...          1\n",
       "24             but word â€œtopâ€ activates many regions.          1\n",
       "25  one responds numbers measurements, another bui...          1\n",
       "26  the scientists created interactive website pub...          1\n",
       "27  strikingly, brain atlases similar participants...          1\n",
       "28  the scientists scanned five men two women, how...          1\n",
       "29  all native english speakers, two authors study...          1\n",
       "30  it highly possible people different background...          1\n",
       "31  armed atlas, researchers piece together brain ...          1\n",
       "32  â€œthe idea murder represented lot brain,â€ galla...          1\n",
       "33  using haul data, group begun work new atlases ...          1\n",
       "34  a brain atlas narrative structure far proved e...          1\n",
       "35  â€œevery time come set narrative features, get t...          1\n",
       "36  uri hasson, neuroscientist princeton universit...          1\n",
       "37  unlike many studies looked brain activity isol...          1\n",
       "38  the next step, said, create comprehensive prec...          1\n",
       "39  ultimately, hasson believes possible reconstru...          1\n",
       "40                 the ethical implications enormous.          1\n",
       "41  one benign use would see brain activity used a...          1\n",
       "42  â€œthere many implications, barely touching surf...          1\n",
       "43  lorraine tyler, cognitive neuroscientist head ...          1\n",
       "44  but brain atlas current form capture fine diff...          1\n",
       "45       it member many different groups, says tyler.          1\n",
       "46  â€œit something eat off, things made wood, thing...          1\n",
       "47  this kind detailed semantic information enable...          1\n",
       "48  â€œwhile research path-breaking scope, still lot...          1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_test_feature_sparse = vec.transform(df_test['sample0'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "df_test['metaphor0'] = logreg_predictions\n",
    "\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample0</th>\n",
       "      <th>metaphor0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>scientists created â€œatlas brainâ€ reveals meani...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>like colourful quilt laid cortex, atlas displa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>â€œour goal build giant atlas shows one specific...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no single brain region holds one word concept.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a single brain spot associated number related ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sample0  metaphor0\n",
       "0  scientists created â€œatlas brainâ€ reveals meani...          1\n",
       "1  like colourful quilt laid cortex, atlas displa...          1\n",
       "2  â€œour goal build giant atlas shows one specific...          1\n",
       "3     no single brain region holds one word concept.          1\n",
       "4  a single brain spot associated number related ...          1"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.ix[:, 0:5].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_test.columns)\n",
    "#df_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"sciencearticle_line.txt\", \"r\") as testset:\n",
    "    filtered_words = []\n",
    "    testmeta = []\n",
    "    testmeta1 = []\n",
    "    word_list = ''\n",
    "    for line in testset:\n",
    "        word_list = line.split()\n",
    "        filtered_words = ' '.join([word.lower() for word in word_list if word not in stopwords.words('english')])\n",
    "        testmeta1.append(filtered_words)\n",
    "df_test = pd.DataFrame(testmeta1)\n",
    "df_test\n",
    "# df_test = df_test.ix[:, 0:6]\n",
    "df_test.columns = ['sample0']\n",
    "df_test = df_test.fillna(\"\")\n",
    "# df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk, re\n",
    "from nltk.corpus import brown\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_data_sets(sentences):\n",
    "    size = int(len(sentences) * 0.9)\n",
    "    train_sents = sentences[:size]\n",
    "    test_sents = sentences[size:]\n",
    "    return train_sents, test_sents\n",
    "\n",
    "def build_backoff_tagger (train_sents):\n",
    "    t0 = nltk.DefaultTagger('NN')\n",
    "    t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
    "    t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
    "    return t2\n",
    "\n",
    "\n",
    "def train_tagger(already_tagged_sents):\n",
    "    train_sents, test_sents = create_data_sets(already_tagged_sents)\n",
    "    ngram_tagger = build_backoff_tagger(train_sents)\n",
    "    print (\"%0.3f pos accuracy on test set\" % ngram_tagger.evaluate(test_sents))\n",
    "    return ngram_tagger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_tagger_on_brown():\n",
    "    brown_tagged_sents = brown.tagged_sents(categories=['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies',\n",
    "    'humor', 'learned', 'lore', 'mystery', 'religion', 'reviews', 'romance','science_fiction'])\n",
    "    return train_tagger(brown_tagged_sents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_text(corpus):\n",
    "    sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sents = sent_tokenizer.tokenize(corpus) # Split text into sentences    \n",
    "    return [nltk.word_tokenize(word) for word in raw_sents]\n",
    "\n",
    "def create_corpus(f):\n",
    "    with open(f, 'r') as text_file:\n",
    "        new_corpus = text_file.read()\n",
    "    return new_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.912 pos accuracy on test set\n"
     ]
    }
   ],
   "source": [
    "brown_tagger = train_tagger_on_brown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('scientists', 'NNS'),\n",
       "  ('have', 'HV'),\n",
       "  ('created', 'VBN'),\n",
       "  ('an', 'AT'),\n",
       "  ('atlas', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('brain', 'NN'),\n",
       "  ('that', 'CS'),\n",
       "  ('reveals', 'VBZ'),\n",
       "  ('how', 'WRB'),\n",
       "  ('the', 'AT'),\n",
       "  ('meanings', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('words', 'NNS'),\n",
       "  ('are', 'BER'),\n",
       "  ('arranged', 'VBN'),\n",
       "  ('across', 'IN'),\n",
       "  ('different', 'JJ'),\n",
       "  ('regions', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('organ', 'NN')],\n",
       " [('like', 'CS'),\n",
       "  ('a', 'AT'),\n",
       "  ('colourful', 'NN'),\n",
       "  ('quilt', 'NN'),\n",
       "  ('laid', 'VBN'),\n",
       "  ('over', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('cortex', 'NN'),\n",
       "  ('the', 'AT'),\n",
       "  ('atlas', 'NN'),\n",
       "  ('displays', 'NNS'),\n",
       "  ('in', 'IN'),\n",
       "  ('rainbow', 'NN'),\n",
       "  ('hues', 'NNS'),\n",
       "  ('how', 'WRB'),\n",
       "  ('individual', 'JJ'),\n",
       "  ('words', 'NNS'),\n",
       "  ('and', 'CC'),\n",
       "  ('the', 'AT'),\n",
       "  ('concepts', 'NNS'),\n",
       "  ('they', 'PPSS'),\n",
       "  ('convey', 'VB'),\n",
       "  ('can', 'MD'),\n",
       "  ('be', 'BE'),\n",
       "  ('grouped', 'VBN'),\n",
       "  ('together', 'RB'),\n",
       "  ('in', 'IN'),\n",
       "  ('clumps', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('white', 'JJ'),\n",
       "  ('matter', 'NN')],\n",
       " [('our', 'PP$'),\n",
       "  ('goal', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('to', 'TO'),\n",
       "  ('build', 'VB'),\n",
       "  ('a', 'AT'),\n",
       "  ('giant', 'JJ'),\n",
       "  ('atlas', 'NN'),\n",
       "  ('that', 'CS'),\n",
       "  ('shows', 'NNS'),\n",
       "  ('how', 'WRB'),\n",
       "  ('one', 'PN'),\n",
       "  ('specific', 'JJ'),\n",
       "  ('aspect', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('language', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('represented', 'VBN'),\n",
       "  ('in', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('brain', 'NN'),\n",
       "  ('in', 'IN'),\n",
       "  ('this', 'DT'),\n",
       "  ('case', 'NN'),\n",
       "  ('semantics', 'NN'),\n",
       "  ('or', 'CC'),\n",
       "  ('the', 'AT'),\n",
       "  ('meanings', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('words', 'NNS'),\n",
       "  ('said', 'VBD'),\n",
       "  ('jack', 'NN'),\n",
       "  ('gallant', 'JJ'),\n",
       "  ('a', 'AT'),\n",
       "  ('neuroscientist', 'NN'),\n",
       "  ('at', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('university', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('california', 'NN'),\n",
       "  ('berkeley', 'NN')],\n",
       " [('no', 'AT'),\n",
       "  ('single', 'AP'),\n",
       "  ('brain', 'NN'),\n",
       "  ('region', 'NN'),\n",
       "  ('holds', 'VBZ'),\n",
       "  ('one', 'CD'),\n",
       "  ('word', 'NN'),\n",
       "  ('or', 'CC'),\n",
       "  ('concept', 'NN')],\n",
       " [('a', 'AT'),\n",
       "  ('single', 'AP'),\n",
       "  ('brain', 'NN'),\n",
       "  ('spot', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('associated', 'VBN'),\n",
       "  ('with', 'IN'),\n",
       "  ('a', 'AT'),\n",
       "  ('number', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('related', 'VBN'),\n",
       "  ('words', 'NNS')],\n",
       " [('and', 'CC'),\n",
       "  ('each', 'DT'),\n",
       "  ('single', 'AP'),\n",
       "  ('word', 'NN'),\n",
       "  ('lights', 'NNS'),\n",
       "  ('up', 'RP'),\n",
       "  ('many', 'AP'),\n",
       "  ('different', 'JJ'),\n",
       "  ('brain', 'NN'),\n",
       "  ('spots', 'NNS')],\n",
       " [('together', 'RB'),\n",
       "  ('they', 'PPSS'),\n",
       "  ('make', 'VB'),\n",
       "  ('up', 'RP'),\n",
       "  ('networks', 'NNS'),\n",
       "  ('that', 'WPS'),\n",
       "  ('represent', 'VB'),\n",
       "  ('the', 'AT'),\n",
       "  ('meanings', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('each', 'DT'),\n",
       "  ('word', 'NN'),\n",
       "  ('we', 'PPSS'),\n",
       "  ('use', 'VB'),\n",
       "  ('life', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('love', 'NN'),\n",
       "  ('death', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('taxes', 'NNS'),\n",
       "  ('clouds', 'NNS'),\n",
       "  ('florida', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('bra', 'NN')],\n",
       " [('all', 'ABN'),\n",
       "  ('light', 'NN'),\n",
       "  ('up', 'RP'),\n",
       "  ('their', 'PP$'),\n",
       "  ('own', 'JJ'),\n",
       "  ('networks', 'NNS')],\n",
       " [('described', 'VBN'),\n",
       "  ('as', 'CS'),\n",
       "  ('a', 'AT'),\n",
       "  ('tour', 'NN'),\n",
       "  ('de', 'FW-IN'),\n",
       "  ('force', 'FW-NN'),\n",
       "  ('by', 'IN'),\n",
       "  ('one', 'CD'),\n",
       "  ('researcher', 'NN'),\n",
       "  ('who', 'WPS'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('not', '*'),\n",
       "  ('involved', 'VBN'),\n",
       "  ('in', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('study', 'NN'),\n",
       "  ('the', 'AT'),\n",
       "  ('atlas', 'NN'),\n",
       "  ('demonstrates', 'VBZ'),\n",
       "  ('how', 'WRB'),\n",
       "  ('modern', 'JJ'),\n",
       "  ('imaging', 'NN'),\n",
       "  ('can', 'MD'),\n",
       "  ('transform', 'VB'),\n",
       "  ('our', 'PP$'),\n",
       "  ('knowledge', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('how', 'WRB'),\n",
       "  ('the', 'AT'),\n",
       "  ('brain', 'NN'),\n",
       "  ('performs', 'VBZ'),\n",
       "  ('some', 'DTI'),\n",
       "  ('of', 'IN'),\n",
       "  ('its', 'PP$'),\n",
       "  ('most', 'QL'),\n",
       "  ('important', 'JJ'),\n",
       "  ('tasks', 'NNS')],\n",
       " [('with', 'IN'),\n",
       "  ('further', 'JJR'),\n",
       "  ('advances', 'NNS'),\n",
       "  ('the', 'AT'),\n",
       "  ('technology', 'NN'),\n",
       "  ('could', 'MD'),\n",
       "  ('have', 'HV'),\n",
       "  ('a', 'AT'),\n",
       "  ('profound', 'JJ'),\n",
       "  ('impact', 'NN'),\n",
       "  ('on', 'IN'),\n",
       "  ('medicine', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('other', 'AP'),\n",
       "  ('fields', 'NNS')],\n",
       " [('it', 'PPS'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('possible', 'JJ'),\n",
       "  ('that', 'CS'),\n",
       "  ('this', 'DT'),\n",
       "  ('approach', 'NN'),\n",
       "  ('could', 'MD'),\n",
       "  ('be', 'BE'),\n",
       "  ('used', 'VBN'),\n",
       "  ('to', 'TO'),\n",
       "  ('decode', 'NN'),\n",
       "  ('information', 'NN'),\n",
       "  ('about', 'IN'),\n",
       "  ('what', 'WDT'),\n",
       "  ('words', 'NNS'),\n",
       "  ('a', 'AT'),\n",
       "  ('person', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('hearing', 'NN'),\n",
       "  ('reading', 'NN'),\n",
       "  ('or', 'CC'),\n",
       "  ('possibly', 'RB'),\n",
       "  ('even', 'RB'),\n",
       "  ('thinking', 'VBG'),\n",
       "  ('said', 'VBD'),\n",
       "  ('alexander', 'NN'),\n",
       "  ('huth', 'NN'),\n",
       "  ('the', 'AT'),\n",
       "  ('first', 'OD'),\n",
       "  ('author', 'NN'),\n",
       "  ('on', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('study', 'NN')],\n",
       " [('one', 'CD'),\n",
       "  ('potential', 'JJ'),\n",
       "  ('use', 'NN'),\n",
       "  ('would', 'MD'),\n",
       "  ('be', 'BE'),\n",
       "  ('a', 'AT'),\n",
       "  ('language', 'NN'),\n",
       "  ('decoder', 'NN'),\n",
       "  ('that', 'CS'),\n",
       "  ('could', 'MD'),\n",
       "  ('allow', 'VB'),\n",
       "  ('people', 'NNS'),\n",
       "  ('silenced', 'VBN'),\n",
       "  ('by', 'IN'),\n",
       "  ('motor', 'NN'),\n",
       "  ('neurone', 'NN'),\n",
       "  ('disease', 'NN'),\n",
       "  ('or', 'CC'),\n",
       "  ('locked', 'VBN'),\n",
       "  ('in', 'IN'),\n",
       "  ('syndrome', 'NN'),\n",
       "  ('to', 'IN'),\n",
       "  ('speak', 'VB'),\n",
       "  ('through', 'IN'),\n",
       "  ('a', 'AT'),\n",
       "  ('computer', 'NN')],\n",
       " [('to', 'IN-HL'),\n",
       "  ('create', 'VB'),\n",
       "  ('the', 'AT'),\n",
       "  ('atlas', 'NN'),\n",
       "  ('the', 'AT'),\n",
       "  ('scientists', 'NNS'),\n",
       "  ('recorded', 'VBN'),\n",
       "  ('people', 'NNS'),\n",
       "  ('s', 'NN'),\n",
       "  ('brain', 'NN'),\n",
       "  ('activity', 'NN'),\n",
       "  ('while', 'CS'),\n",
       "  ('they', 'PPSS'),\n",
       "  ('listened', 'VBD'),\n",
       "  ('to', 'TO'),\n",
       "  ('stories', 'NNS'),\n",
       "  ('read', 'VB'),\n",
       "  ('out', 'RP'),\n",
       "  ('on', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('moth', 'NN'),\n",
       "  ('radio', 'NN'),\n",
       "  ('hour', 'NN'),\n",
       "  ('a', 'AT'),\n",
       "  ('us', 'PPO'),\n",
       "  ('radio', 'NN'),\n",
       "  ('show', 'NN')],\n",
       " [('they', 'PPSS'),\n",
       "  ('then', 'RB'),\n",
       "  ('matched', 'VBN'),\n",
       "  ('the', 'AT'),\n",
       "  ('transcripts', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('stories', 'NNS'),\n",
       "  ('with', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('brain', 'NN'),\n",
       "  ('activity', 'NN'),\n",
       "  ('data', 'NN'),\n",
       "  ('to', 'IN'),\n",
       "  ('show', 'NN'),\n",
       "  ('how', 'WRB'),\n",
       "  ('groups', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('related', 'VBN'),\n",
       "  ('words', 'NNS'),\n",
       "  ('triggered', 'VBN'),\n",
       "  ('neural', 'JJ'),\n",
       "  ('responses', 'NNS'),\n",
       "  ('in', 'IN'),\n",
       "  ('50', 'CD'),\n",
       "  ('000', 'NN'),\n",
       "  ('to', 'IN'),\n",
       "  ('80', 'CD'),\n",
       "  ('000', 'NN'),\n",
       "  ('pea', 'NN'),\n",
       "  ('sized', 'VBD'),\n",
       "  ('spots', 'NNS'),\n",
       "  ('all', 'ABN'),\n",
       "  ('over', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('cerebral', 'JJ'),\n",
       "  ('cortex', 'NN')],\n",
       " [('huth', 'NN'),\n",
       "  ('used', 'VBN'),\n",
       "  ('stories', 'NNS'),\n",
       "  ('from', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('moth', 'NN'),\n",
       "  ('radio', 'NN'),\n",
       "  ('hour', 'NN'),\n",
       "  ('because', 'CS'),\n",
       "  ('they', 'PPSS'),\n",
       "  ('are', 'BER'),\n",
       "  ('short', 'JJ'),\n",
       "  ('and', 'CC'),\n",
       "  ('compelling', 'JJ')],\n",
       " [('the', 'AT'),\n",
       "  ('more', 'QL'),\n",
       "  ('enthralling', 'JJ'),\n",
       "  ('the', 'AT'),\n",
       "  ('stories', 'NNS'),\n",
       "  ('the', 'AT'),\n",
       "  ('more', 'QL'),\n",
       "  ('confident', 'JJ'),\n",
       "  ('the', 'AT'),\n",
       "  ('scientists', 'NNS'),\n",
       "  ('could', 'MD'),\n",
       "  ('be', 'BE'),\n",
       "  ('that', 'CS'),\n",
       "  ('the', 'AT'),\n",
       "  ('people', 'NNS'),\n",
       "  ('being', 'BEG'),\n",
       "  ('scanned', 'VBD'),\n",
       "  ('were', 'BED'),\n",
       "  ('focusing', 'VBG'),\n",
       "  ('on', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('words', 'NNS'),\n",
       "  ('and', 'CC'),\n",
       "  ('not', '*'),\n",
       "  ('drifting', 'VBG'),\n",
       "  ('off', 'RP')],\n",
       " [('seven', 'CD'),\n",
       "  ('people', 'NNS'),\n",
       "  ('listened', 'VBD'),\n",
       "  ('to', 'TO'),\n",
       "  ('two', 'CD'),\n",
       "  ('hours', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('stories', 'NNS'),\n",
       "  ('each', 'DT')],\n",
       " [('per', 'IN'),\n",
       "  ('person', 'NN'),\n",
       "  ('that', 'CS'),\n",
       "  ('amounted', 'VBN'),\n",
       "  ('to', 'TO'),\n",
       "  ('hearing', 'NN'),\n",
       "  ('roughly', 'QL'),\n",
       "  ('25', 'CD'),\n",
       "  ('000', 'NN'),\n",
       "  ('words', 'NNS'),\n",
       "  ('and', 'CC'),\n",
       "  ('more', 'AP'),\n",
       "  ('than', 'IN'),\n",
       "  ('3', 'CD'),\n",
       "  ('000', 'NN'),\n",
       "  ('different', 'JJ'),\n",
       "  ('words', 'NNS'),\n",
       "  ('as', 'CS'),\n",
       "  ('they', 'PPSS'),\n",
       "  ('lay', 'VBD'),\n",
       "  ('in', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('scanner', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('atlas', 'NN'),\n",
       "  ('shows', 'VBZ'),\n",
       "  ('how', 'WRB'),\n",
       "  ('words', 'NNS'),\n",
       "  ('and', 'CC'),\n",
       "  ('related', 'VBN'),\n",
       "  ('terms', 'NNS'),\n",
       "  ('exercise', 'VB'),\n",
       "  ('the', 'AT'),\n",
       "  ('same', 'AP'),\n",
       "  ('regions', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('brain', 'NN')],\n",
       " [('for', 'CS'),\n",
       "  ('example', 'NN'),\n",
       "  ('on', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('left', 'NR'),\n",
       "  ('hand', 'NN'),\n",
       "  ('side', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('brain', 'NN'),\n",
       "  ('above', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('ear', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('one', 'CD'),\n",
       "  ('of', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('tiny', 'JJ'),\n",
       "  ('regions', 'NNS'),\n",
       "  ('that', 'WPS'),\n",
       "  ('represents', 'VBZ'),\n",
       "  ('the', 'AT'),\n",
       "  ('word', 'NN'),\n",
       "  ('victim', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('same', 'AP'),\n",
       "  ('region', 'NN'),\n",
       "  ('responds', 'VBZ'),\n",
       "  ('to', 'TO'),\n",
       "  ('killed', 'VBN'),\n",
       "  ('convicted', 'VBN'),\n",
       "  ('murdered', 'VBN'),\n",
       "  ('and', 'CC'),\n",
       "  ('confessed', 'VBD')],\n",
       " [('on', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('brain', 'NN'),\n",
       "  ('s', 'NN'),\n",
       "  ('right', 'QL'),\n",
       "  ('hand', 'NN'),\n",
       "  ('side', 'NN'),\n",
       "  ('near', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('top', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('head', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('one', 'CD'),\n",
       "  ('of', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('brain', 'NN'),\n",
       "  ('spots', 'NNS'),\n",
       "  ('activated', 'VBN'),\n",
       "  ('by', 'IN'),\n",
       "  ('family', 'NN'),\n",
       "  ('terms', 'NNS'),\n",
       "  ('wife', 'NN'),\n",
       "  ('husband', 'NN'),\n",
       "  ('children', 'NNS'),\n",
       "  ('parents', 'NNS')],\n",
       " [('each', 'DT'),\n",
       "  ('word', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('represented', 'VBN'),\n",
       "  ('by', 'IN'),\n",
       "  ('more', 'AP'),\n",
       "  ('than', 'IN'),\n",
       "  ('one', 'CD'),\n",
       "  ('spot', 'NN'),\n",
       "  ('because', 'CS'),\n",
       "  ('words', 'NNS'),\n",
       "  ('tend', 'VB'),\n",
       "  ('to', 'TO'),\n",
       "  ('have', 'HV'),\n",
       "  ('several', 'AP'),\n",
       "  ('meanings', 'NNS')],\n",
       " [('one', 'CD'),\n",
       "  ('part', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('brain', 'NN'),\n",
       "  ('for', 'IN'),\n",
       "  ('example', 'NN'),\n",
       "  ('reliably', 'NN'),\n",
       "  ('responds', 'VBZ'),\n",
       "  ('to', 'TO'),\n",
       "  ('the', 'AT'),\n",
       "  ('word', 'NN'),\n",
       "  ('top', 'NN'),\n",
       "  ('along', 'IN'),\n",
       "  ('with', 'IN'),\n",
       "  ('other', 'AP'),\n",
       "  ('words', 'NNS'),\n",
       "  ('that', 'WPS'),\n",
       "  ('describe', 'VB'),\n",
       "  ('clothing', 'NN')],\n",
       " [('but', 'CC'),\n",
       "  ('the', 'AT'),\n",
       "  ('word', 'NN'),\n",
       "  ('top', 'NN'),\n",
       "  ('activates', 'NN'),\n",
       "  ('many', 'AP'),\n",
       "  ('other', 'AP'),\n",
       "  ('regions', 'NNS')],\n",
       " [('one', 'CD'),\n",
       "  ('of', 'IN'),\n",
       "  ('them', 'PPO'),\n",
       "  ('responds', 'VBZ'),\n",
       "  ('to', 'TO'),\n",
       "  ('numbers', 'NNS'),\n",
       "  ('and', 'CC'),\n",
       "  ('measurements', 'NNS'),\n",
       "  ('another', 'DT'),\n",
       "  ('to', 'IN'),\n",
       "  ('buildings', 'NNS'),\n",
       "  ('and', 'CC'),\n",
       "  ('places', 'NNS')],\n",
       " [('the', 'AT'),\n",
       "  ('scientists', 'NNS'),\n",
       "  ('have', 'HV'),\n",
       "  ('created', 'VBN'),\n",
       "  ('an', 'AT'),\n",
       "  ('interactive', 'NN'),\n",
       "  ('website', 'NN'),\n",
       "  ('where', 'WRB'),\n",
       "  ('the', 'AT'),\n",
       "  ('public', 'JJ'),\n",
       "  ('can', 'MD'),\n",
       "  ('explore', 'VB'),\n",
       "  ('the', 'AT'),\n",
       "  ('brain', 'NN'),\n",
       "  ('atlas', 'NN')],\n",
       " [('strikingly', 'RB'),\n",
       "  ('the', 'AT'),\n",
       "  ('brain', 'NN'),\n",
       "  ('atlases', 'NN'),\n",
       "  ('were', 'BED'),\n",
       "  ('similar', 'JJ'),\n",
       "  ('for', 'IN'),\n",
       "  ('all', 'ABN'),\n",
       "  ('the', 'AT'),\n",
       "  ('participants', 'NNS'),\n",
       "  ('suggesting', 'VBG'),\n",
       "  ('that', 'CS'),\n",
       "  ('their', 'PP$'),\n",
       "  ('brains', 'NNS'),\n",
       "  ('organised', 'VBD'),\n",
       "  ('the', 'AT'),\n",
       "  ('meanings', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('words', 'NNS'),\n",
       "  ('in', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('same', 'AP'),\n",
       "  ('way', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('scientists', 'NNS'),\n",
       "  ('only', 'RB'),\n",
       "  ('scanned', 'VBD'),\n",
       "  ('five', 'CD'),\n",
       "  ('men', 'NNS'),\n",
       "  ('and', 'CC'),\n",
       "  ('two', 'CD'),\n",
       "  ('women', 'NNS'),\n",
       "  ('however', 'RB')],\n",
       " [('all', 'ABN'),\n",
       "  ('are', 'BER'),\n",
       "  ('native', 'JJ'),\n",
       "  ('english', 'NN'),\n",
       "  ('speakers', 'NNS'),\n",
       "  ('and', 'CC'),\n",
       "  ('two', 'CD'),\n",
       "  ('are', 'BER'),\n",
       "  ('authors', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('study', 'NN'),\n",
       "  ('published', 'VBN'),\n",
       "  ('in', 'IN'),\n",
       "  ('nature', 'NN')],\n",
       " [('it', 'PPS'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('highly', 'QL'),\n",
       "  ('possible', 'JJ'),\n",
       "  ('that', 'CS'),\n",
       "  ('people', 'NNS'),\n",
       "  ('from', 'IN'),\n",
       "  ('different', 'JJ'),\n",
       "  ('backgrounds', 'NNS'),\n",
       "  ('and', 'CC'),\n",
       "  ('cultures', 'NNS'),\n",
       "  ('will', 'MD'),\n",
       "  ('have', 'HV'),\n",
       "  ('different', 'JJ'),\n",
       "  ('semantic', 'JJ'),\n",
       "  ('brain', 'NN'),\n",
       "  ('atlases', 'NN')],\n",
       " [('armed', 'VBN'),\n",
       "  ('with', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('atlas', 'NN'),\n",
       "  ('researchers', 'NNS'),\n",
       "  ('can', 'MD'),\n",
       "  ('now', 'RB'),\n",
       "  ('piece', 'NN'),\n",
       "  ('together', 'RB'),\n",
       "  ('the', 'AT'),\n",
       "  ('brain', 'NN'),\n",
       "  ('networks', 'NNS'),\n",
       "  ('that', 'WPS'),\n",
       "  ('represent', 'VB'),\n",
       "  ('wildly', 'RB'),\n",
       "  ('different', 'JJ'),\n",
       "  ('concepts', 'NNS'),\n",
       "  ('from', 'IN'),\n",
       "  ('numbers', 'NNS'),\n",
       "  ('to', 'TO'),\n",
       "  ('murder', 'VB'),\n",
       "  ('and', 'CC'),\n",
       "  ('religion', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('idea', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('murder', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('represented', 'VBN'),\n",
       "  ('a', 'AT'),\n",
       "  ('lot', 'NN'),\n",
       "  ('in', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('brain', 'NN'),\n",
       "  ('gallant', 'JJ'),\n",
       "  ('said', 'VBD')],\n",
       " [('using', 'VBG'),\n",
       "  ('the', 'AT'),\n",
       "  ('same', 'AP'),\n",
       "  ('haul', 'VB'),\n",
       "  ('of', 'IN'),\n",
       "  ('data', 'NN'),\n",
       "  ('the', 'AT'),\n",
       "  ('group', 'NN'),\n",
       "  ('has', 'HVZ'),\n",
       "  ('begun', 'VBN'),\n",
       "  ('work', 'NN'),\n",
       "  ('on', 'IN'),\n",
       "  ('new', 'JJ'),\n",
       "  ('atlases', 'NN'),\n",
       "  ('that', 'CS'),\n",
       "  ('show', 'VB'),\n",
       "  ('how', 'WRB'),\n",
       "  ('the', 'AT'),\n",
       "  ('brain', 'NN'),\n",
       "  ('holds', 'VBZ'),\n",
       "  ('information', 'NN'),\n",
       "  ('on', 'IN'),\n",
       "  ('other', 'AP'),\n",
       "  ('aspects', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('language', 'NN'),\n",
       "  ('from', 'IN'),\n",
       "  ('phonemes', 'NNS'),\n",
       "  ('to', 'TO'),\n",
       "  ('syntax', 'NN')],\n",
       " [('a', 'AT'),\n",
       "  ('brain', 'NN'),\n",
       "  ('atlas', 'NN'),\n",
       "  ('for', 'IN'),\n",
       "  ('narrative', 'NN'),\n",
       "  ('structure', 'NN'),\n",
       "  ('has', 'HVZ'),\n",
       "  ('so', 'QL'),\n",
       "  ('far', 'RB'),\n",
       "  ('proved', 'VBD'),\n",
       "  ('elusive', 'JJ'),\n",
       "  ('however', 'WRB')],\n",
       " [('every', 'AT'),\n",
       "  ('time', 'NN'),\n",
       "  ('we', 'PPSS'),\n",
       "  ('come', 'VB'),\n",
       "  ('up', 'RP'),\n",
       "  ('with', 'IN'),\n",
       "  ('a', 'AT'),\n",
       "  ('set', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('narrative', 'NN'),\n",
       "  ('features', 'NNS'),\n",
       "  ('we', 'PPSS'),\n",
       "  ('get', 'VB'),\n",
       "  ('told', 'VBD'),\n",
       "  ('they', 'PPSS'),\n",
       "  ('aren', 'NN'),\n",
       "  ('t', 'NN'),\n",
       "  ('the', 'AT'),\n",
       "  ('right', 'NN'),\n",
       "  ('set', 'VBN'),\n",
       "  ('of', 'IN'),\n",
       "  ('narrative', 'NN'),\n",
       "  ('features', 'NNS'),\n",
       "  ('said', 'VBD'),\n",
       "  ('gallant', 'JJ')],\n",
       " [('uri', 'NN'),\n",
       "  ('hasson', 'NN'),\n",
       "  ('a', 'AT'),\n",
       "  ('neuroscientist', 'NN'),\n",
       "  ('at', 'IN'),\n",
       "  ('princeton', 'NN'),\n",
       "  ('university', 'NN'),\n",
       "  ('praised', 'VBD'),\n",
       "  ('the', 'AT'),\n",
       "  ('work', 'NN')],\n",
       " [('unlike', 'IN'),\n",
       "  ('many', 'AP'),\n",
       "  ('studies', 'NNS'),\n",
       "  ('that', 'WPS'),\n",
       "  ('looked', 'VBD'),\n",
       "  ('at', 'IN'),\n",
       "  ('brain', 'NN'),\n",
       "  ('activity', 'NN'),\n",
       "  ('when', 'WRB'),\n",
       "  ('an', 'AT'),\n",
       "  ('isolated', 'VBN'),\n",
       "  ('word', 'NN'),\n",
       "  ('or', 'CC'),\n",
       "  ('sentence', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('spoken', 'VBN'),\n",
       "  ('gallant', 'JJ'),\n",
       "  ('s', 'NN'),\n",
       "  ('team', 'NN'),\n",
       "  ('had', 'HVD'),\n",
       "  ('shed', 'NN'),\n",
       "  ('light', 'NN'),\n",
       "  ('on', 'IN'),\n",
       "  ('how', 'WRB'),\n",
       "  ('the', 'AT'),\n",
       "  ('brain', 'NN'),\n",
       "  ('worked', 'VBD'),\n",
       "  ('in', 'IN'),\n",
       "  ('a', 'AT'),\n",
       "  ('real', 'JJ'),\n",
       "  ('world', 'NN'),\n",
       "  ('scenario', 'NN'),\n",
       "  ('he', 'PPS'),\n",
       "  ('said', 'VBD')],\n",
       " [('the', 'AT'),\n",
       "  ('next', 'AP'),\n",
       "  ('step', 'NN'),\n",
       "  ('he', 'PPS'),\n",
       "  ('said', 'VBD'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('to', 'TO'),\n",
       "  ('create', 'VB'),\n",
       "  ('a', 'AT'),\n",
       "  ('more', 'QL'),\n",
       "  ('comprehensive', 'JJ'),\n",
       "  ('and', 'CC'),\n",
       "  ('precise', 'JJ'),\n",
       "  ('semantic', 'JJ'),\n",
       "  ('brain', 'NN'),\n",
       "  ('atlas', 'NN')],\n",
       " [('ultimately', 'RB'),\n",
       "  ('hasson', 'NN'),\n",
       "  ('believes', 'VBZ'),\n",
       "  ('it', 'PPO'),\n",
       "  ('will', 'MD'),\n",
       "  ('be', 'BE'),\n",
       "  ('possible', 'JJ'),\n",
       "  ('to', 'TO'),\n",
       "  ('reconstruct', 'VB'),\n",
       "  ('the', 'AT'),\n",
       "  ('words', 'NNS'),\n",
       "  ('a', 'AT'),\n",
       "  ('person', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('thinking', 'VBG'),\n",
       "  ('from', 'IN'),\n",
       "  ('their', 'PP$'),\n",
       "  ('brain', 'NN'),\n",
       "  ('activity', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('ethical', 'JJ'),\n",
       "  ('implications', 'NNS'),\n",
       "  ('are', 'BER'),\n",
       "  ('enormous', 'JJ')],\n",
       " [('one', 'CD'),\n",
       "  ('more', 'AP'),\n",
       "  ('benign', 'JJ'),\n",
       "  ('use', 'NN'),\n",
       "  ('would', 'MD'),\n",
       "  ('see', 'VB'),\n",
       "  ('brain', 'NN'),\n",
       "  ('activity', 'NN'),\n",
       "  ('used', 'VBN'),\n",
       "  ('to', 'TO'),\n",
       "  ('assess', 'VB'),\n",
       "  ('whether', 'CS'),\n",
       "  ('political', 'JJ'),\n",
       "  ('messages', 'NNS'),\n",
       "  ('have', 'HV'),\n",
       "  ('been', 'BEN'),\n",
       "  ('effectively', 'RB'),\n",
       "  ('communicated', 'VBN'),\n",
       "  ('to', 'TO'),\n",
       "  ('the', 'AT'),\n",
       "  ('public', 'JJ')],\n",
       " [('there', 'EX'),\n",
       "  ('are', 'BER'),\n",
       "  ('so', 'QL'),\n",
       "  ('many', 'AP'),\n",
       "  ('implications', 'NNS'),\n",
       "  ('and', 'CC'),\n",
       "  ('we', 'PPSS'),\n",
       "  ('are', 'BER'),\n",
       "  ('barely', 'RB'),\n",
       "  ('touching', 'VBG'),\n",
       "  ('the', 'AT'),\n",
       "  ('surface', 'NN'),\n",
       "  ('he', 'PPS'),\n",
       "  ('said', 'VBD')],\n",
       " [('lorraine', 'NN'),\n",
       "  ('tyler', 'NN'),\n",
       "  ('a', 'AT'),\n",
       "  ('cognitive', 'JJ'),\n",
       "  ('neuroscientist', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('head', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('centre', 'NN'),\n",
       "  ('for', 'IN'),\n",
       "  ('speech', 'NN'),\n",
       "  ('language', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('the', 'AT'),\n",
       "  ('brain', 'NN'),\n",
       "  ('at', 'IN'),\n",
       "  ('cambridge', 'NN'),\n",
       "  ('university', 'NN'),\n",
       "  ('said', 'VBD'),\n",
       "  ('the', 'AT'),\n",
       "  ('research', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('tour', 'NN'),\n",
       "  ('de', 'FW-IN'),\n",
       "  ('force', 'FW-NN'),\n",
       "  ('in', 'IN'),\n",
       "  ('its', 'PP$'),\n",
       "  ('scope', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('methods', 'NNS')],\n",
       " [('but', 'CC'),\n",
       "  ('the', 'AT'),\n",
       "  ('brain', 'NN'),\n",
       "  ('atlas', 'NN'),\n",
       "  ('in', 'IN'),\n",
       "  ('its', 'PP$'),\n",
       "  ('current', 'JJ'),\n",
       "  ('form', 'NN'),\n",
       "  ('does', 'DOZ'),\n",
       "  ('not', '*'),\n",
       "  ('capture', 'VB'),\n",
       "  ('fine', 'JJ'),\n",
       "  ('differences', 'NNS'),\n",
       "  ('in', 'IN'),\n",
       "  ('word', 'NN'),\n",
       "  ('meanings', 'NNS'),\n",
       "  ('take', 'VB'),\n",
       "  ('the', 'AT'),\n",
       "  ('word', 'NN'),\n",
       "  ('table', 'NN')],\n",
       " [('it', 'PPS'),\n",
       "  ('can', 'MD'),\n",
       "  ('be', 'BE'),\n",
       "  ('a', 'AT'),\n",
       "  ('member', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('many', 'AP'),\n",
       "  ('different', 'JJ'),\n",
       "  ('groups', 'NNS'),\n",
       "  ('says', 'VBZ'),\n",
       "  ('tyler', 'NN')],\n",
       " [('it', 'PPS'),\n",
       "  ('can', 'MD'),\n",
       "  ('be', 'BE'),\n",
       "  ('something', 'PN'),\n",
       "  ('to', 'TO'),\n",
       "  ('eat', 'VB'),\n",
       "  ('off', 'RP'),\n",
       "  ('things', 'NNS'),\n",
       "  ('made', 'VBN'),\n",
       "  ('of', 'IN'),\n",
       "  ('wood', 'NN'),\n",
       "  ('things', 'NNS'),\n",
       "  ('that', 'WPS'),\n",
       "  ('are', 'BER'),\n",
       "  ('heavy', 'JJ'),\n",
       "  ('things', 'NNS'),\n",
       "  ('having', 'HVG'),\n",
       "  ('four', 'CD'),\n",
       "  ('legs', 'NNS'),\n",
       "  ('non', 'FW-*-TL'),\n",
       "  ('animate', 'JJ'),\n",
       "  ('objects', 'NNS'),\n",
       "  ('and', 'CC'),\n",
       "  ('so', 'RB'),\n",
       "  ('on', 'IN')],\n",
       " [('this', 'DT'),\n",
       "  ('kind', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('detailed', 'VBN'),\n",
       "  ('semantic', 'JJ'),\n",
       "  ('information', 'NN'),\n",
       "  ('that', 'CS'),\n",
       "  ('enables', 'VBZ'),\n",
       "  ('words', 'NNS'),\n",
       "  ('to', 'TO'),\n",
       "  ('be', 'BE'),\n",
       "  ('used', 'VBN'),\n",
       "  ('flexibly', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('lost', 'VBN'),\n",
       "  ('in', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('analysis', 'NN'),\n",
       "  ('she', 'PPS'),\n",
       "  ('said', 'VBD')],\n",
       " [('while', 'CS'),\n",
       "  ('this', 'DT'),\n",
       "  ('research', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('path', 'NN'),\n",
       "  ('breaking', 'VBG'),\n",
       "  ('in', 'IN'),\n",
       "  ('its', 'PP$'),\n",
       "  ('scope', 'NN'),\n",
       "  ('there', 'EX'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('still', 'RB'),\n",
       "  ('a', 'AT'),\n",
       "  ('lot', 'NN'),\n",
       "  ('to', 'IN'),\n",
       "  ('learn', 'VB'),\n",
       "  ('about', 'IN'),\n",
       "  ('how', 'WRB'),\n",
       "  ('semantics', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('represented', 'VBN'),\n",
       "  ('in', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('brain', 'NN')]]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"sciencearticle_line.txt\", \"r\") as testset:\n",
    "    filtered_words = []\n",
    "    testmeta = []\n",
    "    testmeta1 = []\n",
    "    word_list = ''\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    for line in testset:\n",
    "        word_list = line.split()\n",
    "#         filtered_words = ' '.join([word.lower() for word in word_list if word not in stopwords.words('english')])\n",
    "        filtered_words = ' '.join([word.lower() for word in word_list])\n",
    "        testmeta1.append(tokenizer.tokenize(filtered_words))\n",
    "testmeta1[0]\n",
    "\n",
    "def get_pos(sents, tagger):        \n",
    "    return [tagger.tag(sent) for sent in sents]\n",
    "\n",
    "pos = get_pos(testmeta1, brown_tagger)\n",
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_tagger_on_brown_augmented_with_cooking_sents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-09a1d4f969b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrain_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_tagged_sents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mbrown_and_additional_tagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_tagger_on_brown_augmented_with_cooking_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_tagger_on_brown_augmented_with_cooking_sents' is not defined"
     ]
    }
   ],
   "source": [
    "def train_tagger_on_brown_augmented_with_additional_sents():\n",
    "\n",
    "    additional_sents = [[('colorful', 'JJ'), ('quilt', 'NN')],\n",
    "                        [('regions', 'NN'), ('represent', 'VB'), ('world', 'NN')],\n",
    "                        [('public', 'NN'), ('explore', 'VB'), ('brain', 'NN')]]\n",
    "\n",
    "\n",
    "    brown_tagged_sents = brown.tagged_sents(categories=['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies',\n",
    "    'humor', 'learned', 'lore', 'mystery', 'religion', 'reviews', 'romance', 'science_fiction'])\n",
    "    \n",
    "    #append hand-tagged cooking sentences to the front of the training data\n",
    "    all_tagged_sents = additional_sents + brown_tagged_sents\n",
    "    return train_tagger(all_tagged_sents)\n",
    "\n",
    "brown_and_additional_tagger = train_tagger_on_brown_augmented_with_cooking_sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'brown_and_additional_tagger' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-ade19e876bc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbetter_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_pos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestmeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbrown_and_additional_tagger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'brown_and_additional_tagger' is not defined"
     ]
    }
   ],
   "source": [
    "better_sentences = get_pos(testmeta1, brown_and_additional_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('the', 'AT'),\n",
       "  ('snow', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('white', 'JJ'),\n",
       "  ('blanket', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('hospital', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('refrigerator', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('classroom', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('zoo', 'NN')],\n",
       " [('america', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('melting', 'VBG'),\n",
       "  ('pot', 'NN')],\n",
       " [('her', 'PP$'),\n",
       "  ('lovely', 'JJ'),\n",
       "  ('voice', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('music', 'NN'),\n",
       "  ('to', 'IN'),\n",
       "  ('his', 'PP$'),\n",
       "  ('ears', 'NNS')],\n",
       " [('life', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('roller', 'NN'),\n",
       "  ('coaster', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('alligator', 'NN'),\n",
       "  ('s', 'NN'),\n",
       "  ('teeth', 'NNS'),\n",
       "  ('are', 'BER'),\n",
       "  ('white', 'JJ'),\n",
       "  ('daggers', 'NN')],\n",
       " [('their', 'PP$'),\n",
       "  ('home', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('prison', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('slide', 'NN'),\n",
       "  ('on', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('playground', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('hot', 'JJ'),\n",
       "  ('stove', 'NN')],\n",
       " [('his', 'PP$'),\n",
       "  ('heart', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('cold', 'JJ'),\n",
       "  ('iron', 'NN')],\n",
       " [('she', 'PPS'), ('is', 'BEZ'), ('a', 'AT'), ('peacock', 'NN')],\n",
       " [('he', 'PPS'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('shining', 'VBG'),\n",
       "  ('star', 'NN')],\n",
       " [('time', 'NN'), ('is', 'BEZ'), ('money', 'NN')],\n",
       " [('my', 'PP$'),\n",
       "  ('teacher', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('dragon', 'NN')],\n",
       " [('tom', 'NN'), ('s', 'NN'), ('eyes', 'NNS'), ('were', 'BED'), ('ice', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('detective', 'NN'),\n",
       "  ('s', 'NN'),\n",
       "  ('face', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('wood', 'NN'),\n",
       "  ('as', 'CS'),\n",
       "  ('he', 'PPS'),\n",
       "  ('listened', 'VBD'),\n",
       "  ('to', 'TO'),\n",
       "  ('her', 'PPO'),\n",
       "  ('story', 'NN')],\n",
       " [('she', 'PPS'),\n",
       "  ('feels', 'VBZ'),\n",
       "  ('that', 'CS'),\n",
       "  ('life', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('fashion', 'NN'),\n",
       "  ('show', 'NN')],\n",
       " [('the', 'AT'), ('world', 'NN'), ('is', 'BEZ'), ('a', 'AT'), ('stage', 'NN')],\n",
       " [('my', 'PP$'),\n",
       "  ('kid', 'NN'),\n",
       "  ('s', 'NN'),\n",
       "  ('room', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('disaster', 'NN'),\n",
       "  ('area', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('children', 'NNS'),\n",
       "  ('were', 'BED'),\n",
       "  ('flowers', 'NNS'),\n",
       "  ('grown', 'VBN'),\n",
       "  ('in', 'IN'),\n",
       "  ('concrete', 'NN'),\n",
       "  ('gardens', 'NNS')],\n",
       " [('kisses', 'NNS'),\n",
       "  ('are', 'BER'),\n",
       "  ('the', 'AT'),\n",
       "  ('flowers', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('affection', 'NN')],\n",
       " [('his', 'PP$'),\n",
       "  ('words', 'NNS'),\n",
       "  ('were', 'BED'),\n",
       "  ('cotton', 'NN'),\n",
       "  ('candy', 'NN')],\n",
       " [('mary', 'NN'),\n",
       "  ('s', 'NN'),\n",
       "  ('eyes', 'NNS'),\n",
       "  ('were', 'BED'),\n",
       "  ('fireflies', 'NN')],\n",
       " [('john', 'NN'),\n",
       "  ('s', 'NN'),\n",
       "  ('suggestion', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('just', 'RB'),\n",
       "  ('a', 'AT'),\n",
       "  ('band', 'NN'),\n",
       "  ('aid', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('cast', 'NN'),\n",
       "  ('on', 'IN'),\n",
       "  ('his', 'PP$'),\n",
       "  ('broken', 'VBN'),\n",
       "  ('leg', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('plaster', 'NN'),\n",
       "  ('shackle', 'NN')],\n",
       " [('jane', 'NN'),\n",
       "  ('s', 'NN'),\n",
       "  ('ambitions', 'NNS'),\n",
       "  ('are', 'BER'),\n",
       "  ('a', 'AT'),\n",
       "  ('house', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('cards', 'NNS')],\n",
       " [('her', 'PP$'),\n",
       "  ('long', 'JJ'),\n",
       "  ('hair', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('flowing', 'VBG'),\n",
       "  ('golden', 'JJ'),\n",
       "  ('river', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('computers', 'NNS'),\n",
       "  ('at', 'IN'),\n",
       "  ('school', 'NN'),\n",
       "  ('are', 'BER'),\n",
       "  ('old', 'JJ'),\n",
       "  ('dinosaurs', 'NNS')],\n",
       " [('laughter', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('the', 'AT'),\n",
       "  ('music', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('soul', 'NN')],\n",
       " [('he', 'PPS'), ('is', 'BEZ'), ('a', 'AT'), ('night', 'NN'), ('owl', 'NN')],\n",
       " [('maria', 'FW-NNS'), ('is', 'BEZ'), ('a', 'AT'), ('chicken', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('falling', 'VBG'),\n",
       "  ('snowflakes', 'NNS'),\n",
       "  ('are', 'BER'),\n",
       "  ('dancers', 'NNS')],\n",
       " [('with', 'IN'),\n",
       "  ('his', 'PP$'),\n",
       "  ('new', 'JJ'),\n",
       "  ('haircut', 'NN'),\n",
       "  ('he', 'PPS'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('sheepdog', 'NN')],\n",
       " [('at', 'IN'),\n",
       "  ('five', 'CD'),\n",
       "  ('o', 'NN'),\n",
       "  ('clock', 'NN'),\n",
       "  ('the', 'AT'),\n",
       "  ('interstate', 'JJ'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('parking', 'VBG'),\n",
       "  ('lot', 'NN')],\n",
       " [('books', 'NNS'),\n",
       "  ('are', 'BER'),\n",
       "  ('keys', 'NNS'),\n",
       "  ('to', 'TO'),\n",
       "  ('your', 'PP$'),\n",
       "  ('imagination', 'NN')],\n",
       " [('her', 'PP$'),\n",
       "  ('teddy', 'NN'),\n",
       "  ('bear', 'VB'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('her', 'PP$'),\n",
       "  ('best', 'JJT'),\n",
       "  ('friend', 'NN'),\n",
       "  ('never', 'RB'),\n",
       "  ('telling', 'VBG'),\n",
       "  ('her', 'PP$'),\n",
       "  ('secrets', 'NNS')],\n",
       " [('the', 'AT'),\n",
       "  ('peaceful', 'JJ'),\n",
       "  ('lake', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('mirror', 'NN')],\n",
       " [('terry', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('blue', 'JJ'),\n",
       "  ('when', 'WRB'),\n",
       "  ('his', 'PP$'),\n",
       "  ('goldfish', 'NN'),\n",
       "  ('died', 'VBD')],\n",
       " [('the', 'AT'),\n",
       "  ('wind', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('an', 'AT'),\n",
       "  ('angry', 'JJ'),\n",
       "  ('witch', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('ballerina', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('swan', 'NN'),\n",
       "  ('gliding', 'NN'),\n",
       "  ('across', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('stage', 'NN')],\n",
       " [('her', 'PP$'),\n",
       "  ('angry', 'JJ'),\n",
       "  ('words', 'NNS'),\n",
       "  ('were', 'BED'),\n",
       "  ('bullets', 'NNS'),\n",
       "  ('to', 'TO'),\n",
       "  ('him', 'PPO')],\n",
       " [('your', 'PP$'),\n",
       "  ('brain', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('computer', 'NN')],\n",
       " [('jamal', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('pig', 'NN'),\n",
       "  ('at', 'IN'),\n",
       "  ('dinner', 'NN')],\n",
       " [('you', 'PPSS'), ('are', 'BER'), ('my', 'PP$'), ('sunshine', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('car', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('furnace', 'NN'),\n",
       "  ('in', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('sun', 'NN')],\n",
       " [('thank', 'VB'),\n",
       "  ('you', 'PPO'),\n",
       "  ('so', 'QL'),\n",
       "  ('much', 'AP'),\n",
       "  ('you', 'PPSS'),\n",
       "  ('are', 'BER'),\n",
       "  ('an', 'AT'),\n",
       "  ('angel', 'NN')],\n",
       " [('that', 'DT'),\n",
       "  ('coach', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('an', 'AT'),\n",
       "  ('ogre', 'NN')],\n",
       " [('ben', 'NN'),\n",
       "  ('s', 'NN'),\n",
       "  ('temper', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('volcano', 'NN'),\n",
       "  ('ready', 'JJ'),\n",
       "  ('to', 'TO'),\n",
       "  ('explode', 'VB')],\n",
       " [('the', 'AT'),\n",
       "  ('kids', 'NNS'),\n",
       "  ('were', 'BED'),\n",
       "  ('monkeys', 'NNS'),\n",
       "  ('on', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('jungle', 'NN'),\n",
       "  ('gym', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('sun', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('golden', 'JJ'),\n",
       "  ('ball', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('clouds', 'NNS'),\n",
       "  ('are', 'BER'),\n",
       "  ('balls', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('cotton', 'NN')],\n",
       " [('sue', 'VB'),\n",
       "  ('s', 'NN'),\n",
       "  ('room', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('zoo', 'NN'),\n",
       "  ('with', 'IN'),\n",
       "  ('fish', 'NN'),\n",
       "  ('a', 'AT'),\n",
       "  ('gerbil', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('a', 'AT'),\n",
       "  ('parakeet', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('park', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('lake', 'NN'),\n",
       "  ('after', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('rain', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('lightning', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('fireworks', 'NNS'),\n",
       "  ('in', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('sky', 'NN')],\n",
       " [('gary', 'NN'), ('is', 'BEZ'), ('a', 'AT'), ('mule', 'NN')],\n",
       " [('that', 'DT'),\n",
       "  ('lawn', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('green', 'JJ'),\n",
       "  ('carpet', 'NN')],\n",
       " [('my', 'PP$'),\n",
       "  ('dad', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('road', 'NN'),\n",
       "  ('hog', 'NNS')],\n",
       " [('the', 'AT'),\n",
       "  ('stars', 'NNS'),\n",
       "  ('are', 'BER'),\n",
       "  ('sparkling', 'VBG'),\n",
       "  ('diamonds', 'NNS')],\n",
       " [('those', 'DTS'),\n",
       "  ('two', 'CD'),\n",
       "  ('best', 'JJT'),\n",
       "  ('friends', 'NNS'),\n",
       "  ('are', 'BER'),\n",
       "  ('two', 'CD'),\n",
       "  ('peas', 'NNS'),\n",
       "  ('in', 'IN'),\n",
       "  ('a', 'AT'),\n",
       "  ('pod', 'NN')],\n",
       " [('he', 'PPS'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('walking', 'VBG'),\n",
       "  ('dictionary', 'NN')],\n",
       " [('donations', 'NN'),\n",
       "  ('for', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('popular', 'JJ'),\n",
       "  ('charity', 'NN'),\n",
       "  ('were', 'BED'),\n",
       "  ('a', 'AT'),\n",
       "  ('tsunami', 'NN')],\n",
       " [('necessity', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('the', 'AT'),\n",
       "  ('mother', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('invention', 'NN')],\n",
       " [('my', 'PP$'),\n",
       "  ('big', 'JJ'),\n",
       "  ('brother', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('couch', 'NN'),\n",
       "  ('potato', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('road', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('ribbon', 'NN'),\n",
       "  ('stretching', 'VBG'),\n",
       "  ('across', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('desert', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('teenager', 'NN'),\n",
       "  ('s', 'NN'),\n",
       "  ('stomach', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('bottomless', 'JJ'),\n",
       "  ('pit', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('thunder', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('mighty', 'JJ'),\n",
       "  ('lion', 'NN')],\n",
       " [('i', 'NN'),\n",
       "  ('am', 'BEM'),\n",
       "  ('so', 'QL'),\n",
       "  ('excited', 'VBN'),\n",
       "  ('my', 'PP$'),\n",
       "  ('pulse', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('race', 'NN'),\n",
       "  ('car', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('moon', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('white', 'JJ'),\n",
       "  ('balloon', 'NN')],\n",
       " [('toddlers', 'NNS'), ('are', 'BER'), ('rug', 'NN'), ('rats', 'NNS')],\n",
       " [('the', 'AT'),\n",
       "  ('stormy', 'JJ'),\n",
       "  ('ocean', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('raging', 'VBG'),\n",
       "  ('bull', 'NN')],\n",
       " [('her', 'PP$'),\n",
       "  ('tears', 'NNS'),\n",
       "  ('were', 'BED'),\n",
       "  ('a', 'AT'),\n",
       "  ('river', 'NN'),\n",
       "  ('flowing', 'VBG'),\n",
       "  ('down', 'RP'),\n",
       "  ('her', 'PP$'),\n",
       "  ('cheeks', 'NNS')]]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"test_meta_sent.txt\", \"r\") as testset:\n",
    "    filtered_words = []\n",
    "    testmeta = []\n",
    "    testmeta1 = []\n",
    "    word_list = ''\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    for line in testset:\n",
    "        word_list = line.split()\n",
    "#         filtered_words = ' '.join([word.lower() for word in word_list if word not in stopwords.words('english')])\n",
    "        filtered_words = ' '.join([word.lower() for word in word_list])\n",
    "        testmeta1.append(tokenizer.tokenize(filtered_words))\n",
    "testmeta1[0]\n",
    "\n",
    "def get_pos(sents, tagger):        \n",
    "    return [tagger.tag(sent) for sent in sents]\n",
    "\n",
    "pos = get_pos(testmeta1, brown_tagger)\n",
    "pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Switch to Classification Features with NLTK Classification Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import nltk\n",
    "from nltk.corpus import names\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('an_mets.txt', 'r')\n",
    "x = f.readlines()\n",
    "an_mets = [t.rstrip() for t in x[1::]]\n",
    "f = open('an_nonmets.txt', 'r')\n",
    "x = f.readlines()\n",
    "an_nonmets = [t.rstrip() for t in x[1::]]\n",
    "f = open('svo_mets.txt', 'r')\n",
    "x = f.readlines()\n",
    "svo_mets = [t.rstrip() for t in x[1::]]\n",
    "f = open('svo_nonmets.txt', 'r')\n",
    "x = f.readlines()\n",
    "svo_nonmets = [t.rstrip() for t in x[1::]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'POS': 'm'}\n"
     ]
    }
   ],
   "source": [
    "# def double_letter(word):\n",
    "#     letter_list = []\n",
    "#     for letter in word:\n",
    "#         if word.count(letter) > 1:\n",
    "#             letter_list.append(True)\n",
    "#         else:\n",
    "#             letter_list.append(False)\n",
    "#     if True in letter_list:\n",
    "#         return True\n",
    "#     else:\n",
    "#         return False\n",
    "    \n",
    "def metaphor_features(word):\n",
    "    features = {}\n",
    "    word = word.lower()\n",
    "    features['POS'] = word[0]\n",
    "#     features['last'] = word[-1]\n",
    "#     features['last 2'] = word[-2]\n",
    "#     features['first 3'] = word[:3]\n",
    "#     features['first'] = word[:1]\n",
    "#     features['length'] = len(word)\n",
    "#     features['starts with K'] = word.startswith('k')\n",
    "#     features['ends with i'] = word.endswith('i')\n",
    "#     features['ends with a'] = word.endswith('a')\n",
    "#     features['double letter'] = double_letter(word)\n",
    "    return features\n",
    "print(str(metaphor_features('Michelle')))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_an_data():\n",
    "    an_mets_tag = [(re.sub(r'[^\\w\\s]','',line.lower()), 'metaphor') for line in an_mets]\n",
    "    an_nonmets_tag = [(re.sub(r'[^\\w\\s]','',line.lower()), 'NOT metaphor') for line in an_nonmets]\n",
    "    svo_mets_tag = [(re.sub(r'[^\\w\\s]','',line.lower()), 'metaphor') for line in svo_mets]\n",
    "    svo_nonmets_tag = [(re.sub(r'[^\\w\\s]','',line.lower()), 'NOT metaphor') for line in svo_nonmets]\n",
    "    all_an = an_mets_tag + an_nonmets_tag + svo_mets_tag + svo_nonmets_tag\n",
    "    \n",
    "    # Randomize the order of male and female names, and de-alphabatize\n",
    "    random.shuffle(all_an)\n",
    "    return all_an\n",
    "\n",
    "all_an_data = create_an_data()\n",
    "# print(all_an_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function allows experimentation with different feature definitions\n",
    "# items is a list of (key, value) pairs from which features are extracted and training sets are made\n",
    "# Feature sets returned are dictionaries of features\n",
    "\n",
    "# This function also optionally returns the names of the training, development, \n",
    "# and test data for the purposes of error checking\n",
    "\n",
    "def create_training_sets (feature_function, items, return_items=False):\n",
    "    # Create the features sets.  Call the function that was passed in.\n",
    "    # For names data, key is the name, and value is the gender\n",
    "    featuresets = [(feature_function(key), value) for (key, value) in items]\n",
    "    \n",
    "    # Divided training and testing in thirds.  Could divide in other proportions instead.\n",
    "    third = int(float(len(featuresets)) / 3.0)\n",
    "    \n",
    "    train_set, dev_set, test_set = featuresets[0:third], featuresets[third:third*2], featuresets[third*2:]\n",
    "    train_items, dev_items, test_items = items[0:third], items[third:third*2], items[third*2:]\n",
    "    if return_items == True:\n",
    "        return train_set, dev_set, test_set, train_items, dev_items, test_items\n",
    "    else:\n",
    "        return train_set, dev_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# note that this passes in a function name as an argument (gender_features)\n",
    "\n",
    "train_set, dev_set, test_set = create_training_sets(metaphor_features, all_an_data)\n",
    "cl = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'white blanket': NOT metaphor\n",
      "'snow white': metaphor\n",
      "'white as snow': NOT metaphor\n",
      "'happy customer': NOT metaphor\n",
      "'boiling rage': NOT metaphor\n"
     ]
    }
   ],
   "source": [
    "print (\"'white blanket': \" + cl.classify(metaphor_features('white blanket')))\n",
    "print (\"'snow white': \" + cl.classify(metaphor_features('snow white')))\n",
    "print (\"'white as snow': \" + cl.classify(metaphor_features('white as snow')))\n",
    "print (\"'happy customer': \" + cl.classify(metaphor_features('happy customer')))\n",
    "print (\"'boiling rage': \" + cl.classify(metaphor_features('boiling rage')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one specific aspect: metaphor\n"
     ]
    }
   ],
   "source": [
    "testsample = 'one specific aspect'\n",
    "print (testsample + \": \" + cl.classify(metaphor_features(testsample)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.514\n",
      "Most Informative Features\n",
      "                     POS = 'p'            NOT me : metaph =      4.2 : 1.0\n",
      "                     POS = 'n'            NOT me : metaph =      2.2 : 1.0\n",
      "                     POS = 'd'            metaph : NOT me =      2.2 : 1.0\n",
      "                     POS = 'h'            NOT me : metaph =      2.1 : 1.0\n",
      "                     POS = 'r'            metaph : NOT me =      2.1 : 1.0\n",
      "                     POS = 's'            metaph : NOT me =      1.9 : 1.0\n",
      "                     POS = 'm'            NOT me : metaph =      1.8 : 1.0\n",
      "                     POS = 'g'            NOT me : metaph =      1.7 : 1.0\n",
      "                     POS = 'a'            metaph : NOT me =      1.7 : 1.0\n",
      "                     POS = 'l'            metaph : NOT me =      1.7 : 1.0\n",
      "                     POS = 'o'            metaph : NOT me =      1.7 : 1.0\n",
      "                     POS = 'i'            NOT me : metaph =      1.7 : 1.0\n",
      "                     POS = 'c'            metaph : NOT me =      1.6 : 1.0\n",
      "                     POS = 't'            metaph : NOT me =      1.6 : 1.0\n",
      "                     POS = 'b'            NOT me : metaph =      1.4 : 1.0\n"
     ]
    }
   ],
   "source": [
    "print (\"%.3f\" % nltk.classify.accuracy(cl, dev_set))\n",
    "cl.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['scientists created atlas', 'created atlas brain', 'atlas brain reveals', 'brain reveals meanings', 'reveals meanings words', 'meanings words arranged', 'words arranged across', 'arranged across different', 'across different regions', 'different regions organ'], ['like colourful quilt', 'colourful quilt laid', 'quilt laid cortex', 'laid cortex atlas', 'cortex atlas displays', 'atlas displays rainbow', 'displays rainbow hues', 'rainbow hues individual', 'hues individual words', 'individual words concepts', 'words concepts convey', 'concepts convey grouped', 'convey grouped together', 'grouped together clumps', 'together clumps white', 'clumps white matter'], ['our goal build', 'goal build giant', 'build giant atlas', 'giant atlas shows', 'atlas shows one', 'shows one specific', 'one specific aspect', 'specific aspect language', 'aspect language represented', 'language represented brain', 'represented brain case', 'brain case semantics', 'case semantics meanings', 'semantics meanings words', 'meanings words said', 'words said jack', 'said jack gallant', 'jack gallant neuroscientist', 'gallant neuroscientist university', 'neuroscientist university california', 'university california berkeley'], ['single brain region', 'brain region holds', 'region holds one', 'holds one word', 'one word concept'], ['single brain spot', 'brain spot associated', 'spot associated number', 'associated number related', 'number related words'], ['single word lights', 'word lights many', 'lights many different', 'many different brain', 'different brain spots'], ['together make networks', 'make networks represent', 'networks represent meanings', 'represent meanings word', 'meanings word use', 'word use life', 'use life love', 'life love death', 'love death taxes', 'death taxes clouds', 'taxes clouds florida', 'clouds florida bra'], [], ['described tour de', 'tour de force', 'de force one', 'force one researcher', 'one researcher involved', 'researcher involved study', 'involved study atlas', 'study atlas demonstrates', 'atlas demonstrates modern', 'demonstrates modern imaging', 'modern imaging transform', 'imaging transform knowledge', 'transform knowledge brain', 'knowledge brain performs', 'brain performs important', 'performs important tasks'], ['advances technology could', 'technology could profound', 'could profound impact', 'profound impact medicine', 'impact medicine fields'], ['it possible approach', 'possible approach could', 'approach could used', 'could used decode', 'used decode information', 'decode information words', 'information words person', 'words person hearing', 'person hearing reading', 'hearing reading possibly', 'reading possibly even', 'possibly even thinking', 'even thinking said', 'thinking said alexander', 'said alexander huth', 'alexander huth first', 'huth first author', 'first author study'], ['one potential use', 'potential use would', 'use would language', 'would language decoder', 'language decoder could', 'decoder could allow', 'could allow people', 'allow people silenced', 'people silenced motor', 'silenced motor neurone', 'motor neurone disease', 'neurone disease lockedin', 'disease lockedin syndrome', 'lockedin syndrome speak', 'syndrome speak computer'], ['create atlas scientists', 'atlas scientists recorded', 'scientists recorded peoples', 'recorded peoples brain', 'peoples brain activity', 'brain activity listened', 'activity listened stories', 'listened stories read', 'stories read moth', 'read moth radio', 'moth radio hour', 'radio hour us', 'hour us radio', 'us radio show'], ['matched transcripts stories', 'transcripts stories brain', 'stories brain activity', 'brain activity data', 'activity data show', 'data show groups', 'show groups related', 'groups related words', 'related words triggered', 'words triggered neural', 'triggered neural responses', 'neural responses 50000', 'responses 50000 80000', '50000 80000 peasized', '80000 peasized spots', 'peasized spots cerebral', 'spots cerebral cortex'], ['huth used stories', 'used stories moth', 'stories moth radio', 'moth radio hour', 'radio hour short', 'hour short compelling'], ['enthralling stories confident', 'stories confident scientists', 'confident scientists could', 'scientists could people', 'could people scanned', 'people scanned focusing', 'scanned focusing words', 'focusing words drifting', 'words drifting off'], ['seven people listened', 'people listened two', 'listened two hours', 'two hours stories', 'hours stories each'], ['per person amounted', 'person amounted hearing', 'amounted hearing roughly', 'hearing roughly 25000', 'roughly 25000 words', '25000 words 3000', 'words 3000 different', '3000 different words', 'different words ', 'words  lay', ' lay scanner'], ['atlas shows words', 'shows words related', 'words related terms', 'related terms exercise', 'terms exercise regions', 'exercise regions brain'], ['example lefthand side', 'lefthand side brain', 'side brain ear', 'brain ear one', 'ear one tiny', 'one tiny regions', 'tiny regions represents', 'regions represents word', 'represents word victim'], ['region responds killed', 'responds killed convicted', 'killed convicted murdered', 'convicted murdered confessed'], ['brains righthand side', 'righthand side near', 'side near top', 'near top head', 'top head one', 'head one brain', 'one brain spots', 'brain spots activated', 'spots activated family', 'activated family terms', 'family terms wife', 'terms wife husband', 'wife husband children', 'husband children parents'], ['word represented one', 'represented one spot', 'one spot words', 'spot words tend', 'words tend several', 'tend several meanings'], ['one part brain', 'part brain example', 'brain example reliably', 'example reliably responds', 'reliably responds word', 'responds word top', 'word top along', 'top along words', 'along words describe', 'words describe clothing'], ['word top activates', 'top activates many', 'activates many regions'], ['one responds numbers', 'responds numbers measurements', 'numbers measurements another', 'measurements another buildings', 'another buildings places'], ['scientists created interactive', 'created interactive website', 'interactive website public', 'website public explore', 'public explore brain', 'explore brain atlas'], ['strikingly brain atlases', 'brain atlases similar', 'atlases similar participants', 'similar participants suggesting', 'participants suggesting brains', 'suggesting brains organised', 'brains organised meanings', 'organised meanings words', 'meanings words way'], ['scientists scanned five', 'scanned five men', 'five men two', 'men two women', 'two women however'], ['native english speakers', 'english speakers two', 'speakers two authors', 'two authors study', 'authors study published', 'study published nature'], ['highly possible people', 'possible people different', 'people different backgrounds', 'different backgrounds cultures', 'backgrounds cultures different', 'cultures different semantic', 'different semantic brain', 'semantic brain atlases'], ['armed atlas researchers', 'atlas researchers piece', 'researchers piece together', 'piece together brain', 'together brain networks', 'brain networks represent', 'networks represent wildly', 'represent wildly different', 'wildly different concepts', 'different concepts numbers', 'concepts numbers murder', 'numbers murder religion'], ['the idea murder', 'idea murder represented', 'murder represented lot', 'represented lot brain', 'lot brain gallant', 'brain gallant said'], ['using haul data', 'haul data group', 'data group begun', 'group begun work', 'begun work new', 'work new atlases', 'new atlases show', 'atlases show brain', 'show brain holds', 'brain holds information', 'holds information aspects', 'information aspects language', 'aspects language phonemes', 'language phonemes syntax'], ['brain atlas narrative', 'atlas narrative structure', 'narrative structure far', 'structure far proved', 'far proved elusive', 'proved elusive however'], ['every time come', 'time come set', 'come set narrative', 'set narrative features', 'narrative features get', 'features get told', 'get told arent', 'told arent right', 'arent right set', 'right set narrative', 'set narrative features', 'narrative features said', 'features said gallant'], ['uri hasson neuroscientist', 'hasson neuroscientist princeton', 'neuroscientist princeton university', 'princeton university praised', 'university praised work'], ['unlike many studies', 'many studies looked', 'studies looked brain', 'looked brain activity', 'brain activity isolated', 'activity isolated word', 'isolated word sentence', 'word sentence spoken', 'sentence spoken gallants', 'spoken gallants team', 'gallants team shed', 'team shed light', 'shed light brain', 'light brain worked', 'brain worked realworld', 'worked realworld scenario', 'realworld scenario said'], ['next step said', 'step said create', 'said create comprehensive', 'create comprehensive precise', 'comprehensive precise semantic', 'precise semantic brain', 'semantic brain atlas'], ['ultimately hasson believes', 'hasson believes possible', 'believes possible reconstruct', 'possible reconstruct words', 'reconstruct words person', 'words person thinking', 'person thinking brain', 'thinking brain activity'], ['ethical implications enormous'], ['one benign use', 'benign use would', 'use would see', 'would see brain', 'see brain activity', 'brain activity used', 'activity used assess', 'used assess whether', 'assess whether political', 'whether political messages', 'political messages effectively', 'messages effectively communicated', 'effectively communicated public'], ['there many implications', 'many implications barely', 'implications barely touching', 'barely touching surface', 'touching surface said'], ['lorraine tyler cognitive', 'tyler cognitive neuroscientist', 'cognitive neuroscientist head', 'neuroscientist head centre', 'head centre speech', 'centre speech language', 'speech language brain', 'language brain cambridge', 'brain cambridge university', 'cambridge university said', 'university said research', 'said research tour', 'research tour de', 'tour de force', 'de force scope', 'force scope methods'], ['brain atlas current', 'atlas current form', 'current form capture', 'form capture fine', 'capture fine differences', 'fine differences word', 'differences word meanings', 'word meanings take', 'meanings take word', 'take word table'], ['member many different', 'many different groups', 'different groups says', 'groups says tyler'], ['it something eat', 'something eat off', 'eat off things', 'off things made', 'things made wood', 'made wood things', 'wood things heavy', 'things heavy things', 'heavy things four', 'things four legs', 'four legs nonanimate', 'legs nonanimate objects', 'nonanimate objects on'], ['kind detailed semantic', 'detailed semantic information', 'semantic information enables', 'information enables words', 'enables words used', 'words used flexibly', 'used flexibly lost', 'flexibly lost analysis', 'lost analysis said'], ['while research pathbreaking', 'research pathbreaking scope', 'pathbreaking scope still', 'scope still lot', 'still lot learn', 'lot learn semantics', 'learn semantics represented', 'semantics represented brain']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(\"sciencearticle_line.txt\", \"r\") as testset:\n",
    "    filtered_words = []\n",
    "    testmeta = []\n",
    "    word_list = ''\n",
    "    for line in testset:\n",
    "        word_list = line.split()\n",
    "        filtered_words = [re.sub(r'[^\\w\\s]','',word.lower()) for word in word_list if word.lower() not in stopwords.words('english')]\n",
    "        testmeta.append([' '.join(x) for x in trigrams(filtered_words)])\n",
    "print(testmeta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['snow white blanket'], [], [], ['america melting pot'], ['lovely voice music', 'voice music ears'], ['life roller coaster'], ['alligators teeth white', 'teeth white daggers'], [], ['slide playground hot', 'playground hot stove'], ['heart cold iron'], [], [], [], [], ['toms eyes ice'], ['detectives face wood', 'face wood listened', 'wood listened story'], ['feels life fashion', 'life fashion show'], [], ['kids room disaster', 'room disaster area'], ['children flowers grown', 'flowers grown concrete', 'grown concrete gardens'], ['kisses flowers affection'], ['words cotton candy'], ['marys eyes fireflies'], ['johns suggestion bandaid'], ['cast broken leg', 'broken leg plaster', 'leg plaster shackle'], ['janes ambitions house', 'ambitions house cards'], ['long hair flowing', 'hair flowing golden', 'flowing golden river'], ['computers school old', 'school old dinosaurs'], ['laughter music soul'], [], [], ['falling snowflakes dancers'], ['new haircut sheepdog'], ['five oclock interstate', 'oclock interstate parking', 'interstate parking lot'], ['books keys imagination'], ['teddy bear best', 'bear best friend', 'best friend never', 'friend never telling', 'never telling secrets'], ['peaceful lake mirror'], ['terry blue goldfish', 'blue goldfish died'], ['wind angry witch'], ['ballerina swan gliding', 'swan gliding across', 'gliding across stage'], ['angry words bullets', 'words bullets him'], [], ['jamal pig dinner'], [], ['car furnace sun'], ['thank much angel'], [], ['bens temper volcano', 'temper volcano ready', 'volcano ready explode'], ['kids monkeys jungle', 'monkeys jungle gym'], ['sun golden ball'], ['clouds balls cotton'], ['sues room zoo', 'room zoo fish', 'zoo fish gerbil', 'fish gerbil parakeet'], ['park lake rain'], ['lightning fireworks sky'], [], ['lawn green carpet'], ['dad road hog'], ['stars sparkling diamonds'], ['two best friends', 'best friends two', 'friends two peas', 'two peas pod'], [], ['donations popular charity', 'popular charity tsunami'], ['necessity mother invention'], ['big brother couch', 'brother couch potato'], ['road ribbon stretching', 'ribbon stretching across', 'stretching across desert'], ['teenagers stomach bottomless', 'stomach bottomless pit'], ['thunder mighty lion'], ['excited pulse race', 'pulse race car'], ['moon white balloon'], ['toddlers rug rats'], ['stormy ocean raging', 'ocean raging bull'], ['tears river flowing', 'river flowing cheeks']]\n"
     ]
    }
   ],
   "source": [
    "with open(\"test_meta_sent.txt\", \"r\") as testset:\n",
    "    filtered_words = []\n",
    "    testmeta = []\n",
    "    word_list = ''\n",
    "    for line in testset:\n",
    "        word_list = line.split()\n",
    "        filtered_words = [re.sub(r'[^\\w\\s]','',word.lower()) for word in word_list if word.lower() not in stopwords.words('english')]\n",
    "        testmeta.append([' '.join(x) for x in trigrams(filtered_words)])\n",
    "print(testmeta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snow white blanket: metaphor\n",
      "america melting pot: metaphor\n",
      "lovely voice music: metaphor\n",
      "voice music ears: NOT metaphor\n",
      "life roller coaster: metaphor\n",
      "alligators teeth white: metaphor\n",
      "teeth white daggers: metaphor\n",
      "slide playground hot: metaphor\n",
      "playground hot stove: NOT metaphor\n",
      "heart cold iron: NOT metaphor\n",
      "toms eyes ice: metaphor\n",
      "detectives face wood: metaphor\n",
      "face wood listened: metaphor\n",
      "wood listened story: NOT metaphor\n",
      "feels life fashion: metaphor\n",
      "life fashion show: metaphor\n",
      "kids room disaster: metaphor\n",
      "room disaster area: metaphor\n",
      "children flowers grown: metaphor\n",
      "flowers grown concrete: metaphor\n",
      "grown concrete gardens: NOT metaphor\n",
      "kisses flowers affection: metaphor\n",
      "words cotton candy: NOT metaphor\n",
      "marys eyes fireflies: NOT metaphor\n",
      "johns suggestion bandaid: NOT metaphor\n",
      "cast broken leg: metaphor\n",
      "broken leg plaster: NOT metaphor\n",
      "leg plaster shackle: metaphor\n",
      "janes ambitions house: NOT metaphor\n",
      "ambitions house cards: metaphor\n",
      "long hair flowing: metaphor\n",
      "hair flowing golden: NOT metaphor\n",
      "flowing golden river: metaphor\n",
      "computers school old: metaphor\n",
      "school old dinosaurs: metaphor\n",
      "laughter music soul: metaphor\n",
      "falling snowflakes dancers: metaphor\n",
      "new haircut sheepdog: NOT metaphor\n",
      "five oclock interstate: metaphor\n",
      "oclock interstate parking: metaphor\n",
      "interstate parking lot: NOT metaphor\n",
      "books keys imagination: NOT metaphor\n",
      "teddy bear best: metaphor\n",
      "bear best friend: NOT metaphor\n",
      "best friend never: NOT metaphor\n",
      "friend never telling: metaphor\n",
      "never telling secrets: NOT metaphor\n",
      "peaceful lake mirror: NOT metaphor\n",
      "terry blue goldfish: metaphor\n",
      "blue goldfish died: NOT metaphor\n",
      "wind angry witch: NOT metaphor\n",
      "ballerina swan gliding: NOT metaphor\n",
      "swan gliding across: metaphor\n",
      "gliding across stage: NOT metaphor\n",
      "angry words bullets: metaphor\n",
      "words bullets him: NOT metaphor\n",
      "jamal pig dinner: NOT metaphor\n",
      "car furnace sun: metaphor\n",
      "thank much angel: metaphor\n",
      "bens temper volcano: NOT metaphor\n",
      "temper volcano ready: metaphor\n",
      "volcano ready explode: NOT metaphor\n",
      "kids monkeys jungle: metaphor\n",
      "monkeys jungle gym: NOT metaphor\n",
      "sun golden ball: metaphor\n",
      "clouds balls cotton: metaphor\n",
      "sues room zoo: metaphor\n",
      "room zoo fish: metaphor\n",
      "zoo fish gerbil: metaphor\n",
      "fish gerbil parakeet: metaphor\n",
      "park lake rain: NOT metaphor\n",
      "lightning fireworks sky: metaphor\n",
      "lawn green carpet: metaphor\n",
      "dad road hog: metaphor\n",
      "stars sparkling diamonds: metaphor\n",
      "two best friends: metaphor\n",
      "best friends two: NOT metaphor\n",
      "friends two peas: metaphor\n",
      "two peas pod: metaphor\n",
      "donations popular charity: metaphor\n",
      "popular charity tsunami: NOT metaphor\n",
      "necessity mother invention: NOT metaphor\n",
      "big brother couch: NOT metaphor\n",
      "brother couch potato: NOT metaphor\n",
      "road ribbon stretching: metaphor\n",
      "ribbon stretching across: metaphor\n",
      "stretching across desert: metaphor\n",
      "teenagers stomach bottomless: metaphor\n",
      "stomach bottomless pit: metaphor\n",
      "thunder mighty lion: metaphor\n",
      "excited pulse race: metaphor\n",
      "pulse race car: NOT metaphor\n",
      "moon white balloon: NOT metaphor\n",
      "toddlers rug rats: metaphor\n",
      "stormy ocean raging: metaphor\n",
      "ocean raging bull: metaphor\n",
      "tears river flowing: metaphor\n",
      "river flowing cheeks: metaphor\n",
      "metaphor_count:63\n",
      "nonmetaphor_count:35\n"
     ]
    }
   ],
   "source": [
    "metaphor_count = 0\n",
    "nonmetaphor_count = 0\n",
    "for x in testmeta:\n",
    "    for y in x:\n",
    "        testsample = y\n",
    "        print (testsample + \": \" + cl.classify(metaphor_features(testsample)))\n",
    "        if cl.classify(metaphor_features(testsample)) == 'metaphor':\n",
    "            metaphor_count += 1\n",
    "        elif cl.classify(metaphor_features(testsample)) == 'NOT metaphor':\n",
    "            nonmetaphor_count += 1\n",
    "            \n",
    "print('metaphor_count:' + str(metaphor_count))\n",
    "print('nonmetaphor_count:' + str(nonmetaphor_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "scientists created atlas: metaphor\n",
      "created atlas brain: metaphor\n",
      "atlas brain reveals: metaphor\n",
      "brain reveals meanings: NOT metaphor\n",
      "reveals meanings words: metaphor\n",
      "meanings words arranged: NOT metaphor\n",
      "words arranged across: NOT metaphor\n",
      "arranged across different: metaphor\n",
      "across different regions: metaphor\n",
      "different regions organ: metaphor\n",
      "like colourful quilt: metaphor\n",
      "colourful quilt laid: metaphor\n",
      "quilt laid cortex: metaphor\n",
      "laid cortex atlas: metaphor\n",
      "cortex atlas displays: metaphor\n",
      "atlas displays rainbow: metaphor\n",
      "displays rainbow hues: metaphor\n",
      "rainbow hues individual: metaphor\n",
      "hues individual words: NOT metaphor\n",
      "individual words concepts: NOT metaphor\n",
      "words concepts convey: NOT metaphor\n",
      "concepts convey grouped: metaphor\n",
      "convey grouped together: metaphor\n",
      "grouped together clumps: NOT metaphor\n",
      "together clumps white: metaphor\n",
      "clumps white matter: metaphor\n",
      "our goal build: metaphor\n",
      "goal build giant: NOT metaphor\n",
      "build giant atlas: NOT metaphor\n",
      "giant atlas shows: NOT metaphor\n",
      "atlas shows one: metaphor\n",
      "shows one specific: metaphor\n",
      "one specific aspect: metaphor\n",
      "specific aspect language: metaphor\n",
      "aspect language represented: metaphor\n",
      "language represented brain: metaphor\n",
      "represented brain case: metaphor\n",
      "brain case semantics: NOT metaphor\n",
      "case semantics meanings: metaphor\n",
      "semantics meanings words: metaphor\n",
      "meanings words said: NOT metaphor\n",
      "words said jack: NOT metaphor\n",
      "said jack gallant: metaphor\n",
      "jack gallant neuroscientist: NOT metaphor\n",
      "gallant neuroscientist university: NOT metaphor\n",
      "neuroscientist university california: NOT metaphor\n",
      "university california berkeley: metaphor\n",
      "no single brain: NOT metaphor\n",
      "single brain region: metaphor\n",
      "brain region holds: NOT metaphor\n",
      "region holds one: metaphor\n",
      "holds one word: NOT metaphor\n",
      "one word concept: metaphor\n",
      "word concept a: NOT metaphor\n",
      "concept a single: metaphor\n",
      "a single brain: metaphor\n",
      "single brain spot: metaphor\n",
      "brain spot associated: NOT metaphor\n",
      "spot associated number: metaphor\n",
      "associated number related: metaphor\n",
      "number related words: NOT metaphor\n",
      "related words and: metaphor\n",
      "words and single: NOT metaphor\n",
      "and single word: metaphor\n",
      "single word lights: metaphor\n",
      "word lights many: NOT metaphor\n",
      "lights many different: metaphor\n",
      "many different brain: NOT metaphor\n",
      "different brain spots: metaphor\n",
      "brain spots together: NOT metaphor\n",
      "spots together make: metaphor\n",
      "together make networks: metaphor\n",
      "make networks represent: NOT metaphor\n",
      "networks represent meanings: NOT metaphor\n",
      "represent meanings word: metaphor\n",
      "meanings word use: NOT metaphor\n",
      "word use life: NOT metaphor\n",
      "use life love: metaphor\n",
      "life love death: metaphor\n",
      "love death taxes: metaphor\n",
      "death taxes clouds: metaphor\n",
      "taxes clouds florida: metaphor\n",
      "clouds florida bra: metaphor\n",
      "florida bra all: metaphor\n",
      "bra all light: NOT metaphor\n",
      "all light networks: metaphor\n",
      "described tour de: metaphor\n",
      "tour de force: metaphor\n",
      "de force one: metaphor\n",
      "force one researcher: metaphor\n",
      "one researcher involved: metaphor\n",
      "researcher involved study: metaphor\n",
      "involved study atlas: NOT metaphor\n",
      "study atlas demonstrates: metaphor\n",
      "atlas demonstrates modern: metaphor\n",
      "demonstrates modern imaging: metaphor\n",
      "modern imaging transform: NOT metaphor\n",
      "imaging transform knowledge: NOT metaphor\n",
      "transform knowledge brain: metaphor\n",
      "knowledge brain performs: metaphor\n",
      "brain performs important: NOT metaphor\n",
      "performs important tasks: NOT metaphor\n",
      "important tasks with: NOT metaphor\n",
      "tasks with advances: metaphor\n",
      "with advances technology: NOT metaphor\n",
      "advances technology could: metaphor\n",
      "technology could profound: metaphor\n",
      "could profound impact: metaphor\n",
      "profound impact medicine: NOT metaphor\n",
      "impact medicine fields: NOT metaphor\n",
      "it possible approach: NOT metaphor\n",
      "possible approach could: NOT metaphor\n",
      "approach could used: metaphor\n",
      "could used decode: metaphor\n",
      "used decode information: metaphor\n",
      "decode information words: metaphor\n",
      "information words person: NOT metaphor\n",
      "words person hearing: NOT metaphor\n",
      "person hearing reading: NOT metaphor\n",
      "hearing reading possibly: NOT metaphor\n",
      "reading possibly even: metaphor\n",
      "possibly even thinking: NOT metaphor\n",
      "even thinking said: metaphor\n",
      "thinking said alexander: metaphor\n",
      "said alexander huth: metaphor\n",
      "alexander huth first: metaphor\n",
      "huth first author: NOT metaphor\n",
      "first author study: metaphor\n",
      "author study one: metaphor\n",
      "study one potential: metaphor\n",
      "one potential use: metaphor\n",
      "potential use would: NOT metaphor\n",
      "use would language: metaphor\n",
      "would language decoder: NOT metaphor\n",
      "language decoder could: metaphor\n",
      "decoder could allow: metaphor\n",
      "could allow people: metaphor\n",
      "allow people silenced: metaphor\n",
      "people silenced motor: NOT metaphor\n",
      "silenced motor neurone: metaphor\n",
      "motor neurone disease: NOT metaphor\n",
      "neurone disease lockedin: NOT metaphor\n",
      "disease lockedin syndrome: metaphor\n",
      "lockedin syndrome speak: metaphor\n",
      "syndrome speak computer: metaphor\n",
      "to create atlas: metaphor\n",
      "create atlas scientists: metaphor\n",
      "atlas scientists recorded: metaphor\n",
      "scientists recorded peoples: metaphor\n",
      "recorded peoples brain: metaphor\n",
      "peoples brain activity: NOT metaphor\n",
      "brain activity listened: NOT metaphor\n",
      "activity listened stories: metaphor\n",
      "listened stories read: metaphor\n",
      "stories read the: metaphor\n",
      "read the moth: metaphor\n",
      "the moth radio: metaphor\n",
      "moth radio hour: NOT metaphor\n",
      "radio hour us: metaphor\n",
      "hour us radio: NOT metaphor\n",
      "us radio show: metaphor\n",
      "radio show they: metaphor\n",
      "show they matched: metaphor\n",
      "they matched transcripts: metaphor\n",
      "matched transcripts stories: NOT metaphor\n",
      "transcripts stories brain: metaphor\n",
      "stories brain activity: metaphor\n",
      "brain activity data: NOT metaphor\n",
      "activity data show: metaphor\n",
      "data show groups: metaphor\n",
      "show groups related: metaphor\n",
      "groups related words: NOT metaphor\n",
      "related words triggered: metaphor\n",
      "words triggered neural: NOT metaphor\n",
      "triggered neural responses: metaphor\n",
      "neural responses 50000: NOT metaphor\n",
      "responses 50000 80000: metaphor\n",
      "50000 80000 peasized: metaphor\n",
      "80000 peasized spots: metaphor\n",
      "peasized spots cerebral: NOT metaphor\n",
      "spots cerebral cortex: metaphor\n",
      "huth used stories: NOT metaphor\n",
      "used stories the: metaphor\n",
      "stories the moth: metaphor\n",
      "the moth radio: metaphor\n",
      "moth radio hour: NOT metaphor\n",
      "radio hour short: metaphor\n",
      "hour short compelling: NOT metaphor\n",
      "short compelling the: metaphor\n",
      "compelling the enthralling: metaphor\n",
      "the enthralling stories: metaphor\n",
      "enthralling stories confident: metaphor\n",
      "stories confident scientists: metaphor\n",
      "confident scientists could: metaphor\n",
      "scientists could people: metaphor\n",
      "could people scanned: metaphor\n",
      "people scanned focusing: NOT metaphor\n",
      "scanned focusing words: metaphor\n",
      "focusing words drifting: metaphor\n",
      "words drifting off: NOT metaphor\n",
      "drifting off seven: metaphor\n",
      "off seven people: metaphor\n",
      "seven people listened: metaphor\n",
      "people listened two: NOT metaphor\n",
      "listened two hours: metaphor\n",
      "two hours stories: metaphor\n",
      "hours stories each: NOT metaphor\n",
      "stories each per: metaphor\n",
      "each per person: metaphor\n",
      "per person amounted: NOT metaphor\n",
      "person amounted hearing: NOT metaphor\n",
      "amounted hearing roughly: metaphor\n",
      "hearing roughly 25000: NOT metaphor\n",
      "roughly 25000 words: metaphor\n",
      "25000 words 3000: metaphor\n",
      "words 3000 different: NOT metaphor\n",
      "3000 different words: metaphor\n",
      "different words : metaphor\n",
      "words  lay: NOT metaphor\n",
      " lay scanner: metaphor\n",
      "the atlas shows: metaphor\n",
      "atlas shows words: metaphor\n",
      "shows words related: metaphor\n",
      "words related terms: NOT metaphor\n",
      "related terms exercise: metaphor\n",
      "terms exercise regions: metaphor\n",
      "exercise regions brain: metaphor\n",
      "regions brain for: metaphor\n",
      "brain for example: NOT metaphor\n",
      "for example lefthand: metaphor\n",
      "example lefthand side: metaphor\n",
      "lefthand side brain: metaphor\n",
      "side brain ear: metaphor\n",
      "brain ear one: NOT metaphor\n",
      "ear one tiny: metaphor\n",
      "one tiny regions: metaphor\n",
      "tiny regions represents: metaphor\n",
      "regions represents word: metaphor\n",
      "represents word victim: metaphor\n",
      "word victim the: NOT metaphor\n",
      "victim the region: NOT metaphor\n",
      "the region responds: metaphor\n",
      "region responds killed: metaphor\n",
      "responds killed convicted: metaphor\n",
      "killed convicted murdered: metaphor\n",
      "convicted murdered confessed: metaphor\n",
      "murdered confessed on: NOT metaphor\n",
      "confessed on brains: metaphor\n",
      "on brains righthand: metaphor\n",
      "brains righthand side: NOT metaphor\n",
      "righthand side near: metaphor\n",
      "side near top: metaphor\n",
      "near top head: NOT metaphor\n",
      "top head one: metaphor\n",
      "head one brain: NOT metaphor\n",
      "one brain spots: metaphor\n",
      "brain spots activated: NOT metaphor\n",
      "spots activated family: metaphor\n",
      "activated family terms: metaphor\n",
      "family terms wife: metaphor\n",
      "terms wife husband: metaphor\n",
      "wife husband children: NOT metaphor\n",
      "husband children parents: NOT metaphor\n",
      "each word represented: metaphor\n",
      "word represented one: NOT metaphor\n",
      "represented one spot: metaphor\n",
      "one spot words: metaphor\n",
      "spot words tend: metaphor\n",
      "words tend several: NOT metaphor\n",
      "tend several meanings: metaphor\n",
      "several meanings one: metaphor\n",
      "meanings one part: NOT metaphor\n",
      "one part brain: metaphor\n",
      "part brain example: NOT metaphor\n",
      "brain example reliably: NOT metaphor\n",
      "example reliably responds: metaphor\n",
      "reliably responds word: metaphor\n",
      "responds word top: metaphor\n",
      "word top along: NOT metaphor\n",
      "top along words: metaphor\n",
      "along words describe: metaphor\n",
      "words describe clothing: NOT metaphor\n",
      "describe clothing but: metaphor\n",
      "clothing but word: metaphor\n",
      "but word top: NOT metaphor\n",
      "word top activates: NOT metaphor\n",
      "top activates many: metaphor\n",
      "activates many regions: metaphor\n",
      "many regions one: NOT metaphor\n",
      "regions one responds: metaphor\n",
      "one responds numbers: metaphor\n",
      "responds numbers measurements: metaphor\n",
      "numbers measurements another: NOT metaphor\n",
      "measurements another buildings: NOT metaphor\n",
      "another buildings places: metaphor\n",
      "buildings places the: NOT metaphor\n",
      "places the scientists: NOT metaphor\n",
      "the scientists created: metaphor\n",
      "scientists created interactive: metaphor\n",
      "created interactive website: metaphor\n",
      "interactive website public: NOT metaphor\n",
      "website public explore: NOT metaphor\n",
      "public explore brain: NOT metaphor\n",
      "explore brain atlas: metaphor\n",
      "strikingly brain atlases: metaphor\n",
      "brain atlases similar: NOT metaphor\n",
      "atlases similar participants: metaphor\n",
      "similar participants suggesting: metaphor\n",
      "participants suggesting brains: NOT metaphor\n",
      "suggesting brains organised: metaphor\n",
      "brains organised meanings: NOT metaphor\n",
      "organised meanings words: metaphor\n",
      "meanings words way: NOT metaphor\n",
      "words way the: NOT metaphor\n",
      "way the scientists: NOT metaphor\n",
      "the scientists scanned: metaphor\n",
      "scientists scanned five: metaphor\n",
      "scanned five men: metaphor\n",
      "five men two: metaphor\n",
      "men two women: NOT metaphor\n",
      "two women however: metaphor\n",
      "women however all: NOT metaphor\n",
      "however all native: NOT metaphor\n",
      "all native english: metaphor\n",
      "native english speakers: NOT metaphor\n",
      "english speakers two: metaphor\n",
      "speakers two authors: metaphor\n",
      "two authors study: metaphor\n",
      "authors study published: metaphor\n",
      "study published nature: metaphor\n",
      "published nature it: NOT metaphor\n",
      "nature it highly: NOT metaphor\n",
      "it highly possible: NOT metaphor\n",
      "highly possible people: NOT metaphor\n",
      "possible people different: NOT metaphor\n",
      "people different backgrounds: NOT metaphor\n",
      "different backgrounds cultures: metaphor\n",
      "backgrounds cultures different: NOT metaphor\n",
      "cultures different semantic: metaphor\n",
      "different semantic brain: metaphor\n",
      "semantic brain atlases: metaphor\n",
      "armed atlas researchers: metaphor\n",
      "atlas researchers piece: metaphor\n",
      "researchers piece together: metaphor\n",
      "piece together brain: NOT metaphor\n",
      "together brain networks: metaphor\n",
      "brain networks represent: NOT metaphor\n",
      "networks represent wildly: NOT metaphor\n",
      "represent wildly different: metaphor\n",
      "wildly different concepts: NOT metaphor\n",
      "different concepts numbers: metaphor\n",
      "concepts numbers murder: metaphor\n",
      "numbers murder religion: NOT metaphor\n",
      "murder religion the: NOT metaphor\n",
      "religion the idea: metaphor\n",
      "the idea murder: metaphor\n",
      "idea murder represented: NOT metaphor\n",
      "murder represented lot: NOT metaphor\n",
      "represented lot brain: metaphor\n",
      "lot brain gallant: metaphor\n",
      "brain gallant said: NOT metaphor\n",
      "using haul data: metaphor\n",
      "haul data group: NOT metaphor\n",
      "data group begun: metaphor\n",
      "group begun work: NOT metaphor\n",
      "begun work new: NOT metaphor\n",
      "work new atlases: NOT metaphor\n",
      "new atlases show: NOT metaphor\n",
      "atlases show brain: metaphor\n",
      "show brain holds: metaphor\n",
      "brain holds information: NOT metaphor\n",
      "holds information aspects: NOT metaphor\n",
      "information aspects language: NOT metaphor\n",
      "aspects language phonemes: metaphor\n",
      "language phonemes syntax: metaphor\n",
      "phonemes syntax a: NOT metaphor\n",
      "syntax a brain: metaphor\n",
      "a brain atlas: metaphor\n",
      "brain atlas narrative: NOT metaphor\n",
      "atlas narrative structure: metaphor\n",
      "narrative structure far: NOT metaphor\n",
      "structure far proved: metaphor\n",
      "far proved elusive: metaphor\n",
      "proved elusive however: NOT metaphor\n",
      "elusive however every: metaphor\n",
      "however every time: NOT metaphor\n",
      "every time come: metaphor\n",
      "time come set: metaphor\n",
      "come set narrative: metaphor\n",
      "set narrative features: metaphor\n",
      "narrative features get: NOT metaphor\n",
      "features get told: metaphor\n",
      "get told arent: NOT metaphor\n",
      "told arent right: metaphor\n",
      "arent right set: metaphor\n",
      "right set narrative: metaphor\n",
      "set narrative features: metaphor\n",
      "narrative features said: NOT metaphor\n",
      "features said gallant: metaphor\n",
      "uri hasson neuroscientist: metaphor\n",
      "hasson neuroscientist princeton: NOT metaphor\n",
      "neuroscientist princeton university: NOT metaphor\n",
      "princeton university praised: NOT metaphor\n",
      "university praised work: metaphor\n",
      "praised work unlike: NOT metaphor\n",
      "work unlike many: NOT metaphor\n",
      "unlike many studies: metaphor\n",
      "many studies looked: NOT metaphor\n",
      "studies looked brain: metaphor\n",
      "looked brain activity: metaphor\n",
      "brain activity isolated: NOT metaphor\n",
      "activity isolated word: metaphor\n",
      "isolated word sentence: NOT metaphor\n",
      "word sentence spoken: NOT metaphor\n",
      "sentence spoken gallants: metaphor\n",
      "spoken gallants team: metaphor\n",
      "gallants team shed: NOT metaphor\n",
      "team shed light: metaphor\n",
      "shed light brain: metaphor\n",
      "light brain worked: metaphor\n",
      "brain worked realworld: NOT metaphor\n",
      "worked realworld scenario: NOT metaphor\n",
      "realworld scenario said: metaphor\n",
      "scenario said the: metaphor\n",
      "said the next: metaphor\n",
      "the next step: metaphor\n",
      "next step said: NOT metaphor\n",
      "step said create: metaphor\n",
      "said create comprehensive: metaphor\n",
      "create comprehensive precise: metaphor\n",
      "comprehensive precise semantic: metaphor\n",
      "precise semantic brain: NOT metaphor\n",
      "semantic brain atlas: metaphor\n",
      "brain atlas ultimately: NOT metaphor\n",
      "atlas ultimately hasson: metaphor\n",
      "ultimately hasson believes: metaphor\n",
      "hasson believes possible: NOT metaphor\n",
      "believes possible reconstruct: NOT metaphor\n",
      "possible reconstruct words: NOT metaphor\n",
      "reconstruct words person: metaphor\n",
      "words person thinking: NOT metaphor\n",
      "person thinking brain: NOT metaphor\n",
      "thinking brain activity: metaphor\n",
      "brain activity the: NOT metaphor\n",
      "activity the ethical: metaphor\n",
      "the ethical implications: metaphor\n",
      "ethical implications enormous: metaphor\n",
      "implications enormous one: NOT metaphor\n",
      "enormous one benign: metaphor\n",
      "one benign use: metaphor\n",
      "benign use would: NOT metaphor\n",
      "use would see: metaphor\n",
      "would see brain: NOT metaphor\n",
      "see brain activity: metaphor\n",
      "brain activity used: NOT metaphor\n",
      "activity used assess: metaphor\n",
      "used assess whether: metaphor\n",
      "assess whether political: metaphor\n",
      "whether political messages: NOT metaphor\n",
      "political messages effectively: NOT metaphor\n",
      "messages effectively communicated: NOT metaphor\n",
      "effectively communicated public: metaphor\n",
      "communicated public there: metaphor\n",
      "public there many: NOT metaphor\n",
      "there many implications: metaphor\n",
      "many implications barely: NOT metaphor\n",
      "implications barely touching: NOT metaphor\n",
      "barely touching surface: NOT metaphor\n",
      "touching surface said: metaphor\n",
      "lorraine tyler cognitive: metaphor\n",
      "tyler cognitive neuroscientist: metaphor\n",
      "cognitive neuroscientist head: metaphor\n",
      "neuroscientist head centre: NOT metaphor\n",
      "head centre speech: NOT metaphor\n",
      "centre speech language: metaphor\n",
      "speech language brain: metaphor\n",
      "language brain cambridge: metaphor\n",
      "brain cambridge university: NOT metaphor\n",
      "cambridge university said: metaphor\n",
      "university said research: metaphor\n",
      "said research tour: metaphor\n",
      "research tour de: metaphor\n",
      "tour de force: metaphor\n",
      "de force scope: metaphor\n",
      "force scope methods: metaphor\n",
      "scope methods but: metaphor\n",
      "methods but brain: NOT metaphor\n",
      "but brain atlas: NOT metaphor\n",
      "brain atlas current: NOT metaphor\n",
      "atlas current form: metaphor\n",
      "current form capture: metaphor\n",
      "form capture fine: metaphor\n",
      "capture fine differences: metaphor\n",
      "fine differences word: metaphor\n",
      "differences word meanings: metaphor\n",
      "word meanings take: NOT metaphor\n",
      "meanings take word: NOT metaphor\n",
      "take word table: metaphor\n",
      "word table it: NOT metaphor\n",
      "table it member: metaphor\n",
      "it member many: NOT metaphor\n",
      "member many different: NOT metaphor\n",
      "many different groups: NOT metaphor\n",
      "different groups says: metaphor\n",
      "groups says tyler: NOT metaphor\n",
      "says tyler it: metaphor\n",
      "tyler it something: metaphor\n",
      "it something eat: NOT metaphor\n",
      "something eat off: metaphor\n",
      "eat off things: metaphor\n",
      "off things made: metaphor\n",
      "things made wood: metaphor\n",
      "made wood things: NOT metaphor\n",
      "wood things heavy: NOT metaphor\n",
      "things heavy things: metaphor\n",
      "heavy things four: NOT metaphor\n",
      "things four legs: metaphor\n",
      "four legs nonanimate: metaphor\n",
      "legs nonanimate objects: metaphor\n",
      "nonanimate objects on: NOT metaphor\n",
      "objects on this: metaphor\n",
      "on this kind: metaphor\n",
      "this kind detailed: metaphor\n",
      "kind detailed semantic: metaphor\n",
      "detailed semantic information: metaphor\n",
      "semantic information enables: metaphor\n",
      "information enables words: NOT metaphor\n",
      "enables words used: metaphor\n",
      "words used flexibly: NOT metaphor\n",
      "used flexibly lost: metaphor\n",
      "flexibly lost analysis: metaphor\n",
      "lost analysis said: metaphor\n",
      "analysis said while: metaphor\n",
      "said while research: metaphor\n",
      "while research pathbreaking: NOT metaphor\n",
      "research pathbreaking scope: metaphor\n",
      "pathbreaking scope still: NOT metaphor\n",
      "scope still lot: metaphor\n",
      "still lot learn: metaphor\n",
      "lot learn semantics: metaphor\n",
      "learn semantics represented: metaphor\n",
      "semantics represented brain: metaphor\n",
      "metaphor_count:342\n",
      "nonmetaphor_count:200\n",
      "['scientists created atlas', 'created atlas brain', 'atlas brain reveals', 'reveals meanings words', 'arranged across different', 'across different regions', 'different regions organ', 'like colourful quilt', 'colourful quilt laid', 'quilt laid cortex', 'laid cortex atlas', 'cortex atlas displays', 'atlas displays rainbow', 'displays rainbow hues', 'rainbow hues individual', 'concepts convey grouped', 'convey grouped together', 'together clumps white', 'clumps white matter', 'our goal build', 'atlas shows one', 'shows one specific', 'one specific aspect', 'specific aspect language', 'aspect language represented', 'language represented brain', 'represented brain case', 'case semantics meanings', 'semantics meanings words', 'said jack gallant', 'university california berkeley', 'single brain region', 'region holds one', 'one word concept', 'concept a single', 'a single brain', 'single brain spot', 'spot associated number', 'associated number related', 'related words and', 'and single word', 'single word lights', 'lights many different', 'different brain spots', 'spots together make', 'together make networks', 'represent meanings word', 'use life love', 'life love death', 'love death taxes', 'death taxes clouds', 'taxes clouds florida', 'clouds florida bra', 'florida bra all', 'all light networks', 'described tour de', 'tour de force', 'de force one', 'force one researcher', 'one researcher involved', 'researcher involved study', 'study atlas demonstrates', 'atlas demonstrates modern', 'demonstrates modern imaging', 'transform knowledge brain', 'knowledge brain performs', 'tasks with advances', 'advances technology could', 'technology could profound', 'could profound impact', 'approach could used', 'could used decode', 'used decode information', 'decode information words', 'reading possibly even', 'even thinking said', 'thinking said alexander', 'said alexander huth', 'alexander huth first', 'first author study', 'author study one', 'study one potential', 'one potential use', 'use would language', 'language decoder could', 'decoder could allow', 'could allow people', 'allow people silenced', 'silenced motor neurone', 'disease lockedin syndrome', 'lockedin syndrome speak', 'syndrome speak computer', 'to create atlas', 'create atlas scientists', 'atlas scientists recorded', 'scientists recorded peoples', 'recorded peoples brain', 'activity listened stories', 'listened stories read', 'stories read the', 'read the moth', 'the moth radio', 'radio hour us', 'us radio show', 'radio show they', 'show they matched', 'they matched transcripts', 'transcripts stories brain', 'stories brain activity', 'activity data show', 'data show groups', 'show groups related', 'related words triggered', 'triggered neural responses', 'responses 50000 80000', '50000 80000 peasized', '80000 peasized spots', 'spots cerebral cortex', 'used stories the', 'stories the moth', 'the moth radio', 'radio hour short', 'short compelling the', 'compelling the enthralling', 'the enthralling stories', 'enthralling stories confident', 'stories confident scientists', 'confident scientists could', 'scientists could people', 'could people scanned', 'scanned focusing words', 'focusing words drifting', 'drifting off seven', 'off seven people', 'seven people listened', 'listened two hours', 'two hours stories', 'stories each per', 'each per person', 'amounted hearing roughly', 'roughly 25000 words', '25000 words 3000', '3000 different words', 'different words ', ' lay scanner', 'the atlas shows', 'atlas shows words', 'shows words related', 'related terms exercise', 'terms exercise regions', 'exercise regions brain', 'regions brain for', 'for example lefthand', 'example lefthand side', 'lefthand side brain', 'side brain ear', 'ear one tiny', 'one tiny regions', 'tiny regions represents', 'regions represents word', 'represents word victim', 'the region responds', 'region responds killed', 'responds killed convicted', 'killed convicted murdered', 'convicted murdered confessed', 'confessed on brains', 'on brains righthand', 'righthand side near', 'side near top', 'top head one', 'one brain spots', 'spots activated family', 'activated family terms', 'family terms wife', 'terms wife husband', 'each word represented', 'represented one spot', 'one spot words', 'spot words tend', 'tend several meanings', 'several meanings one', 'one part brain', 'example reliably responds', 'reliably responds word', 'responds word top', 'top along words', 'along words describe', 'describe clothing but', 'clothing but word', 'top activates many', 'activates many regions', 'regions one responds', 'one responds numbers', 'responds numbers measurements', 'another buildings places', 'the scientists created', 'scientists created interactive', 'created interactive website', 'explore brain atlas', 'strikingly brain atlases', 'atlases similar participants', 'similar participants suggesting', 'suggesting brains organised', 'organised meanings words', 'the scientists scanned', 'scientists scanned five', 'scanned five men', 'five men two', 'two women however', 'all native english', 'english speakers two', 'speakers two authors', 'two authors study', 'authors study published', 'study published nature', 'different backgrounds cultures', 'cultures different semantic', 'different semantic brain', 'semantic brain atlases', 'armed atlas researchers', 'atlas researchers piece', 'researchers piece together', 'together brain networks', 'represent wildly different', 'different concepts numbers', 'concepts numbers murder', 'religion the idea', 'the idea murder', 'represented lot brain', 'lot brain gallant', 'using haul data', 'data group begun', 'atlases show brain', 'show brain holds', 'aspects language phonemes', 'language phonemes syntax', 'syntax a brain', 'a brain atlas', 'atlas narrative structure', 'structure far proved', 'far proved elusive', 'elusive however every', 'every time come', 'time come set', 'come set narrative', 'set narrative features', 'features get told', 'told arent right', 'arent right set', 'right set narrative', 'set narrative features', 'features said gallant', 'uri hasson neuroscientist', 'university praised work', 'unlike many studies', 'studies looked brain', 'looked brain activity', 'activity isolated word', 'sentence spoken gallants', 'spoken gallants team', 'team shed light', 'shed light brain', 'light brain worked', 'realworld scenario said', 'scenario said the', 'said the next', 'the next step', 'step said create', 'said create comprehensive', 'create comprehensive precise', 'comprehensive precise semantic', 'semantic brain atlas', 'atlas ultimately hasson', 'ultimately hasson believes', 'reconstruct words person', 'thinking brain activity', 'activity the ethical', 'the ethical implications', 'ethical implications enormous', 'enormous one benign', 'one benign use', 'use would see', 'see brain activity', 'activity used assess', 'used assess whether', 'assess whether political', 'effectively communicated public', 'communicated public there', 'there many implications', 'touching surface said', 'lorraine tyler cognitive', 'tyler cognitive neuroscientist', 'cognitive neuroscientist head', 'centre speech language', 'speech language brain', 'language brain cambridge', 'cambridge university said', 'university said research', 'said research tour', 'research tour de', 'tour de force', 'de force scope', 'force scope methods', 'scope methods but', 'atlas current form', 'current form capture', 'form capture fine', 'capture fine differences', 'fine differences word', 'differences word meanings', 'take word table', 'table it member', 'different groups says', 'says tyler it', 'tyler it something', 'something eat off', 'eat off things', 'off things made', 'things made wood', 'things heavy things', 'things four legs', 'four legs nonanimate', 'legs nonanimate objects', 'objects on this', 'on this kind', 'this kind detailed', 'kind detailed semantic', 'detailed semantic information', 'semantic information enables', 'enables words used', 'used flexibly lost', 'flexibly lost analysis', 'lost analysis said', 'analysis said while', 'said while research', 'research pathbreaking scope', 'scope still lot', 'still lot learn', 'lot learn semantics', 'learn semantics represented', 'semantics represented brain']\n"
     ]
    }
   ],
   "source": [
    "def read_article_for_metaphor(article):\n",
    "    with open(\"sciencearticle.txt\", \"r\") as testset:\n",
    "        filtered_words = []\n",
    "        testmeta = []\n",
    "        word_list = ''\n",
    "        para_index = 0\n",
    "    #     text = ''.join(testset.readlines())\n",
    "    #     sentences = re.split(r' *[\\.\\?!][\\'\"\\)\\]]* *', text)\n",
    "    #     sentences = [t.rstrip() for t in sentences]\n",
    "    #     print(sentences)\n",
    "        for line in testset:\n",
    "            para_index += 1\n",
    "            word_list = line.split()\n",
    "            filtered_words = [re.sub(r'[^\\w\\s]','',word.lower()) for word in word_list if word not in stopwords.words('english')]\n",
    "            testmeta.append([' '.join(x) for x in trigrams(filtered_words)])\n",
    "            print(para_index)\n",
    "    # print(testmeta)\n",
    "\n",
    "    metaphor_count = 0\n",
    "    nonmetaphor_count = 0\n",
    "    metaphor_list = []\n",
    "    for x in testmeta:\n",
    "        for y in x:\n",
    "            testsample = y\n",
    "            print (testsample + \": \" + cl.classify(metaphor_features(testsample)))\n",
    "            if cl.classify(metaphor_features(testsample)) == 'metaphor':\n",
    "                metaphor_count += 1\n",
    "                metaphor_list.append(testsample)\n",
    "            elif cl.classify(metaphor_features(testsample)) == 'NOT metaphor':\n",
    "                nonmetaphor_count += 1\n",
    "\n",
    "    print('metaphor_count:' + str(metaphor_count))\n",
    "    print('nonmetaphor_count:' + str(nonmetaphor_count))\n",
    "    print(metaphor_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scientists have created an â€œatlas of the brainâ€ that reveals how the meanings of words are arranged across different regions of the organ.\n",
      "\n",
      "\n",
      "\n",
      "Like a colourful quilt laid over the cortex, the atlas displays in rainbow hues how individual words and the concepts they convey can be grouped together in clumps of white matter.\n",
      "\n",
      "\n",
      "\n",
      "â€œOur goal was to build a giant atlas that shows how one specific aspect of language is represented in the brain, in this case semantics, or the meanings of words,â€ said Jack Gallant, a neuroscientist at the University of California, Berkeley.\n",
      "\n",
      "\n",
      "\n",
      "No single brain region holds one word or concept. A single brain spot is associated with a number of related words. And each single word lights up many different brain spots. Together they make up networks that represent the meanings of each word we use: life and love; death and taxes; clouds, Florida and bra. All light up their own networks.\n",
      "\n",
      "\n",
      "\n",
      "Described as a â€œtour de forceâ€ by one researcher who was not involved in the study, the atlas demonstrates how modern imaging can transform our knowledge of how the brain performs some of its most important tasks. With further advances, the technology could have a profound impact on medicine and other fields.\n",
      "\n",
      "\n",
      "\n",
      "â€œIt is possible that this approach could be used to decode information about what words a person is hearing, reading, or possibly even thinking,â€ said Alexander Huth, the first author on the study. One potential use would be a language decoder that could allow people silenced by motor neurone disease or locked-in syndrome to speak through a computer.\n",
      "\n",
      "\n",
      "\n",
      "To create the atlas, the scientists recorded peopleâ€™s brain activity while they listened to stories read out on The Moth Radio Hour, a US radio show. They then matched the transcripts of the stories with the brain activity data to show how groups of related words triggered neural responses in 50,000 to 80,000 pea-sized spots all over the cerebral cortex.\n",
      "\n",
      "\n",
      "\n",
      "Huth used stories from The Moth Radio Hour because they are short and compelling. The more enthralling the stories, the more confident the scientists could be that the people being scanned were focusing on the words and not drifting off. Seven people listened to two hours of stories each. Per person, that amounted to hearing roughly 25,000 words- and more than 3,000 different words - as they lay in the scanner.\n",
      "\n",
      "\n",
      "\n",
      "The atlas shows how words and related terms exercise the same regions of the brain. For example, on the left-hand side of the brain, above the ear, is one of the tiny regions that represents the word â€œvictimâ€. The same region responds to â€œkilledâ€, â€œconvictedâ€, â€œmurderedâ€ and â€œconfessedâ€. On the brainâ€™s right-hand side, near the top of the head, is one of the brain spots activated by family terms: â€œwifeâ€, â€œhusbandâ€, â€œchildrenâ€, â€œparentsâ€.\n",
      "\n",
      "\n",
      "\n",
      "Each word is represented by more than one spot because words tend to have several meanings. One part of the brain, for example, reliably responds to the word â€œtopâ€, along with other words that describe clothing. But the word â€œtopâ€ activates many other regions. One of them responds to numbers and measurements, another to buildings and places. The scientists have created an interactive website where the public can explore the brain atlas.\n",
      "\n",
      "\n",
      "\n",
      "Strikingly, the brain atlases were similar for all the participants, suggesting that their brains organised the meanings of words in the same way. The scientists only scanned five men and two women, however. All are native English speakers, and two are authors of the study published in Nature. It is highly possible that people from different backgrounds and cultures will have different semantic brain atlases.\n",
      "\n",
      "\n",
      "\n",
      "Armed with the atlas, researchers can now piece together the brain networks that represent wildly different concepts, from numbers to murder and religion. â€œThe idea of murder is represented a lot in the brain,â€ Gallant said.\n",
      "\n",
      "\n",
      "\n",
      "Using the same haul of data, the group has begun work on new atlases that show how the brain holds information on other aspects of language, from phonemes to syntax. A brain atlas for narrative structure has so far proved elusive, however. â€œEvery time we come up with a set of narrative features, we get told they arenâ€™t the right set of narrative features,â€ said Gallant.\n",
      "\n",
      "\n",
      "\n",
      "Uri Hasson, a neuroscientist at Princeton University, praised the work. Unlike many studies that looked at brain activity when an isolated word or sentence was spoken, Gallantâ€™s team had shed light on how the brain worked in a real-world scenario, he said. The next step, he said, was to create a more comprehensive and precise semantic brain atlas. Ultimately, Hasson believes it will be possible to reconstruct the words a person is thinking from their brain activity. The ethical implications are enormous. One more benign use would see brain activity used to assess whether political messages have been effectively communicated to the public. â€œThere are so many implications, and we are barely touching the surface,â€ he said.\n",
      "\n",
      "\n",
      "\n",
      "Lorraine Tyler, a cognitive neuroscientist and head of the Centre for Speech, Language and the Brain at Cambridge University said the research was a â€œtour de force in its scope and methodsâ€. But the brain atlas in its current form does not capture fine differences in word meanings. Take the word â€œtableâ€. It can be a member of many different groups, says Tyler. â€œIt can be something to eat off, things made of wood, things that are heavy, things having four legs, non-animate objects, and so on. This kind of detailed semantic information that enables words to be used flexibly is lost in the analysis,â€ she said. â€œWhile this research is path-breaking in its scope, there is still a lot to learn about how semantics is represented in the brain.â€\n",
      "['', 'created an â€œatlas o', 'atlas of the brainâ€', 'reveals how the meanin', 'arranged across different', 'across different regions', 'different regions of th', '', '', '', '', '', 'atlas of the brainâ€ th', '', '', '', '', '', '', '', 'atlas of the br', '', '', '', '', '', '', '', '', '', '', '', 'regions of the o', '', '', 'ave created an', '', '', '', '', '', '', '', 'different regions of ', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'atlas of the brainâ€ that ', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'created an â€œatlas of th', 'atlas of the brainâ€ that ', '', '', '', '', '', '', 'the brainâ€ tha', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'the brainâ€ tha', '', '', '', 'the brainâ€ that reveals', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'different region', '', 'the brainâ€ that', 'atlas of the brai', '', '', '', '', 'regions of the or', '', '', '', '', '', '', '', 'regions of the organ.\\n', '', 'the brainâ€ that rev', 'regions of the organ.\\n', '', '', '', '', 'ons of the organ.\\n', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'regions of the organ', '', '', '', 'the brainâ€ that reveal', '', 'created an â€œatlas of the br', '', '', '', '', '', '', 'the brainâ€ that reveal', '', '', '', '', '', '', '', '', '', '', 'different regions of the organ', '', 'different regions of the', '', '', 'atlas of the brainâ€ tha', '', '', '', 'different regions of the o', '', '', 'the brainâ€ that', '', '', '', '', '', '', '', '', '', 'ave created a', 'atlas of the brainâ€ that ', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'the brainâ€ th', '', '', 'created an â€œatlas of the bra', '', '', 'atlas of the brainâ€ tha', '', '', '', '', 'the brainâ€ that reveals ', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'atlas of the brain', '', '', '', '', '', '', '', 'different regions of ', '', '', '', 'eated an â€œatla', '', '', '', '', '', '', '', 'ons of the o', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '', '', 'atlas displays in r', '', '', '', '', '', 'colourful quilt laid', 'quilt laid over t', 'laid over the cor', 'cortex, the atlas dis', 'atlas displays in rain', 'displays in rainbow h', 'rainbow hues how indivi', 'concepts they convey ca', 'convey can be grouped t', 'together in clumps of', 'clumps of white mat', 'ourful quilt l', 'atlas displays ', '', '', '', '', '', '', '', '', '', '', '', '', '', 'concepts they co', 'a colourful qu', '', '', '', '', 'and the concept', '', '', '', '', 'together in clumps of ', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'atlas displays in rainbow', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'together in clu', '', 'atlas displays in rainbow', '', '', '', '', '', '', 'the cortex, th', '', '', '', '', 'they convey can be group', '', '', '', '', '', '', '', '', '', '', '', '', '', 'the cortex, th', '', '', '', 'the cortex, the atlas d', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'lays in rain', 'the cortex, the', 'atlas displays in', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'the cortex, the atl', '', '', '', '', '', 'oncepts they convey', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'the cortex, the atlas ', '', '', '', '', '', '', '', '', 'the cortex, the atlas ', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'atlas displays in rainb', '', 'together in clumps of w', '', '', 'concepts they convey ca', '', 'the cortex, the', '', '', '', '', '', '', '', '', '', 'a colourful q', 'atlas displays in rainbow', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'the cortex, t', '', '', '', '', '', 'atlas displays in rainb', '', '', '', '', 'the cortex, the atlas di', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'atlas displays in ', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'oncepts they', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '', '', 'atlas that shows ho', '', '', '', '', '', '', '', '', '', 'atlas that shows how o', '', '', '', '', '', '', '', 'atlas that show', 'shows how one spec', 'one specific aspect', 'specific aspect of langu', 'aspect of language is repre', 'language is represented in', 'represented in the bra', 'case semantics, or the ', 'semantics, or the meanin', 'said Jack Gallant', '', '', '', 'one specific asp', '', 'al was to buil', '', '', '', '', '', '', '', '', '', '', 'represented in the brai', '', '', '', '', '', '', '', 'allant, a neurosci', '', '', '', '', 'one specific aspect of ', '', '', 'atlas that shows how one ', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'said Jack Gallant, ', '', '', '', '', 'one specific aspe', '', 'language is represente', '', '', '', '', '', '', '', 'to build a gian', '', 'atlas that shows how one ', '', '', '', '', '', '', 'the brain, in ', '', '', '', 'shows how one spe', '', '', '', '', '', 'shows how one speci', '', '', '', '', '', '', '', '', 'the brain, in ', '', '', '', 'the brain, in this case', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'the brain, in t', 'atlas that shows ', 'shows how one speci', '', '', '', '', 'fornia, Berkeley.\\n', '', '', '', '', 'one specific asp', '', '', '', 'the brain, in this ', '', '', '', '', '', 'one specific aspect', '', '', '', 'one specific as', '', '', '', '', '', 'represented in the b', 'one specific a', '', '', '', 'one specific a', '', '', '', '', '', '', '', '', '', '', 'one specific aspect ', '', '', 'the brain, in this cas', '', '', '', '', '', '', '', '', 'the brain, in this cas', '', '', '', '', 'allant, a neurosci', '', '', '', '', '', '', '', '', 'semantics, or the mean', '', 'atlas that shows how on', '', '', 'represented in the brain, ', '', '', '', 'the brain, in t', 'represented in the br', '', '', '', '', 'shows how one sp', '', 'language is represented ', '', 'al was to bui', 'atlas that shows how one ', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'said Jack Gal', 'the brain, in', '', 'said Jack Gallant, a neur', '', '', 'semantics, or the me', 'atlas that shows how on', '', '', '', '', 'the brain, in this case ', '', '', 'one specific a', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'language is represented ', '', '', 'said Jack Gallant,', '', '', '', '', '', 'atlas that shows h', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'one specific', 'this case semantic', '', '', 'semantics, or the meanings o', '', '', '', '', '', 'said Jack Gallant, ', '', '', '', '', '', 'semantics, or the meanings ', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '', '', '', '', '', '', 'different brain spots. ', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'one word or concept', '', '', '', '', '', '', '', '', 'single brain region', 'region holds one', 'one word or conc', 'concept. A singl', 'ain region hol', 'single brain regi', 'spot is associated wit', 'associated with a number ', 'related words. An', 'and love; death', 'single brain regio', 'lights up many differ', 'different brain spots', 'spots. Together the', '', 'represent the meanings ', 'use: life and', 'life and love; ', 'love; death and ', 'death and taxes; c', 'taxes; clouds, Flori', 'clouds, Florida an', '', '', '', '', 'death and ta', '', 'one word or concept. A ', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'one word or conce', 'use: life and love', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'ther they make', '', 'use: life and', '', '', 'they make up networks th', '', '', '', '', '', 'related words. And each', '', '', '', '', 'spots. Together they ', '', '', 'ther they make', '', '', '', 'ther they make up netwo', '', '', '', '', '', '', '', '', '', '', '', 'tworks that repre', '', 'each single wor', '', '', '', '', 'different brain ', '', 'ther they make ', '', '', 'related words. And eac', '', '', '', '', '', '', '', '', 'one word or conc', '', '', '', 'ther they make up n', 'region holds one word ', '', '', '', '', 'on holds one word o', '', '', '', 'one word or con', 'spots. Together they m', '', '', '', 'each single word ligh', '', 'one word or co', 'spot is associa', '', '', 'one word or co', '', '', '', '', '', '', '', '', '', '', 'one word or concept.', '', '', 'ther they make up netw', '', '', '', '', '', '', '', '', 'ther they make up netw', '', '', '', 'tworks that repre', '', '', '', 'tworks that repre', '', '', 'different brain spots. Togethe', '', 'different brain spots. T', '', '', '', '', '', 'represent the meanings of ', 'different brain spots. Tog', '', '', 'ther they make ', '', '', '', '', '', '', '', '', '', 'ain region ho', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'lights up many dif', '', '', '', 'ther they mak', '', '', '', '', '', '', '', '', '', '', 'ther they make up networ', '', '', 'one word or co', 'use: life and', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'death and taxe', '', '', '', '', '', '', '', '', '', '', 'different brain spots', '', '', '', 'eath and taxes', '', '', '', '', '', '', '', 'on holds one', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '', '', 'atlas demonstrates ', '', '', '', '', '', '', '', '', '', 'atlas demonstrates how', '', '', '', '', '', '', 'our de forceâ€ ', 'atlas demonstra', '', 'one researcher who ', '', '', '', '', '', '', '', '', '', '', 'one researcher w', '', 'as a â€œtour de ', '', '', '', '', 'and other field', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'tour de force', 'de forceâ€ by', 'forceâ€ by one resear', 'one researcher who was ', 'researcher who was not in', 'study, the atlas demonst', 'atlas demonstrates how mo', 'demonstrates how modern ima', 'transform our knowledge o', 'knowledge of how the bra', 'tasks. With further', 'advances, the technology ', 'technology could have a p', 'could have a profound', '', 'could have a prof', '', '', '', '', '', '', '', '', '', 'study, the atlas de', 'one researcher wh', '', '', '', 'could have a profo', '', '', '', '', '', 'tour de forceâ€ ', '', 'atlas demonstrates how mo', '', '', '', '', '', '', 'the study, the', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'the study, the', '', '', '', 'the study, the atlas de', '', '', '', '', 'could have a profoun', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'the study, the ', 'atlas demonstrate', '', '', '', '', '', 'forceâ€ by one resear', '', '', '', 'earcher who ', 'one researcher w', '', '', '', 'the study, the atla', '', '', '', '', '', 'one researcher who ', '', '', '', 'one researcher ', '', '', '', '', '', '', 'one researcher', '', '', '', 'one researcher', '', '', '', '', '', '', '', '', '', '', 'one researcher who w', '', '', 'the study, the atlas d', '', '', '', '', '', '', '', '', 'the study, the atlas d', '', '', '', '', '', '', '', '', '', 'study, the atlas demon', '', '', '', '', '', 'atlas demonstrates how ', '', '', '', '', '', '', 'the study, the ', '', '', '', '', '', '', '', '', '', 'as a â€œtour de', 'atlas demonstrates how mo', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'the study, th', '', '', '', '', '', 'atlas demonstrates how ', '', '', '', '', 'the study, the atlas dem', '', '', 'one researcher', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'researcher who w', 'tour de force', 'de forceâ€ by o', 'forceâ€ by one resea', '', 'atlas demonstrates', '', 'form our knowledg', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'one research', '', '', '', '', '', '', '', '', '', '', 'researcher who was not invo', '', '', '', '', '', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'one disease or lock', '', '', 'language decoder that coul', '', '', '', 'said Alexander Hu', '', '', '', 'one disease or l', '', 'at this approa', '', '', '', '', 'ander Huth, the', '', '', '', '', '', '', 'used to decod', '', '', '', '', '', '', 'allow people silen', '', '', 'decode infor', '', 'one disease or locked-i', '', 'study. One potential use', '', '', '', '', '', '', '', 'could be used to deco', 'approach could be u', 'could be used to ', 'used to decode informat', 'decode information about', 'reading, or possibly ', 'even thinking,â€ sa', 'thinking,â€ said Alexand', 'said Alexander Huth', '', 'first author on th', 'author on the st', 'study. One potentia', 'one disease or lo', 'used to decode inf', 'language decoder that ', 'decoder that could ', 'could be used to d', 'allow people silenced', 'silenced by motor neur', 'disease or locked-in synd', '', 'syndrome to speak throu', 'to decode infor', '', '', '', '', '', '', '', 'reading, or p', 'the first auth', '', 'used to decod', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'used to decode i', '', 'the first auth', '', '', '', 'the first author on the', '', '', '', '', 'could be used to dec', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'the first autho', '', '', '', '', '', '', 'formation about what', '', '', '', 'earing, read', 'one disease or l', '', '', '', 'the first author on', '', '', '', '', '', 'on about what words', '', '', '', 'one disease or ', '', '', '', '', '', '', 'one disease or', '', '', '', 'one disease or', '', '', '', '', '', '', '', '', '', '', 'one disease or locke', '', '', 'the first author on th', '', '', '', '', '', '', '', '', 'the first author on th', '', '', '', '', 'allow people silen', '', '', '', '', 'study. One potential u', '', '', '', '', '', '', '', '', '', '', '', '', 'the first autho', '', '', '', '', '', '', '', 'language decoder that co', '', 'at this appro', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'said Alexande', 'the first aut', '', 'said Alexander Huth, the ', '', '', '', '', '', '', 'thinking,â€ said Alexand', '', 'the first author on the ', '', '', 'one disease or', 'used to decod', '', '', 'used to decode info', '', '', '', '', '', '', '', '', '', '', 'language decoder that co', '', '', 'said Alexander Hut', '', '', 'decode informa', '', '', '', '', 'formation about w', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'on about wha', 'this approach coul', '', '', '', '', 'used to decode inf', '', '', '', 'said Alexander Huth', '', '', '', '', '', '', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', 'scientists recorded peop', '', 'atlas, the scientis', '', '', '', '', '', '', '', '', 'cortex.\\n', 'atlas, the scientists ', '', '', '', '', '', '', 'our, a US radi', 'atlas, the scie', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'ate the atlas,', '', 'spots all over the cer', '', 'related words tri', '', '', '', '', 'spots all over the ', '', '', '', '', '', '', '', '', '', 'all over the cereb', '', '', 'ded peopleâ€™s', '', '', '', '', 'atlas, the scientists rec', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'to stories read', 'create the atlas, the s', 'atlas, the scientists rec', 'scientists recorded peopleâ€™', 'recorded peopleâ€™s brai', 'activity while they liste', 'listened to stories r', 'stories read out', 'read out on T', 'the atlas, the', 'radio show. T', '', 'radio show. The', 'show. They then m', 'they listened to stories', 'transcripts of the storie', 'stories read out on Th', 'activity while the', 'data to show how', 'show. They then mat', 'related words triggered', 'triggered neural responses', 'responses in 50,000 t', '', '', 'spots all over the ce', '', 'stories read out', 'the atlas, the', 'radio show. They', '', '', 'the atlas, the scientis', '', 'stories read out on The Moth', '', 'scientists recorded peo', '', '', '', '', '', '', 'listened to storie', '', 'stories read out', '', '', '', '', '', '', '', 'the atlas, the ', 'atlas, the scient', '', 'related words triggere', '', '', '', '', '', '', '', '', '', '', '', '', 'the atlas, the scie', '', '', '', '', '', 'on The Moth Radio H', '', '', '', '', 'spots all over the cer', '', '', '', '', '', '', 'spots all over ', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'the atlas, the scienti', 'scientists recorded peopleâ€™s b', '', '', '', '', '', '', '', 'the atlas, the scienti', 'scientists recorded peo', '', '', '', 'all over the cereb', '', '', '', '', '', '', '', '', '', '', 'atlas, the scientists r', '', '', '', '', '', '', 'the atlas, the ', '', '', '', 'data to show how', '', 'show. They then ', '', '', '', 'ate the atlas', 'atlas, the scientists rec', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'activity while they li', '', '', '', '', '', '', '', '', 'the atlas, th', '', '', 'create the atlas, the scient', '', '', 'atlas, the scientists r', '', '', '', 'activity while they ', 'the atlas, the scientist', '', '', '', '', '', 'activity while they ', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'ded peopleâ€™s b', '', '', 'atlas, the scienti', '', '', '', '', '', '', '', '', '', '', '', 'eate the atlas', '', '', '', '', '', '', '', 'on The Moth ', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', 'scientists could be that', '', '', '', '', '', 'different words - as th', '', '', '', '', '', '', '', '', '', '', '', '', 'our because th', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'adio Hour beca', '', '', '', '', 'and compelling.', '', '', 'different words - as ', '', '', '', 'used stories ', '', '', '', '', '', '', 'alling the stories', '', '', 'dent the sci', '', '', '', '', '', '', '', '', '', '', '', 'could be that the peo', '', 'could be that the', 'used stories from The M', '', '', 'even people listen', '', '', '', '', '', '', '', 'used stories from ', '', '', 'could be that the ', '', '', '', '', '', 'tories from The', '', '', 'scientists could be that th', '', '', 'listened to two hours', 'stories from The', '', 'they are short', '', 'used stories ', '', '', 'they are short and compe', '', 'stories from The Moth ', '', '', '', '', '', '', '', '', '', 'used stories fro', 'stories from The', 'they are short', '', 'short and compelling', 'compelling. The more enthr', 'they are short and comp', 'enthralling the stories, the ', 'stories from The Moth Radio ', 'confident the scientists c', 'scientists could be tha', 'could be that the pe', 'scanned were focusing ', 'focusing on the words a', 'drifting off. Seve', 'off. Seven peopl', '', 'listened to two ho', 'two hours of stor', 'stories from The', 'each. Per perso', 'amounted to hearing roug', 'roughly 25,000 word', '', '', 'different words ', 'lay in the s', 'they are short ', '', '', '', '', '', '', '', '', '', '', 'earing rough', '', '', '', '', 'they are short and ', '', '', '', '', '', 'onfident the scient', '', '', '', '', '', '', '', '', 'each. Per person, tha', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'they are short and com', 'scientists could be that the p', '', '', '', '', '', '', '', 'they are short and com', 'scientists could be tha', 'scanned were foc', '', 'two hours of stor', 'alling the stories', '', '', 'two hours of stor', '', '', 'different words - as they lay ', '', 'different words - as the', '', '', '', '', '', '', 'different words - as they ', '', '', 'they are short ', '', '', 'using on the wo', '', '', '', '', '', '', 'adio Hour bec', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'they are shor', '', '', '', '', '', '', '', '', '', '', 'they are short and compe', '', '', '', 'used stories ', '', '', 'used stories from T', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'dent the scien', '', '', '', '', '', '', '', '', '', '', 'different words - as ', '', '', '', '', 'off. Seven peop', '', '', '', '', '', '', 'onfident the', '', '', '', '', '', 'used stories from ', '', '', '', '', '', '', '', '', '', '', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '', '', 'atlas shows how wor', '', '', '', '', '', '', '', '', '', 'atlas shows how words ', '', '', '', '', '', '', '', 'atlas shows how', 'shows how words an', 'one of the tiny reg', '', '', '', '', '', '', '', '', '', 'regions of the b', 'one of the tiny ', '', 'atlas shows ho', '', 'spots activated by fam', '', 'related terms exe', 'and related ter', '', '', '', 'spots activated by ', '', 'represents the word â€œvi', '', '', '', '', '', '', '', '', '', '', 'de of the br', '', 'one of the tiny regions', '', '', 'atlas shows how words and', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'one of the tiny r', '', '', '', '', '', '', '', '', '', 'to â€œkilledâ€, â€œc', '', 'atlas shows how words and', '', '', '', '', '', '', 'the same regio', '', 'usbandâ€, â€œchi', '', 'shows how words a', '', '', '', '', '', 'shows how words and', 'related terms exercise ', '', '', '', '', 'spots activated by fa', '', '', 'the same regio', '', '', '', 'the same regions of the', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'the same region', 'atlas shows how w', 'shows how words and', 'related terms exercise', 'terms exercise the sam', 'exercise the same regi', 'regions of the br', '', 'example, on the left-', '', 'side of the br', 'ear, is one ', 'one of the tiny ', 'tiny regions that repre', 'regions of the brain. F', 'represents the word â€œv', 'the same regions of', 'regions of the brain. ', 'responds to â€œkilledâ€, â€œco', 'killedâ€, â€œconvictedâ€, â€œmu', 'convictedâ€, â€œmurderedâ€ and â€œ', 'confessedâ€. On the ', 'ons of the brain. F', '', 'side of the b', 'top of the h', 'one of the tiny', 'spots activated by fam', 'activated by family te', 'family terms: â€œwi', 'terms exercise the', '', '', 'one of the tin', 'spots activated', '', '', 'one of the tin', 'example, on the left-hand', '', 'responds to â€œkill', 'top of the head', '', '', '', 'top of the head, i', '', 'regions of the brain', 'one of the tiny regi', 'responds to â€œkilledâ€, â€œconvic', '', 'the same regions of th', '', '', '', '', '', '', '', '', 'the same regions of th', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'atlas shows how words a', '', '', 'represents the word â€œvicti', '', '', '', 'the same region', '', '', '', '', '', 'shows how words ', '', '', '', 'atlas shows h', 'atlas shows how words and', '', '', '', '', '', '', '', '', '', 'arentsâ€.\\n', 'right-hand side, ne', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'the same regi', '', '', '', '', '', 'atlas shows how words a', '', '', '', '', 'the same regions of the ', '', '', 'one of the tin', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'de of the brai', '', '', 'atlas shows how wo', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'ons of the b', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', 'scientists have created ', 'created an interact', 'atlas.\\n', '', '', '', '', '', '', '', '', '', 'atlas.\\n', '', '', '', '', '', '', '', 'atlas.\\n', '', 'one spot because wo', '', '', '', 'represented by more th', '', '', '', '', '', 'regions. One of ', 'one spot because', '', 'ach word is re', '', 'spot because words ten', '', '', 'and measurement', '', '', '', '', '', 'represented by more tha', 'use words ten', '', '', '', '', '', '', '', '', '', 'describe clo', '', 'one spot because words ', '', '', 'atlas.\\n', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'one spot because ', 'use words tend to ', '', '', '', '', '', '', '', '', 'to have several', 'created an interactive ', 'atlas.\\n', 'scientists have created an ', '', '', '', '', '', 'the brain, for', '', 'use words ten', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'the brain, for', '', '', '', 'the brain, for example,', '', '', '', 'scientists have created', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'the brain, for ', 'atlas.\\n', '', '', '', '', 'regions. One of t', 'for example, reliabl', 'example, reliably res', '', '', '', 'one spot because', '', 'regions. One of them re', '', 'the brain, for exam', 'regions. One of them r', 'responds to the word â€œtop', '', '', '', 'one spot because wo', '', '', 'topâ€, along ', 'one spot becaus', '', '', '', '', '', 'represented by more ', 'one spot becau', 'spot because wo', 'tend to have several ', 'several meanings. On', 'one spot becau', 'example, reliably respond', 'reliably responds to t', 'responds to the w', 'topâ€, along wit', 'along with other wor', 'describe clothing. Bu', 'clothing. But the', 'topâ€, along with o', 'activates many other r', 'regions. One of them', 'one spot because wor', 'responds to the word â€œtopâ€, a', 'another to buildings and', 'the brain, for example', 'scientists have created an int', 'created an interactive webs', 'explore the brain a', '', '', '', '', '', 'the brain, for example', 'scientists have created', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'atlas.\\n', '', '', 'represented by more than o', '', '', '', 'the brain, for ', 'represented by more t', 'lothing. But the ', '', '', '', '', '', '', '', 'ach word is r', 'atlas.\\n', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'the brain, fo', '', '', 'created an interactive websi', '', '', 'atlas.\\n', '', '', '', '', 'the brain, for example, ', '', '', 'one spot becau', 'use words ten', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'describe cloth', '', '', 'atlas.\\n', '', '', '', '', '', '', '', '', '', '', '', 'eated an inter', '', '', '', '', '', '', '', 'one spot bec', '', '', '', '', '', '', '', '', '', '', '', '', '', 'lothing. But the wo', '', '', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', 'scientists only scanned ', '', 'atlases were simila', '', '', '', 'different backgrounds a', '', '', '', '', '', 'atlases were similar f', '', '', '', '', '', '', '', 'atlases were si', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'ain atlases we', '', '', '', '', 'and two women, ', '', '', 'different backgrounds', '', '', '', '', '', '', '', '', '', '', 'all the participan', '', '', '', '', '', '', 'study published in Natur', 'atlases were similar for ', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'authors of the s', 'study published in ', '', '', '', '', '', '', '', '', '', '', '', '', 'atlases were similar for ', 'scientists only scanned fiv', '', '', '', '', '', 'the brain atla', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'the brain atla', '', '', '', 'the brain atlases were ', '', '', '', 'scientists only scanned', '', 'scanned five men and t', '', '', '', '', '', 'two women, howeve', '', '', '', '', '', '', 'different backgr', '', 'the brain atlas', 'atlases were simi', '', '', '', '', '', 'for all the particip', '', '', '', '', '', '', '', '', 'the brain atlases w', '', '', '', '', '', 'only scanned five m', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'the brain atlases were', 'scientists only scanned five m', '', '', '', 'atlases were similar for all', 'similar for all the participant', 'suggesting that their brain', 'organised the meanings o', 'the brain atlases were', 'scientists only scanned', 'scanned five men', 'five men and', 'two women, howeve', 'all the participan', '', 'speakers, and two ar', 'two women, howeve', 'authors of the study pu', 'study published in Nat', 'different backgrounds and cult', 'cultures will have differen', 'different backgrounds an', 'semantic brain atlases', '', 'atlases were similar fo', '', '', '', 'different backgrounds and ', '', '', 'the brain atlas', '', '', '', '', 'atlases were simil', '', '', '', '', 'ain atlases w', 'atlases were similar for ', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'shed in Nature. ', '', '', '', '', 'the brain atl', '', '', '', '', 'semantic brain atlas', 'atlases were similar fo', '', '', '', '', 'the brain atlases were s', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'atlases were simil', '', '', '', '', '', '', '', 'different backgrounds', '', '', '', '', '', '', '', '', '', '', '', 'only scanned', '', '', '', 'semantic brain atlases.\\n', '', '', '', '', '', '', '', '', '', '', '', '', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '', '', 'atlas, researchers ', '', '', '', 'different concepts, fro', '', '', '', '', '', 'atlas, researchers can', '', '', 'concepts, from numbers ', '', 'together the brain ne', '', '', 'atlas, research', '', '', '', '', '', 'represented a lot in t', '', '', 'said.\\n', '', '', '', '', 'concepts, from n', 'atlas, researc', '', '', '', '', 'and religion. â€œ', '', '', 'different concepts, f', '', 'together the brain net', 'represent wildly differ', '', '', '', '', '', '', '', 'allant said.\\n', '', '', 'der and reli', '', '', 'researchers can now piece', '', 'atlas, researchers can no', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'said.\\n', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'together the br', '', 'atlas, researchers can no', '', '', '', '', '', '', 'the atlas, res', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'the atlas, res', '', '', '', 'the atlas, researchers ', '', '', '', '', '', '', '', '', '', '', '', 'tworks that repre', '', '', '', '', '', '', 'different concep', '', 'the atlas, rese', 'atlas, researcher', '', '', '', '', '', '', '', '', '', 'earchers can', '', '', '', '', 'the atlas, research', '', '', '', '', '', 'oncepts, from numbe', '', '', '', '', '', '', '', '', '', 'represented a lot in', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'the atlas, researchers', '', '', '', '', '', '', '', '', 'the atlas, researchers', '', '', '', 'tworks that repre', 'allant said.\\n', '', '', 'tworks that repre', '', '', 'different concepts, from numbe', '', 'different concepts, from', '', '', 'atlas, researchers can ', 'researchers can now piece ', 'together the brain netw', 'represent wildly different', 'different concepts, from n', 'concepts, from numbers ', 'religion. â€œThe id', 'the atlas, rese', 'represented a lot in ', 'lot in the brain,', '', '', '', '', '', '', '', 'atlas, resear', 'atlas, researchers can no', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'said.\\n', 'the atlas, re', '', 'said.\\n', '', '', '', 'atlas, researchers can ', '', '', '', '', 'the atlas, researchers c', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'said.\\n', 'researchers can ', '', 'der and religi', '', '', 'atlas, researchers', '', '', '', '', '', '', '', 'different concepts, f', '', '', '', '', '', '', '', '', '', '', '', 'oncepts, fro', '', '', '', '', '', '', '', '', '', 'said.\\n', 'researchers can now piece t', '', '', 'lot in the brain,â€ ', '', '', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '', '', 'atlases that show h', '', '', '', '', '', '', '', '', '', 'atlases that show how ', '', '', '', '', '', '', '', 'atlases that sh', '', 'onemes to syntax. A', '', 'aspects of language, from p', 'language, from phonemes to', '', '', '', 'said Gallant.\\n', '', '', '', 'onemes to syntax', '', 'ame haul of da', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'allant.\\n', '', '', '', '', 'onemes to syntax. A bra', '', '', 'atlases that show how the', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'said Gallant.\\n', '', '', '', '', 'onemes to syntax.', '', 'language, from phoneme', '', '', '', '', '', '', '', 'to syntax. A br', '', 'atlases that show how the', '', '', '', '', '', '', 'the same haul ', '', 'usive, howeve', '', 'show how the brai', 'they arenâ€™t the right se', '', '', '', 'data, the group ', 'show how the brain ', '', '', '', '', '', '', '', '', 'the same haul ', '', '', '', 'the same haul of data, ', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'the same haul o', 'atlases that show', '', '', '', '', '', 'formation on other a', '', '', '', '', 'onemes to syntax', '', '', '', 'the same haul of da', '', '', '', '', '', 'on new atlases that', '', '', '', 'onemes to synta', '', '', '', '', '', '', 'onemes to synt', '', '', '', 'onemes to synt', '', '', '', '', '', '', '', '', '', '', 'onemes to syntax. A ', '', '', 'the same haul of data,', '', '', '', '', 'atlases that show how the br', '', '', '', 'the same haul of data,', '', '', '', '', 'allant.\\n', '', '', '', '', '', '', '', '', '', '', 'atlases that show how t', '', '', '', '', '', '', 'the same haul o', '', '', '', 'data, the group ', 'atlases that show ', 'show how the bra', 'aspects of language, from', 'language, from phonemes ', 'syntax. A brai', 'ame haul of d', 'atlases that show how the', 'structure has so far', 'far proved elusive', 'elusive, however. â€œEv', '', 'time we come ', 'come up with a set', 'set of narrative featu', 'features, we get ', 'told they arenâ€™t', '', 'right set of narrat', 'set of narrative featu', 'features, we get told', '', '', '', '', '', '', '', '', '', '', '', '', '', 'said Gallant.', 'the same haul', '', 'said Gallant.\\n', '', '', '', 'atlases that show how t', '', '', '', '', 'the same haul of data, t', '', '', 'onemes to synt', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'language, from phonemes ', '', '', 'said Gallant.\\n', '', '', '', '', '', 'atlases that show ', '', 'formation on othe', '', '', '', '', '', '', '', '', '', 'eatures, we ge', '', '', '', '', '', '', '', 'on new atlas', '', '', '', '', '', '', '', '', '', 'said Gallant.\\n', '', '', '', '', '', '', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '', '', 'atlas. Ultimately, ', '', '', '', '', 'like many studies th', '', '', '', '', 'atlas. Ultimately, Has', '', '', '', '', '', '', '', 'atlas. Ultimate', '', '', '', '', '', '', '', '', 'said. The next st', '', '', '', '', '', 'asson, a neuro', '', '', '', '', 'and precise sem', '', '', '', '', '', '', 'use would see', '', '', '', '', '', '', 'allantâ€™s team had ', '', '', '', '', '', '', '', 'atlas. Ultimately, Hasson', '', '', '', '', '', '', '', '', '', 'used to assess whether ', '', '', '', 'thinking from their bra', 'said. The next step', '', '', '', '', '', 'use would see brai', '', '', '', '', '', '', '', '', 'ton University,', 'create a more comprehen', 'atlas. Ultimately, Hasson', '', '', 'activity when an isolated', '', '', '', 'the work. Unli', '', 'us. One more ', '', '', '', '', '', 'activity when an i', '', '', '', '', '', '', '', '', 'used to assess w', '', 'the work. Unli', '', '', '', 'the work. Unlike many s', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'the work. Unlik', 'atlas. Ultimately', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'the work. Unlike ma', '', '', '', '', '', 'on, a neuroscientis', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'the work. Unlike many ', '', '', '', '', '', '', '', '', 'the work. Unlike many ', '', '', '', '', 'allantâ€™s team had ', '', '', '', '', '', '', '', '', 'semantic brain atlas. ', '', 'atlas. Ultimately, Hass', '', '', '', '', '', '', 'the work. Unlik', '', '', '', '', '', '', '', '', '', 'asson, a neur', 'atlas. Ultimately, Hasson', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'studies that looked ', 'looked at brain activ', 'activity when an isola', 'sentence was spoken, Gal', 'spoken, Gallantâ€™s te', 'team had shed l', 'shed light on ho', 'light on how the b', '', 'scenario, he said', 'said. The nex', 'the work. Unl', 'step, he said, w', 'said. The next step, he s', 'create a more comprehensive ', 'comprehensive and precise sema', 'semantic brain atlas', 'atlas. Ultimately, Hass', '', 'reconstruct the words a ', 'thinking from their bra', 'activity when an iso', 'the work. Unlike many st', 'ethical implications are enor', 'enormous. One more ', '', 'use would see', 'see brain activity', 'activity when an iso', 'used to assess whet', 'assess whether political', 'effectively communicated to the', 'communicated to the publi', '', 'touching the surface,', '', '', '', '', '', '', '', '', 'said. The next ste', '', '', '', '', '', 'atlas. Ultimately,', '', '', '', '', '', '', '', '', '', '', '', 'eate a more co', '', '', '', '', '', '', '', 'on, a neuros', '', '', '', 'semantic brain atlas. Ultima', '', 'used to assess whe', '', '', '', 'said. The next step', '', '', '', '', '', '', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '', '', 'atlas in its curren', '', '', '', 'different groups, says ', '', '', '', '', '', 'atlas in its current f', '', '', '', '', '', '', 'our de force i', 'atlas in its cu', '', '', '', '', '', 'represented in the bra', '', 'semantics is represented', 'said the research', '', '', '', '', '', 'aine Tyler, a ', '', '', '', '', 'and head of the', '', '', 'different groups, say', '', '', 'represented in the brai', 'used flexibly', '', '', '', '', '', '', '', '', 'tour de force', 'de force in ', 'force in its scope a', '', '', '', 'atlas in its current form', '', '', '', '', '', '', '', '', '', 'used flexibly is lost i', '', '', '', '', 'said the research w', '', '', '', '', '', 'used flexibly is l', '', '', '', '', '', '', '', '', 'tour de force i', '', 'atlas in its current form', '', '', '', '', '', '', 'the Centre for', '', 'used flexibly', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'used flexibly is', '', 'the Centre for', '', '', '', 'the Centre for Speech, ', '', '', '', '', '', '', '', '', 'off, things made', '', '', '', '', '', '', '', '', '', 'different groups', '', 'the Centre for ', 'atlas in its curr', '', '', '', '', '', 'for Speech, Language', '', '', '', 'earch was a ', '', '', '', '', 'the Centre for Spee', '', '', '', '', '', 'on-animate objects,', '', '', '', '', '', '', '', '', '', 'represented in the b', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'the Centre for Speech,', '', '', '', '', '', '', '', '', 'the Centre for Speech,', '', '', '', '', '', '', '', '', '', '', 'different groups, says Tyler. ', '', 'different groups, says T', 'semantic information t', '', 'atlas in its current fo', '', '', 'represented in the brain.â€', 'different groups, says Tyl', '', '', 'the Centre for ', 'represented in the br', 'lot to learn abou', '', '', '', '', '', '', '', 'aine Tyler, a', 'atlas in its current form', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'said the rese', 'the Centre fo', '', 'said the research was a â€œ', '', '', 'semantic information', 'atlas in its current fo', '', '', '', '', 'the Centre for Speech, L', '', '', '', 'used flexibly', '', '', 'used flexibly is lo', '', '', '', 'there is still a lot to', '', '', '', 'cognitive neuroscientist and ', '', '', '', '', '', 'said the research ', 'research was a â€œ', 'tour de force', 'de force in it', 'force in its scope ', 'scope and methods', 'atlas in its curre', 'current form does no', 'form does not cap', 'capture fine differences', 'fine differences in w', 'differences in word meani', '', 'tableâ€. It can ', 'different groups, say', 'says Tyler. â€œ', '', 'something to eat ', 'eat off, thing', 'off, things mad', 'things made of w', 'things made of wood', 'things made of w', 'four legs, non-anima', 'legs, non-animate objec', 'objects, and so', 'on-animate o', 'this research is p', 'kind of detailed seman', 'detailed semantic information', 'semantic information that en', 'enables words to b', 'used flexibly is l', 'flexibly is lost in th', 'lost in the analys', 'analysis,â€ she said', 'said the research w', 'research was a â€œtour de for', 'scope and metho', 'still a lot to ', 'lot to learn about ', 'learn about how semantics i', 'semantics is represented in']\n"
     ]
    }
   ],
   "source": [
    "with open(\"sciencearticle.txt\", \"r\") as testset:\n",
    "    word_list = ''\n",
    "    highlight_bucket = []\n",
    "    for line in testset:\n",
    "        print(line)\n",
    "        for trigram in metaphor_list:\n",
    "            trigram_list = trigram.split()\n",
    "            index = line.find(trigram_list[0])\n",
    "            index_len = len(trigram)\n",
    "            highlight_bucket.append(line[index:index+index_len])\n",
    "print(highlight_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "sample = ['the', 'methods', 'i', 'am', 'michelle']\n",
    "# word_list\n",
    "word_str = ' '.join(word_list)\n",
    "# print(word_str)\n",
    "for word in sample:\n",
    "    print(word_str.find(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_regex_strings():\n",
    "    stopwords_regex = \"(\"\n",
    "    # print(stopwords.words('english'))\n",
    "    for word in stopwords.words('english'):\n",
    "        stopwords_regex = stopwords_regex + word + \"|\"\n",
    "    stopwords_regex = stopwords_regex[:-1] + \"|an)\"\n",
    "#     print(stopwords_regex)\n",
    "    punctuation_regex = r\"[â€œâ€!\\\"#$%&'\\(\\)*+,-./:;<=>?@^_`{}~\\s]*\"\n",
    "    return stopwords_regex, punctuation_regex\n",
    "\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not found: disease lockedin syndrome\n",
      "Not found: lockedin syndrome speak\n",
      "Not found: scientists recorded peoples\n",
      "Not found: recorded peoples brain\n",
      "Not found: responses 50000 80000\n",
      "Not found: 50000 80000 peasized\n",
      "Not found: 80000 peasized spots\n",
      "Not found: roughly 25000 words\n",
      "Not found: 25000 words 3000\n",
      "Not found: 3000 different words\n",
      "Not found: for example lefthand\n",
      "Not found: example lefthand side\n",
      "Not found: lefthand side brain\n",
      "Not found: confessed on brains\n",
      "Not found: on brains righthand\n",
      "Not found: righthand side near\n",
      "Not found: told arent right\n",
      "Not found: arent right set\n",
      "Not found: sentence spoken gallants\n",
      "Not found: spoken gallants team\n",
      "Not found: realworld scenario said\n",
      "Not found: four legs nonanimate\n",
      "Not found: legs nonanimate objects\n",
      "Not found: research pathbreaking scope\n",
      "[(0, 24), (16, 19), (28, 19), (53, 22), (91, 25), (100, 24), (107, 23), (140, 20), (147, 20), (157, 17), (163, 17), (177, 21), (189, 22), (195, 21), (207, 23), (249, 23), (263, 23), (285, 21), (297, 19), (323, 14), (353, 15), (364, 18), (374, 19), (378, 24), (387, 27), (397, 26), (409, 22), (443, 23), (448, 24), (486, 17), (529, 30), (569, 19), (582, 16), (595, 16), (607, 16), (616, 14), (618, 17), (631, 22), (639, 25), (667, 17), (682, 15), (691, 18), (703, 21), (718, 21), (734, 19), (741, 22), (777, 23), (816, 13), (821, 15), (830, 16), (836, 18), (846, 20), (853, 18), (861, 15), (878, 18), (912, 17), (928, 13), (933, 12), (936, 20), (946, 23), (950, 25), (989, 24), (1000, 25), (1006, 27), (1042, 25), (1056, 24), (1119, 19), (1139, 25), (1153, 25), (1164, 21), (1250, 19), (1259, 17), (1268, 23), (1276, 24), (1333, 21), (1354, 18), (1359, 23), (1370, 19), (1375, 20), (1395, 18), (1401, 16), (1415, 19), (1422, 17), (1436, 18), (1451, 22), (1460, 19), (1473, 18), (1479, 21), (1492, 22), (1539, 23), (1578, 15), (1581, 23), (1592, 25), (1638, 25), (1658, 21), (1670, 16), (1678, 13), (1690, 14), (1699, 13), (1713, 13), (1716, 15), (1722, 17), (1728, 24), (1750, 25), (1769, 22), (1792, 18), (1801, 16), (1809, 19), (1828, 23), (1842, 26), (1899, 21), (1941, 16), (1670, 16), (1690, 14), (1968, 16), (1996, 20), (2006, 26), (2018, 23), (2027, 29), (2043, 28), (2061, 26), (2075, 23), (2086, 20), (2117, 22), (2130, 23), (2160, 18), (2169, 16), (2174, 21), (2187, 18), (2199, 17), (2212, 16), (2220, 15), (2243, 24), (2305, 16), (2316, 12), (2352, 15), (2356, 17), (2362, 19), (2382, 22), (2390, 22), (2396, 22), (2414, 17), (2466, 14), (2495, 12), (2503, 16), (2514, 23), (2519, 23), (2532, 22), (2562, 19), (2571, 22), (2578, 25), (2591, 25), (2601, 28), (2667, 13), (2682, 12), (2702, 15), (2719, 22), (2725, 22), (2738, 17), (2745, 18), (2795, 21), (2808, 20), (2833, 14), (2837, 15), (2856, 21), (2869, 20), (2887, 14), (2914, 25), (2923, 22), (2932, 17), (2954, 15), (2960, 20), (2988, 21), (2997, 17), (3021, 18), (3026, 22), (3047, 20), (3056, 20), (3068, 29), (3106, 24), (3139, 22), (3143, 30), (3159, 27), (3211, 19), (3237, 24), (3259, 28), (3272, 31), (3306, 27), (3335, 24), (3384, 22), (3388, 23), (3404, 16), (3412, 12), (3425, 17), (3445, 18), (3460, 20), (3468, 20), (3482, 17), (3490, 23), (3505, 22), (3571, 30), (3597, 27), (3616, 24), (3626, 22), (3651, 23), (3666, 23), (3673, 26), (3699, 23), (3732, 26), (3749, 26), (3759, 23), (3796, 17), (3807, 15), (3829, 21), (3843, 17), (3877, 15), (3900, 16), (3938, 18), (3951, 16), (3997, 25), (4008, 24), (4035, 14), (4043, 13), (4051, 25), (4071, 20), (4088, 18), (4099, 21), (4118, 15), (4124, 13), (4132, 18), (4147, 22), (4164, 17), (4202, 19), (4147, 22), (4225, 21), (4251, 25), (4293, 23), (4323, 19), (4335, 20), (4348, 21), (4364, 22), (4429, 15), (4438, 16), (4443, 18), (4489, 17), (4502, 13), (4508, 13), (4517, 16), (4526, 25), (4539, 28), (4553, 30), (3626, 20), (4594, 23), (4601, 26), (4652, 24), (4686, 23), (4712, 20), (4722, 24), (4726, 29), (4751, 19), (4761, 14), (4777, 13), (4787, 18), (4797, 20), (4806, 19), (4814, 24), (4858, 31), (4870, 25), (4899, 23), (4949, 21), (4982, 24), (4991, 30), (5000, 29), (5041, 22), (5052, 21), (5060, 24), (5086, 25), (5096, 24), (5107, 18), (5116, 16), (928, 13), (5137, 14), (5140, 19), (5153, 17), (5187, 18), (5200, 20), (5208, 17), (5222, 24), (5230, 21), (5235, 25), (5265, 15), (5280, 15), (5315, 21), (5333, 13), (5338, 18), (5356, 17), (5369, 14), (5373, 15), (5378, 16), (5399, 19), (5422, 16), (5459, 15), (5475, 12), (5479, 18), (5484, 22), (5492, 29), (5501, 28), (5527, 18), (5547, 18), (5552, 22), (5564, 18), (5576, 19), (5591, 19), (5642, 15), (5658, 15), (5666, 19), (5673, 27), (5689, 27)]\n"
     ]
    }
   ],
   "source": [
    "# with open(\"sciencearticle.txt\", \"r\") as testset:\n",
    "def Rule1_metaphor(article):\n",
    "    article = article.lower()\n",
    "    word_list = ''\n",
    "    highlight_bucket = []\n",
    "    tag_list = []\n",
    "    stopwords_regex, punctuation_regex = create_regex_strings()\n",
    "    \n",
    "    for trigram in metaphor_list:\n",
    "#         print(trigram)\n",
    "        trigram_list = trigram.split(\" \")\n",
    "        regex_string = \"(\" + trigram_list[0] + punctuation_regex + \"(\" + stopwords_regex + punctuation_regex + \")*\" + trigram_list[1] + punctuation_regex + \"(\" + stopwords_regex + punctuation_regex + \")*\" + trigram_list[2] + \")\" \n",
    "#         print(regex_string)\n",
    "#         print(article)\n",
    "        match = re.search(regex_string, article)\n",
    "        if match is not None:\n",
    "            complete_trigram = match.group(0)\n",
    "#             print(trigram, complete_trigram)\n",
    "            index = article.find(complete_trigram)\n",
    "#             print(index, complete_trigram)\n",
    "            index_len = len(trigram)\n",
    "            highlight_bucket.append(article[index:index+index_len])\n",
    "            tag_list.append((index, index_len))\n",
    "#             article = article[index+index_len:]\n",
    "        else:\n",
    "             print(\"Not found: \" + trigram)\n",
    "    \n",
    "#     for line in testset:\n",
    "# #         print(line)\n",
    "#         for trigram in metaphor_list:\n",
    "#             trigram_list = trigram.split()\n",
    "#             index = line.find(trigram_list[0])\n",
    "#             index_len = len(trigram)\n",
    "#             highlight_bucket.append(line[index:index+index_len])\n",
    "#             line = line[index+index_len:]\n",
    "#         print(highlight_bucket)\n",
    "#         len(highlight_bucket)\n",
    "        \n",
    "    return tag_list\n",
    "with open(\"sciencearticle.txt\", \"r\") as testset:   \n",
    "#     print(metaphor_list)\n",
    "    print(Rule1_metaphor(testset.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "scientists created atlas: metaphor\n",
      "created atlas brain: metaphor\n",
      "atlas brain reveals: metaphor\n",
      "brain reveals meanings: NOT metaphor\n",
      "reveals meanings words: metaphor\n",
      "meanings words arranged: NOT metaphor\n",
      "words arranged across: NOT metaphor\n",
      "arranged across different: metaphor\n",
      "across different regions: metaphor\n",
      "different regions organ: metaphor\n",
      "like colourful quilt: metaphor\n",
      "colourful quilt laid: metaphor\n",
      "quilt laid cortex: metaphor\n",
      "laid cortex atlas: metaphor\n",
      "cortex atlas displays: metaphor\n",
      "atlas displays rainbow: metaphor\n",
      "displays rainbow hues: metaphor\n",
      "rainbow hues individual: metaphor\n",
      "hues individual words: NOT metaphor\n",
      "individual words concepts: NOT metaphor\n",
      "words concepts convey: NOT metaphor\n",
      "concepts convey grouped: metaphor\n",
      "convey grouped together: metaphor\n",
      "grouped together clumps: NOT metaphor\n",
      "together clumps white: metaphor\n",
      "clumps white matter: metaphor\n",
      "our goal build: metaphor\n",
      "goal build giant: NOT metaphor\n",
      "build giant atlas: NOT metaphor\n",
      "giant atlas shows: NOT metaphor\n",
      "atlas shows one: metaphor\n",
      "shows one specific: metaphor\n",
      "one specific aspect: metaphor\n",
      "specific aspect language: metaphor\n",
      "aspect language represented: metaphor\n",
      "language represented brain: metaphor\n",
      "represented brain case: metaphor\n",
      "brain case semantics: NOT metaphor\n",
      "case semantics meanings: metaphor\n",
      "semantics meanings words: metaphor\n",
      "meanings words said: NOT metaphor\n",
      "words said jack: NOT metaphor\n",
      "said jack gallant: metaphor\n",
      "jack gallant neuroscientist: NOT metaphor\n",
      "gallant neuroscientist university: NOT metaphor\n",
      "neuroscientist university california: NOT metaphor\n",
      "university california berkeley: metaphor\n",
      "no single brain: NOT metaphor\n",
      "single brain region: metaphor\n",
      "brain region holds: NOT metaphor\n",
      "region holds one: metaphor\n",
      "holds one word: NOT metaphor\n",
      "one word concept: metaphor\n",
      "word concept a: NOT metaphor\n",
      "concept a single: metaphor\n",
      "a single brain: metaphor\n",
      "single brain spot: metaphor\n",
      "brain spot associated: NOT metaphor\n",
      "spot associated number: metaphor\n",
      "associated number related: metaphor\n",
      "number related words: NOT metaphor\n",
      "related words and: metaphor\n",
      "words and single: NOT metaphor\n",
      "and single word: metaphor\n",
      "single word lights: metaphor\n",
      "word lights many: NOT metaphor\n",
      "lights many different: metaphor\n",
      "many different brain: NOT metaphor\n",
      "different brain spots: metaphor\n",
      "brain spots together: NOT metaphor\n",
      "spots together make: metaphor\n",
      "together make networks: metaphor\n",
      "make networks represent: NOT metaphor\n",
      "networks represent meanings: NOT metaphor\n",
      "represent meanings word: metaphor\n",
      "meanings word use: NOT metaphor\n",
      "word use life: NOT metaphor\n",
      "use life love: metaphor\n",
      "life love death: metaphor\n",
      "love death taxes: metaphor\n",
      "death taxes clouds: metaphor\n",
      "taxes clouds florida: metaphor\n",
      "clouds florida bra: metaphor\n",
      "florida bra all: metaphor\n",
      "bra all light: NOT metaphor\n",
      "all light networks: metaphor\n",
      "described tour de: metaphor\n",
      "tour de force: metaphor\n",
      "de force one: metaphor\n",
      "force one researcher: metaphor\n",
      "one researcher involved: metaphor\n",
      "researcher involved study: metaphor\n",
      "involved study atlas: NOT metaphor\n",
      "study atlas demonstrates: metaphor\n",
      "atlas demonstrates modern: metaphor\n",
      "demonstrates modern imaging: metaphor\n",
      "modern imaging transform: NOT metaphor\n",
      "imaging transform knowledge: NOT metaphor\n",
      "transform knowledge brain: metaphor\n",
      "knowledge brain performs: metaphor\n",
      "brain performs important: NOT metaphor\n",
      "performs important tasks: NOT metaphor\n",
      "important tasks with: NOT metaphor\n",
      "tasks with advances: metaphor\n",
      "with advances technology: NOT metaphor\n",
      "advances technology could: metaphor\n",
      "technology could profound: metaphor\n",
      "could profound impact: metaphor\n",
      "profound impact medicine: NOT metaphor\n",
      "impact medicine fields: NOT metaphor\n",
      "it possible approach: NOT metaphor\n",
      "possible approach could: NOT metaphor\n",
      "approach could used: metaphor\n",
      "could used decode: metaphor\n",
      "used decode information: metaphor\n",
      "decode information words: metaphor\n",
      "information words person: NOT metaphor\n",
      "words person hearing: NOT metaphor\n",
      "person hearing reading: NOT metaphor\n",
      "hearing reading possibly: NOT metaphor\n",
      "reading possibly even: metaphor\n",
      "possibly even thinking: NOT metaphor\n",
      "even thinking said: metaphor\n",
      "thinking said alexander: metaphor\n",
      "said alexander huth: metaphor\n",
      "alexander huth first: metaphor\n",
      "huth first author: NOT metaphor\n",
      "first author study: metaphor\n",
      "author study one: metaphor\n",
      "study one potential: metaphor\n",
      "one potential use: metaphor\n",
      "potential use would: NOT metaphor\n",
      "use would language: metaphor\n",
      "would language decoder: NOT metaphor\n",
      "language decoder could: metaphor\n",
      "decoder could allow: metaphor\n",
      "could allow people: metaphor\n",
      "allow people silenced: metaphor\n",
      "people silenced motor: NOT metaphor\n",
      "silenced motor neurone: metaphor\n",
      "motor neurone disease: NOT metaphor\n",
      "neurone disease lockedin: NOT metaphor\n",
      "disease lockedin syndrome: metaphor\n",
      "lockedin syndrome speak: metaphor\n",
      "syndrome speak computer: metaphor\n",
      "to create atlas: metaphor\n",
      "create atlas scientists: metaphor\n",
      "atlas scientists recorded: metaphor\n",
      "scientists recorded peoples: metaphor\n",
      "recorded peoples brain: metaphor\n",
      "peoples brain activity: NOT metaphor\n",
      "brain activity listened: NOT metaphor\n",
      "activity listened stories: metaphor\n",
      "listened stories read: metaphor\n",
      "stories read the: metaphor\n",
      "read the moth: metaphor\n",
      "the moth radio: metaphor\n",
      "moth radio hour: NOT metaphor\n",
      "radio hour us: metaphor\n",
      "hour us radio: NOT metaphor\n",
      "us radio show: metaphor\n",
      "radio show they: metaphor\n",
      "show they matched: metaphor\n",
      "they matched transcripts: metaphor\n",
      "matched transcripts stories: NOT metaphor\n",
      "transcripts stories brain: metaphor\n",
      "stories brain activity: metaphor\n",
      "brain activity data: NOT metaphor\n",
      "activity data show: metaphor\n",
      "data show groups: metaphor\n",
      "show groups related: metaphor\n",
      "groups related words: NOT metaphor\n",
      "related words triggered: metaphor\n",
      "words triggered neural: NOT metaphor\n",
      "triggered neural responses: metaphor\n",
      "neural responses 50000: NOT metaphor\n",
      "responses 50000 80000: metaphor\n",
      "50000 80000 peasized: metaphor\n",
      "80000 peasized spots: metaphor\n",
      "peasized spots cerebral: NOT metaphor\n",
      "spots cerebral cortex: metaphor\n",
      "huth used stories: NOT metaphor\n",
      "used stories the: metaphor\n",
      "stories the moth: metaphor\n",
      "the moth radio: metaphor\n",
      "moth radio hour: NOT metaphor\n",
      "radio hour short: metaphor\n",
      "hour short compelling: NOT metaphor\n",
      "short compelling the: metaphor\n",
      "compelling the enthralling: metaphor\n",
      "the enthralling stories: metaphor\n",
      "enthralling stories confident: metaphor\n",
      "stories confident scientists: metaphor\n",
      "confident scientists could: metaphor\n",
      "scientists could people: metaphor\n",
      "could people scanned: metaphor\n",
      "people scanned focusing: NOT metaphor\n",
      "scanned focusing words: metaphor\n",
      "focusing words drifting: metaphor\n",
      "words drifting off: NOT metaphor\n",
      "drifting off seven: metaphor\n",
      "off seven people: metaphor\n",
      "seven people listened: metaphor\n",
      "people listened two: NOT metaphor\n",
      "listened two hours: metaphor\n",
      "two hours stories: metaphor\n",
      "hours stories each: NOT metaphor\n",
      "stories each per: metaphor\n",
      "each per person: metaphor\n",
      "per person amounted: NOT metaphor\n",
      "person amounted hearing: NOT metaphor\n",
      "amounted hearing roughly: metaphor\n",
      "hearing roughly 25000: NOT metaphor\n",
      "roughly 25000 words: metaphor\n",
      "25000 words 3000: metaphor\n",
      "words 3000 different: NOT metaphor\n",
      "3000 different words: metaphor\n",
      "different words : metaphor\n",
      "words  lay: NOT metaphor\n",
      " lay scanner: metaphor\n",
      "the atlas shows: metaphor\n",
      "atlas shows words: metaphor\n",
      "shows words related: metaphor\n",
      "words related terms: NOT metaphor\n",
      "related terms exercise: metaphor\n",
      "terms exercise regions: metaphor\n",
      "exercise regions brain: metaphor\n",
      "regions brain for: metaphor\n",
      "brain for example: NOT metaphor\n",
      "for example lefthand: metaphor\n",
      "example lefthand side: metaphor\n",
      "lefthand side brain: metaphor\n",
      "side brain ear: metaphor\n",
      "brain ear one: NOT metaphor\n",
      "ear one tiny: metaphor\n",
      "one tiny regions: metaphor\n",
      "tiny regions represents: metaphor\n",
      "regions represents word: metaphor\n",
      "represents word victim: metaphor\n",
      "word victim the: NOT metaphor\n",
      "victim the region: NOT metaphor\n",
      "the region responds: metaphor\n",
      "region responds killed: metaphor\n",
      "responds killed convicted: metaphor\n",
      "killed convicted murdered: metaphor\n",
      "convicted murdered confessed: metaphor\n",
      "murdered confessed on: NOT metaphor\n",
      "confessed on brains: metaphor\n",
      "on brains righthand: metaphor\n",
      "brains righthand side: NOT metaphor\n",
      "righthand side near: metaphor\n",
      "side near top: metaphor\n",
      "near top head: NOT metaphor\n",
      "top head one: metaphor\n",
      "head one brain: NOT metaphor\n",
      "one brain spots: metaphor\n",
      "brain spots activated: NOT metaphor\n",
      "spots activated family: metaphor\n",
      "activated family terms: metaphor\n",
      "family terms wife: metaphor\n",
      "terms wife husband: metaphor\n",
      "wife husband children: NOT metaphor\n",
      "husband children parents: NOT metaphor\n",
      "each word represented: metaphor\n",
      "word represented one: NOT metaphor\n",
      "represented one spot: metaphor\n",
      "one spot words: metaphor\n",
      "spot words tend: metaphor\n",
      "words tend several: NOT metaphor\n",
      "tend several meanings: metaphor\n",
      "several meanings one: metaphor\n",
      "meanings one part: NOT metaphor\n",
      "one part brain: metaphor\n",
      "part brain example: NOT metaphor\n",
      "brain example reliably: NOT metaphor\n",
      "example reliably responds: metaphor\n",
      "reliably responds word: metaphor\n",
      "responds word top: metaphor\n",
      "word top along: NOT metaphor\n",
      "top along words: metaphor\n",
      "along words describe: metaphor\n",
      "words describe clothing: NOT metaphor\n",
      "describe clothing but: metaphor\n",
      "clothing but word: metaphor\n",
      "but word top: NOT metaphor\n",
      "word top activates: NOT metaphor\n",
      "top activates many: metaphor\n",
      "activates many regions: metaphor\n",
      "many regions one: NOT metaphor\n",
      "regions one responds: metaphor\n",
      "one responds numbers: metaphor\n",
      "responds numbers measurements: metaphor\n",
      "numbers measurements another: NOT metaphor\n",
      "measurements another buildings: NOT metaphor\n",
      "another buildings places: metaphor\n",
      "buildings places the: NOT metaphor\n",
      "places the scientists: NOT metaphor\n",
      "the scientists created: metaphor\n",
      "scientists created interactive: metaphor\n",
      "created interactive website: metaphor\n",
      "interactive website public: NOT metaphor\n",
      "website public explore: NOT metaphor\n",
      "public explore brain: NOT metaphor\n",
      "explore brain atlas: metaphor\n",
      "strikingly brain atlases: metaphor\n",
      "brain atlases similar: NOT metaphor\n",
      "atlases similar participants: metaphor\n",
      "similar participants suggesting: metaphor\n",
      "participants suggesting brains: NOT metaphor\n",
      "suggesting brains organised: metaphor\n",
      "brains organised meanings: NOT metaphor\n",
      "organised meanings words: metaphor\n",
      "meanings words way: NOT metaphor\n",
      "words way the: NOT metaphor\n",
      "way the scientists: NOT metaphor\n",
      "the scientists scanned: metaphor\n",
      "scientists scanned five: metaphor\n",
      "scanned five men: metaphor\n",
      "five men two: metaphor\n",
      "men two women: NOT metaphor\n",
      "two women however: metaphor\n",
      "women however all: NOT metaphor\n",
      "however all native: NOT metaphor\n",
      "all native english: metaphor\n",
      "native english speakers: NOT metaphor\n",
      "english speakers two: metaphor\n",
      "speakers two authors: metaphor\n",
      "two authors study: metaphor\n",
      "authors study published: metaphor\n",
      "study published nature: metaphor\n",
      "published nature it: NOT metaphor\n",
      "nature it highly: NOT metaphor\n",
      "it highly possible: NOT metaphor\n",
      "highly possible people: NOT metaphor\n",
      "possible people different: NOT metaphor\n",
      "people different backgrounds: NOT metaphor\n",
      "different backgrounds cultures: metaphor\n",
      "backgrounds cultures different: NOT metaphor\n",
      "cultures different semantic: metaphor\n",
      "different semantic brain: metaphor\n",
      "semantic brain atlases: metaphor\n",
      "armed atlas researchers: metaphor\n",
      "atlas researchers piece: metaphor\n",
      "researchers piece together: metaphor\n",
      "piece together brain: NOT metaphor\n",
      "together brain networks: metaphor\n",
      "brain networks represent: NOT metaphor\n",
      "networks represent wildly: NOT metaphor\n",
      "represent wildly different: metaphor\n",
      "wildly different concepts: NOT metaphor\n",
      "different concepts numbers: metaphor\n",
      "concepts numbers murder: metaphor\n",
      "numbers murder religion: NOT metaphor\n",
      "murder religion the: NOT metaphor\n",
      "religion the idea: metaphor\n",
      "the idea murder: metaphor\n",
      "idea murder represented: NOT metaphor\n",
      "murder represented lot: NOT metaphor\n",
      "represented lot brain: metaphor\n",
      "lot brain gallant: metaphor\n",
      "brain gallant said: NOT metaphor\n",
      "using haul data: metaphor\n",
      "haul data group: NOT metaphor\n",
      "data group begun: metaphor\n",
      "group begun work: NOT metaphor\n",
      "begun work new: NOT metaphor\n",
      "work new atlases: NOT metaphor\n",
      "new atlases show: NOT metaphor\n",
      "atlases show brain: metaphor\n",
      "show brain holds: metaphor\n",
      "brain holds information: NOT metaphor\n",
      "holds information aspects: NOT metaphor\n",
      "information aspects language: NOT metaphor\n",
      "aspects language phonemes: metaphor\n",
      "language phonemes syntax: metaphor\n",
      "phonemes syntax a: NOT metaphor\n",
      "syntax a brain: metaphor\n",
      "a brain atlas: metaphor\n",
      "brain atlas narrative: NOT metaphor\n",
      "atlas narrative structure: metaphor\n",
      "narrative structure far: NOT metaphor\n",
      "structure far proved: metaphor\n",
      "far proved elusive: metaphor\n",
      "proved elusive however: NOT metaphor\n",
      "elusive however every: metaphor\n",
      "however every time: NOT metaphor\n",
      "every time come: metaphor\n",
      "time come set: metaphor\n",
      "come set narrative: metaphor\n",
      "set narrative features: metaphor\n",
      "narrative features get: NOT metaphor\n",
      "features get told: metaphor\n",
      "get told arent: NOT metaphor\n",
      "told arent right: metaphor\n",
      "arent right set: metaphor\n",
      "right set narrative: metaphor\n",
      "set narrative features: metaphor\n",
      "narrative features said: NOT metaphor\n",
      "features said gallant: metaphor\n",
      "uri hasson neuroscientist: metaphor\n",
      "hasson neuroscientist princeton: NOT metaphor\n",
      "neuroscientist princeton university: NOT metaphor\n",
      "princeton university praised: NOT metaphor\n",
      "university praised work: metaphor\n",
      "praised work unlike: NOT metaphor\n",
      "work unlike many: NOT metaphor\n",
      "unlike many studies: metaphor\n",
      "many studies looked: NOT metaphor\n",
      "studies looked brain: metaphor\n",
      "looked brain activity: metaphor\n",
      "brain activity isolated: NOT metaphor\n",
      "activity isolated word: metaphor\n",
      "isolated word sentence: NOT metaphor\n",
      "word sentence spoken: NOT metaphor\n",
      "sentence spoken gallants: metaphor\n",
      "spoken gallants team: metaphor\n",
      "gallants team shed: NOT metaphor\n",
      "team shed light: metaphor\n",
      "shed light brain: metaphor\n",
      "light brain worked: metaphor\n",
      "brain worked realworld: NOT metaphor\n",
      "worked realworld scenario: NOT metaphor\n",
      "realworld scenario said: metaphor\n",
      "scenario said the: metaphor\n",
      "said the next: metaphor\n",
      "the next step: metaphor\n",
      "next step said: NOT metaphor\n",
      "step said create: metaphor\n",
      "said create comprehensive: metaphor\n",
      "create comprehensive precise: metaphor\n",
      "comprehensive precise semantic: metaphor\n",
      "precise semantic brain: NOT metaphor\n",
      "semantic brain atlas: metaphor\n",
      "brain atlas ultimately: NOT metaphor\n",
      "atlas ultimately hasson: metaphor\n",
      "ultimately hasson believes: metaphor\n",
      "hasson believes possible: NOT metaphor\n",
      "believes possible reconstruct: NOT metaphor\n",
      "possible reconstruct words: NOT metaphor\n",
      "reconstruct words person: metaphor\n",
      "words person thinking: NOT metaphor\n",
      "person thinking brain: NOT metaphor\n",
      "thinking brain activity: metaphor\n",
      "brain activity the: NOT metaphor\n",
      "activity the ethical: metaphor\n",
      "the ethical implications: metaphor\n",
      "ethical implications enormous: metaphor\n",
      "implications enormous one: NOT metaphor\n",
      "enormous one benign: metaphor\n",
      "one benign use: metaphor\n",
      "benign use would: NOT metaphor\n",
      "use would see: metaphor\n",
      "would see brain: NOT metaphor\n",
      "see brain activity: metaphor\n",
      "brain activity used: NOT metaphor\n",
      "activity used assess: metaphor\n",
      "used assess whether: metaphor\n",
      "assess whether political: metaphor\n",
      "whether political messages: NOT metaphor\n",
      "political messages effectively: NOT metaphor\n",
      "messages effectively communicated: NOT metaphor\n",
      "effectively communicated public: metaphor\n",
      "communicated public there: metaphor\n",
      "public there many: NOT metaphor\n",
      "there many implications: metaphor\n",
      "many implications barely: NOT metaphor\n",
      "implications barely touching: NOT metaphor\n",
      "barely touching surface: NOT metaphor\n",
      "touching surface said: metaphor\n",
      "lorraine tyler cognitive: metaphor\n",
      "tyler cognitive neuroscientist: metaphor\n",
      "cognitive neuroscientist head: metaphor\n",
      "neuroscientist head centre: NOT metaphor\n",
      "head centre speech: NOT metaphor\n",
      "centre speech language: metaphor\n",
      "speech language brain: metaphor\n",
      "language brain cambridge: metaphor\n",
      "brain cambridge university: NOT metaphor\n",
      "cambridge university said: metaphor\n",
      "university said research: metaphor\n",
      "said research tour: metaphor\n",
      "research tour de: metaphor\n",
      "tour de force: metaphor\n",
      "de force scope: metaphor\n",
      "force scope methods: metaphor\n",
      "scope methods but: metaphor\n",
      "methods but brain: NOT metaphor\n",
      "but brain atlas: NOT metaphor\n",
      "brain atlas current: NOT metaphor\n",
      "atlas current form: metaphor\n",
      "current form capture: metaphor\n",
      "form capture fine: metaphor\n",
      "capture fine differences: metaphor\n",
      "fine differences word: metaphor\n",
      "differences word meanings: metaphor\n",
      "word meanings take: NOT metaphor\n",
      "meanings take word: NOT metaphor\n",
      "take word table: metaphor\n",
      "word table it: NOT metaphor\n",
      "table it member: metaphor\n",
      "it member many: NOT metaphor\n",
      "member many different: NOT metaphor\n",
      "many different groups: NOT metaphor\n",
      "different groups says: metaphor\n",
      "groups says tyler: NOT metaphor\n",
      "says tyler it: metaphor\n",
      "tyler it something: metaphor\n",
      "it something eat: NOT metaphor\n",
      "something eat off: metaphor\n",
      "eat off things: metaphor\n",
      "off things made: metaphor\n",
      "things made wood: metaphor\n",
      "made wood things: NOT metaphor\n",
      "wood things heavy: NOT metaphor\n",
      "things heavy things: metaphor\n",
      "heavy things four: NOT metaphor\n",
      "things four legs: metaphor\n",
      "four legs nonanimate: metaphor\n",
      "legs nonanimate objects: metaphor\n",
      "nonanimate objects on: NOT metaphor\n",
      "objects on this: metaphor\n",
      "on this kind: metaphor\n",
      "this kind detailed: metaphor\n",
      "kind detailed semantic: metaphor\n",
      "detailed semantic information: metaphor\n",
      "semantic information enables: metaphor\n",
      "information enables words: NOT metaphor\n",
      "enables words used: metaphor\n",
      "words used flexibly: NOT metaphor\n",
      "used flexibly lost: metaphor\n",
      "flexibly lost analysis: metaphor\n",
      "lost analysis said: metaphor\n",
      "analysis said while: metaphor\n",
      "said while research: metaphor\n",
      "while research pathbreaking: NOT metaphor\n",
      "research pathbreaking scope: metaphor\n",
      "pathbreaking scope still: NOT metaphor\n",
      "scope still lot: metaphor\n",
      "still lot learn: metaphor\n",
      "lot learn semantics: metaphor\n",
      "learn semantics represented: metaphor\n",
      "semantics represented brain: metaphor\n",
      "metaphor_count:342\n",
      "nonmetaphor_count:200\n",
      "['scientists created atlas', 'created atlas brain', 'atlas brain reveals', 'reveals meanings words', 'arranged across different', 'across different regions', 'different regions organ', 'like colourful quilt', 'colourful quilt laid', 'quilt laid cortex', 'laid cortex atlas', 'cortex atlas displays', 'atlas displays rainbow', 'displays rainbow hues', 'rainbow hues individual', 'concepts convey grouped', 'convey grouped together', 'together clumps white', 'clumps white matter', 'our goal build', 'atlas shows one', 'shows one specific', 'one specific aspect', 'specific aspect language', 'aspect language represented', 'language represented brain', 'represented brain case', 'case semantics meanings', 'semantics meanings words', 'said jack gallant', 'university california berkeley', 'single brain region', 'region holds one', 'one word concept', 'concept a single', 'a single brain', 'single brain spot', 'spot associated number', 'associated number related', 'related words and', 'and single word', 'single word lights', 'lights many different', 'different brain spots', 'spots together make', 'together make networks', 'represent meanings word', 'use life love', 'life love death', 'love death taxes', 'death taxes clouds', 'taxes clouds florida', 'clouds florida bra', 'florida bra all', 'all light networks', 'described tour de', 'tour de force', 'de force one', 'force one researcher', 'one researcher involved', 'researcher involved study', 'study atlas demonstrates', 'atlas demonstrates modern', 'demonstrates modern imaging', 'transform knowledge brain', 'knowledge brain performs', 'tasks with advances', 'advances technology could', 'technology could profound', 'could profound impact', 'approach could used', 'could used decode', 'used decode information', 'decode information words', 'reading possibly even', 'even thinking said', 'thinking said alexander', 'said alexander huth', 'alexander huth first', 'first author study', 'author study one', 'study one potential', 'one potential use', 'use would language', 'language decoder could', 'decoder could allow', 'could allow people', 'allow people silenced', 'silenced motor neurone', 'disease lockedin syndrome', 'lockedin syndrome speak', 'syndrome speak computer', 'to create atlas', 'create atlas scientists', 'atlas scientists recorded', 'scientists recorded peoples', 'recorded peoples brain', 'activity listened stories', 'listened stories read', 'stories read the', 'read the moth', 'the moth radio', 'radio hour us', 'us radio show', 'radio show they', 'show they matched', 'they matched transcripts', 'transcripts stories brain', 'stories brain activity', 'activity data show', 'data show groups', 'show groups related', 'related words triggered', 'triggered neural responses', 'responses 50000 80000', '50000 80000 peasized', '80000 peasized spots', 'spots cerebral cortex', 'used stories the', 'stories the moth', 'the moth radio', 'radio hour short', 'short compelling the', 'compelling the enthralling', 'the enthralling stories', 'enthralling stories confident', 'stories confident scientists', 'confident scientists could', 'scientists could people', 'could people scanned', 'scanned focusing words', 'focusing words drifting', 'drifting off seven', 'off seven people', 'seven people listened', 'listened two hours', 'two hours stories', 'stories each per', 'each per person', 'amounted hearing roughly', 'roughly 25000 words', '25000 words 3000', '3000 different words', 'different words ', ' lay scanner', 'the atlas shows', 'atlas shows words', 'shows words related', 'related terms exercise', 'terms exercise regions', 'exercise regions brain', 'regions brain for', 'for example lefthand', 'example lefthand side', 'lefthand side brain', 'side brain ear', 'ear one tiny', 'one tiny regions', 'tiny regions represents', 'regions represents word', 'represents word victim', 'the region responds', 'region responds killed', 'responds killed convicted', 'killed convicted murdered', 'convicted murdered confessed', 'confessed on brains', 'on brains righthand', 'righthand side near', 'side near top', 'top head one', 'one brain spots', 'spots activated family', 'activated family terms', 'family terms wife', 'terms wife husband', 'each word represented', 'represented one spot', 'one spot words', 'spot words tend', 'tend several meanings', 'several meanings one', 'one part brain', 'example reliably responds', 'reliably responds word', 'responds word top', 'top along words', 'along words describe', 'describe clothing but', 'clothing but word', 'top activates many', 'activates many regions', 'regions one responds', 'one responds numbers', 'responds numbers measurements', 'another buildings places', 'the scientists created', 'scientists created interactive', 'created interactive website', 'explore brain atlas', 'strikingly brain atlases', 'atlases similar participants', 'similar participants suggesting', 'suggesting brains organised', 'organised meanings words', 'the scientists scanned', 'scientists scanned five', 'scanned five men', 'five men two', 'two women however', 'all native english', 'english speakers two', 'speakers two authors', 'two authors study', 'authors study published', 'study published nature', 'different backgrounds cultures', 'cultures different semantic', 'different semantic brain', 'semantic brain atlases', 'armed atlas researchers', 'atlas researchers piece', 'researchers piece together', 'together brain networks', 'represent wildly different', 'different concepts numbers', 'concepts numbers murder', 'religion the idea', 'the idea murder', 'represented lot brain', 'lot brain gallant', 'using haul data', 'data group begun', 'atlases show brain', 'show brain holds', 'aspects language phonemes', 'language phonemes syntax', 'syntax a brain', 'a brain atlas', 'atlas narrative structure', 'structure far proved', 'far proved elusive', 'elusive however every', 'every time come', 'time come set', 'come set narrative', 'set narrative features', 'features get told', 'told arent right', 'arent right set', 'right set narrative', 'set narrative features', 'features said gallant', 'uri hasson neuroscientist', 'university praised work', 'unlike many studies', 'studies looked brain', 'looked brain activity', 'activity isolated word', 'sentence spoken gallants', 'spoken gallants team', 'team shed light', 'shed light brain', 'light brain worked', 'realworld scenario said', 'scenario said the', 'said the next', 'the next step', 'step said create', 'said create comprehensive', 'create comprehensive precise', 'comprehensive precise semantic', 'semantic brain atlas', 'atlas ultimately hasson', 'ultimately hasson believes', 'reconstruct words person', 'thinking brain activity', 'activity the ethical', 'the ethical implications', 'ethical implications enormous', 'enormous one benign', 'one benign use', 'use would see', 'see brain activity', 'activity used assess', 'used assess whether', 'assess whether political', 'effectively communicated public', 'communicated public there', 'there many implications', 'touching surface said', 'lorraine tyler cognitive', 'tyler cognitive neuroscientist', 'cognitive neuroscientist head', 'centre speech language', 'speech language brain', 'language brain cambridge', 'cambridge university said', 'university said research', 'said research tour', 'research tour de', 'tour de force', 'de force scope', 'force scope methods', 'scope methods but', 'atlas current form', 'current form capture', 'form capture fine', 'capture fine differences', 'fine differences word', 'differences word meanings', 'take word table', 'table it member', 'different groups says', 'says tyler it', 'tyler it something', 'something eat off', 'eat off things', 'off things made', 'things made wood', 'things heavy things', 'things four legs', 'four legs nonanimate', 'legs nonanimate objects', 'objects on this', 'on this kind', 'this kind detailed', 'kind detailed semantic', 'detailed semantic information', 'semantic information enables', 'enables words used', 'used flexibly lost', 'flexibly lost analysis', 'lost analysis said', 'analysis said while', 'said while research', 'research pathbreaking scope', 'scope still lot', 'still lot learn', 'lot learn semantics', 'learn semantics represented', 'semantics represented brain']\n"
     ]
    }
   ],
   "source": [
    "with open(\"sciencearticle.txt\", \"r\") as testset:\n",
    "    filtered_words = []\n",
    "    testmeta = []\n",
    "    word_list = ''\n",
    "    para_index = 0\n",
    "#     text = ''.join(testset.readlines())\n",
    "#     sentences = re.split(r' *[\\.\\?!][\\'\"\\)\\]]* *', text)\n",
    "#     sentences = [t.rstrip() for t in sentences]\n",
    "#     print(sentences)\n",
    "    for line in testset:\n",
    "        para_index += 1\n",
    "        word_list = line.split()\n",
    "        filtered_words = [re.sub(r'[^\\w\\s]','',word.lower()) for word in word_list if word not in stopwords.words('english')]\n",
    "        testmeta.append([' '.join(x) for x in trigrams(filtered_words)])\n",
    "        print(para_index)\n",
    "# print(testmeta)\n",
    "\n",
    "metaphor_count = 0\n",
    "nonmetaphor_count = 0\n",
    "metaphor_list = []\n",
    "for x in testmeta:\n",
    "    for y in x:\n",
    "        testsample = y\n",
    "        print (testsample + \": \" + cl.classify(metaphor_features(testsample)))\n",
    "        if cl.classify(metaphor_features(testsample)) == 'metaphor':\n",
    "            metaphor_count += 1\n",
    "            metaphor_list.append(testsample)\n",
    "        elif cl.classify(metaphor_features(testsample)) == 'NOT metaphor':\n",
    "            nonmetaphor_count += 1\n",
    "            \n",
    "print('metaphor_count:' + str(metaphor_count))\n",
    "print('nonmetaphor_count:' + str(nonmetaphor_count))\n",
    "print(metaphor_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
