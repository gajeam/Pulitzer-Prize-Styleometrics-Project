{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at https://github.com/ytsvetko/metaphor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##all the imports...\n",
    "#%pylab inline\n",
    "import re\n",
    "import math\n",
    "import string\n",
    "import nltk\n",
    "import pprint\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "sent_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "from collections import Counter\n",
    "from __future__ import division\n",
    "from nltk.collocations import *\n",
    "import string, random\n",
    "from nltk.corpus import brown\n",
    "from nltk.collocations import *\n",
    "from string import punctuation\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk import bigrams\n",
    "from nltk import collocations\n",
    "from nltk import trigrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start by loading the .csv files and setting the columns.\n",
    "### For these .csv files, downloaded from https://github.com/ytsvetko/metaphor inputs and saved as .csv with the first row added as \"sample\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>angry welt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bald assertion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bare outline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>black humor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blind alley</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           sample\n",
       "0      angry welt\n",
       "1  bald assertion\n",
       "2    bare outline\n",
       "3     black humor\n",
       "4     blind alley"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_anmet = pd.read_csv(\"an_mets.csv\", low_memory=False)\n",
    "df_anmet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_anmet = pd.read_csv(\"an_mets.csv\", low_memory=False)\n",
    "df_anmet['metaphor'] = 1\n",
    "df_anmet['an'] = 1\n",
    "df_anmet['svo'] = 0\n",
    "df_anmet['metanet'] = 0\n",
    "df_anmet.head()\n",
    "len(df_anmet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_annonmet = pd.read_csv(\"an_nonmets.csv\", low_memory=False)\n",
    "df_annonmet['metaphor'] = 0\n",
    "df_annonmet['an'] = 1\n",
    "df_annonmet['svo'] = 0\n",
    "df_annonmet['metanet'] = 0\n",
    "df_annonmet.head()\n",
    "len(df_annonmet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_svomet = pd.read_csv(\"svo_mets.csv\", low_memory=False)\n",
    "df_svomet['metaphor'] = 1\n",
    "df_svomet['an'] = 0\n",
    "df_svomet['svo'] = 1\n",
    "df_svomet['metanet'] = 0\n",
    "df_svomet.head()\n",
    "len(df_svomet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_svononmet = pd.read_csv(\"svo_nonmets.csv\", low_memory=False)\n",
    "df_svononmet['metaphor'] = 0\n",
    "df_svononmet['an'] = 0\n",
    "df_svononmet['svo'] = 1\n",
    "df_svononmet['metanet'] = 0\n",
    "df_svononmet.head()\n",
    "len(df_svononmet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>metaphor</th>\n",
       "      <th>an</th>\n",
       "      <th>svo</th>\n",
       "      <th>metanet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ability to evaluate government is ability to see</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ability to evaluate is ability to see</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ability to know is ability to see</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abusive political leaders are physical bullies</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>accepting is swallowing</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sample  metaphor  an  svo  \\\n",
       "0  ability to evaluate government is ability to see         1   0    0   \n",
       "1             ability to evaluate is ability to see         1   0    0   \n",
       "2                 ability to know is ability to see         1   0    0   \n",
       "3    abusive political leaders are physical bullies         1   0    0   \n",
       "4                           accepting is swallowing         1   0    0   \n",
       "\n",
       "   metanet  \n",
       "0        1  \n",
       "1        1  \n",
       "2        1  \n",
       "3        1  \n",
       "4        1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metanet = pd.read_csv(\"metanet.csv\", low_memory=False)\n",
    "df_metanet['metaphor'] = 1\n",
    "df_metanet['an'] = 0\n",
    "df_metanet['svo'] = 0\n",
    "df_metanet['metanet'] = 1\n",
    "df_metanet.head()\n",
    "# len(df_metanet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>metaphor</th>\n",
       "      <th>an</th>\n",
       "      <th>svo</th>\n",
       "      <th>metanet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>conversation turn subject</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>resumption bring relief</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>economy move direction</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>service meet expectation</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>material live dream</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>unemployment stand *none</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>action talk *none</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>statement sit *none</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>income fall *none</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>silence speak volume</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>car decide *none</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>battery die *none</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>accident wait *none</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>Texan break record</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>temperature break *number</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>insurance cover care</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>state cut spending</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>Hawaii kill  proposal</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>electronics drive innovation</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>teenager wear attitude</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>*pronoun catch flight</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>envy eat *none</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>data point pain</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>advertiser pull ad</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>*pronoun close deal</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>police close investigation</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>bicycle suffer damage</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>Tori throw tantrum</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>fortune smile *pronoun</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>excitement fill street</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>analysis of social problems is diagnosis of af...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>analyzing is dissecting</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>anger is fire</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>anger is heat</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>anger is insanity</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>anger is pressure in a container</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>anger is the heat of fluid in a container</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>argument is physical combat</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>argument is war</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>arithmetic is object construction</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>assessing is measuring</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>assistance is support</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>attaining control is gaining a possession</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>attributes are entities</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>attributes are possessions</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>attributes of government are entities</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>augmenting economic assets is creating objects</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>bad is stinky</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>becoming impoverished is moving downwards</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>being alive is being physically at this location</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>being good is being upright</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>being immoral is being low</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>being impoverished is being at a low location</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>being impoverished is being in a bounded region</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>being in a high social class is being high on ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>being in a low social class is being low on a ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>being in a middle class is being in the middle...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>being in a state is being at a point on a line...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>being poised to know is being positioned to se...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>being wealthy is being at a high location</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sample  metaphor  an  svo  \\\n",
       "200                          conversation turn subject         1   0    1   \n",
       "201                            resumption bring relief         1   0    1   \n",
       "202                             economy move direction         1   0    1   \n",
       "203                           service meet expectation         1   0    1   \n",
       "204                                material live dream         1   0    1   \n",
       "205                           unemployment stand *none         1   0    1   \n",
       "206                                  action talk *none         1   0    1   \n",
       "207                                statement sit *none         1   0    1   \n",
       "208                                  income fall *none         1   0    1   \n",
       "209                               silence speak volume         1   0    1   \n",
       "210                                   car decide *none         1   0    1   \n",
       "211                                  battery die *none         1   0    1   \n",
       "212                                accident wait *none         1   0    1   \n",
       "213                                 Texan break record         1   0    1   \n",
       "214                          temperature break *number         1   0    1   \n",
       "215                               insurance cover care         1   0    1   \n",
       "216                                 state cut spending         1   0    1   \n",
       "217                              Hawaii kill  proposal         1   0    1   \n",
       "218                       electronics drive innovation         1   0    1   \n",
       "219                             teenager wear attitude         1   0    1   \n",
       "220                              *pronoun catch flight         1   0    1   \n",
       "221                                     envy eat *none         1   0    1   \n",
       "222                                    data point pain         1   0    1   \n",
       "223                                 advertiser pull ad         1   0    1   \n",
       "224                                *pronoun close deal         1   0    1   \n",
       "225                         police close investigation         1   0    1   \n",
       "226                              bicycle suffer damage         1   0    1   \n",
       "227                                 Tori throw tantrum         1   0    1   \n",
       "228                             fortune smile *pronoun         1   0    1   \n",
       "229                             excitement fill street         1   0    1   \n",
       "..                                                 ...       ...  ..  ...   \n",
       "470  analysis of social problems is diagnosis of af...         1   0    0   \n",
       "471                            analyzing is dissecting         1   0    0   \n",
       "472                                      anger is fire         1   0    0   \n",
       "473                                      anger is heat         1   0    0   \n",
       "474                                  anger is insanity         1   0    0   \n",
       "475                   anger is pressure in a container         1   0    0   \n",
       "476          anger is the heat of fluid in a container         1   0    0   \n",
       "477                        argument is physical combat         1   0    0   \n",
       "478                                    argument is war         1   0    0   \n",
       "479                  arithmetic is object construction         1   0    0   \n",
       "480                             assessing is measuring         1   0    0   \n",
       "481                              assistance is support         1   0    0   \n",
       "482          attaining control is gaining a possession         1   0    0   \n",
       "483                            attributes are entities         1   0    0   \n",
       "484                         attributes are possessions         1   0    0   \n",
       "485              attributes of government are entities         1   0    0   \n",
       "486     augmenting economic assets is creating objects         1   0    0   \n",
       "487                                      bad is stinky         1   0    0   \n",
       "488          becoming impoverished is moving downwards         1   0    0   \n",
       "489   being alive is being physically at this location         1   0    0   \n",
       "490                        being good is being upright         1   0    0   \n",
       "491                         being immoral is being low         1   0    0   \n",
       "492      being impoverished is being at a low location         1   0    0   \n",
       "493    being impoverished is being in a bounded region         1   0    0   \n",
       "494  being in a high social class is being high on ...         1   0    0   \n",
       "495  being in a low social class is being low on a ...         1   0    0   \n",
       "496  being in a middle class is being in the middle...         1   0    0   \n",
       "497  being in a state is being at a point on a line...         1   0    0   \n",
       "498  being poised to know is being positioned to se...         1   0    0   \n",
       "499          being wealthy is being at a high location         1   0    0   \n",
       "\n",
       "     metanet  \n",
       "200        0  \n",
       "201        0  \n",
       "202        0  \n",
       "203        0  \n",
       "204        0  \n",
       "205        0  \n",
       "206        0  \n",
       "207        0  \n",
       "208        0  \n",
       "209        0  \n",
       "210        0  \n",
       "211        0  \n",
       "212        0  \n",
       "213        0  \n",
       "214        0  \n",
       "215        0  \n",
       "216        0  \n",
       "217        0  \n",
       "218        0  \n",
       "219        0  \n",
       "220        0  \n",
       "221        0  \n",
       "222        0  \n",
       "223        0  \n",
       "224        0  \n",
       "225        0  \n",
       "226        0  \n",
       "227        0  \n",
       "228        0  \n",
       "229        0  \n",
       "..       ...  \n",
       "470        1  \n",
       "471        1  \n",
       "472        1  \n",
       "473        1  \n",
       "474        1  \n",
       "475        1  \n",
       "476        1  \n",
       "477        1  \n",
       "478        1  \n",
       "479        1  \n",
       "480        1  \n",
       "481        1  \n",
       "482        1  \n",
       "483        1  \n",
       "484        1  \n",
       "485        1  \n",
       "486        1  \n",
       "487        1  \n",
       "488        1  \n",
       "489        1  \n",
       "490        1  \n",
       "491        1  \n",
       "492        1  \n",
       "493        1  \n",
       "494        1  \n",
       "495        1  \n",
       "496        1  \n",
       "497        1  \n",
       "498        1  \n",
       "499        1  \n",
       "\n",
       "[300 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#combine into one df\n",
    "frames = [df_anmet, df_annonmet, df_svomet, df_svononmet, df_metanet]\n",
    "df_combo = pd.concat(frames)\n",
    "df_combo.reset_index(drop=True, inplace=True)\n",
    "df_combo.shape\n",
    "# df_combo.replace(to_replace='none', value=\"\")\n",
    "# #         tokenizer = RegexpTokenizer(r'\\w+')\n",
    "# #         word_list = tokenizer.tokenize(line)\n",
    "# #         filtered_words = [word for word in word_list if word not in stopwords.words('english')]\n",
    "# df_combo.replace(to_replace='is', value=\"\")\n",
    "stop = ['*none', '.', 'is']\n",
    "df_combo['sample'].apply(lambda x: [item for item in x if item not in stop])\n",
    "df_combo[200:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Break down into training, dev, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1606"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_index = np.random.permutation(df_combo.index)\n",
    "df_combo.ix[random_index, ['sample', 'metaphor', 'an', 'svo', 'metanet']]\n",
    "df_shuffled = df_combo.ix[random_index, ['sample', 'metaphor', 'an', 'svo', 'metanet']]\n",
    "df_shuffled.reset_index(drop=True, inplace=True)\n",
    "len(df_shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 1606\n",
      "Columns: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows, columns = df_shuffled.shape\n",
    "print(\"Rows:\", rows)\n",
    "print(\"Columns:\", columns)\n",
    "#train_size = round(rows*.6)\n",
    "train_size = round(rows*.9)\n",
    "#dev_size   = round(rows*.2)\n",
    "dev_size   = round(rows*.1)\n",
    "df_train = df_shuffled.loc[:train_size]\n",
    "df_train.shape\n",
    "df_dev = df_shuffled.loc[train_size:dev_size+train_size].reset_index(drop=True)\n",
    "df_dev.shape\n",
    "df_test = df_shuffled.loc[dev_size+train_size:].reset_index(drop=True)\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer\n",
    "### Use count vecotrizer with df = 3 for unigram and bigrams to predict dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = list(set(brown.words()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# vec = CountVectorizer(ngram_range=(1, 3), token_pattern=r'\\b\\w+\\b', analyzer=u'word', min_df=1, vocabulary=vocab)\n",
    "# df_train = df_train.fillna(\"\")\n",
    "# df_dev = df_dev.fillna(\"\")\n",
    "# df_test = df_test.fillna(\"\")\n",
    "\n",
    "\n",
    "vec = TfidfVectorizer(ngram_range=(2, 3), token_pattern=r'\\b\\w+\\b', analyzer=u'char', min_df=1)\n",
    "df_train = df_train.fillna(\"\")\n",
    "df_dev = df_dev.fillna(\"\")\n",
    "df_test = df_test.fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arr_train_feature_sparse = vec.fit_transform(df_train['sample'])\n",
    "arr_train_feature_sparse\n",
    "arr_train_feature = arr_train_feature_sparse.toarray()\n",
    "feature_labels = vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arr_dev_feature_sparse = vec.transform(df_dev[\"sample\"])\n",
    "arr_dev_feature = arr_dev_feature_sparse.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90683229813664601"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor']) #defining features (from reviews) and passing in Category label\n",
    "logreg_predictions = logreg_model.predict(arr_dev_feature)\n",
    "accuracy_score(df_dev['metaphor'], logreg_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gabe/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:5: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>s</th>\n",
       "      <td>56.144560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>54.688433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>51.950445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>50.792850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ti</th>\n",
       "      <td>50.457058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on</th>\n",
       "      <td>50.097886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>49.599649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>44.981488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>44.686130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ng</th>\n",
       "      <td>41.995896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        counts\n",
       "s    56.144560\n",
       "in   54.688433\n",
       " i   51.950445\n",
       "is   50.792850\n",
       "ti   50.457058\n",
       "on   50.097886\n",
       " a   49.599649\n",
       "is   44.981488\n",
       " is  44.686130\n",
       "ng   41.995896"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_sum = arr_train_feature.sum(axis=0)   #sum the counts of each feature\n",
    "\n",
    "df_feature_sum = pd.DataFrame({'counts': feature_sum})\n",
    "df_feature_sum.index = vec.get_feature_names()\n",
    "df_feature_sum.sort('counts', ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## make a file of sample sentences to practice on\n",
    "## loop through sentence and .split()\n",
    "## use nltk bigrams\n",
    "##\n",
    "## put into pandas as df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jamal', 'pig', 'dinner']\n"
     ]
    }
   ],
   "source": [
    "word_list = 'Jamal was a pig at dinner'.split()\n",
    "filtered_words = [word for word in word_list if word not in stopwords.words('english')]\n",
    "filtered_words\n",
    "testmeta = list(trigrams(filtered_words))\n",
    "testmeta\n",
    "testmeta1 = [' '.join(x) for x in testmeta]\n",
    "testmeta1\n",
    "df_test = pd.DataFrame(testmeta1)\n",
    "df_test\n",
    "df_test.columns = ['sample']\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_test_feature_sparse = vec.transform(df_test['sample'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "logreg_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jamal pig dinner</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0\n",
       "0  Jamal pig dinner"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.DataFrame(testmeta1)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"test_meta_sent.txt\", \"r\") as testset:\n",
    "    filtered_words = []\n",
    "    testmeta = []\n",
    "    testmeta1 = []\n",
    "    word_list = ''\n",
    "    for line in testset:\n",
    "#         word_list = line.split()\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        word_list = tokenizer.tokenize(line)\n",
    "        filtered_words = [word for word in word_list if word not in stopwords.words('english')]\n",
    "#         testmeta1.append(' '.join(filtered_words))\n",
    "        testmeta1.append(' '.join(x) for x in bigrams(filtered_words))\n",
    "#         testmeta1.append(' '.join(x) for x in filtered_words)\n",
    "# print(testmeta1)\n",
    "\n",
    "df_test =pd.DataFrame(testmeta1)\n",
    "df_test\n",
    "df_test = df_test.fillna(\"\")\n",
    "df_test\n",
    "# df_test.columns = ['sample']\n",
    "# df_test = pd.DataFrame(testmeta1)\n",
    "# df_test\n",
    "df_test.columns = ['sample0', 'sample1', 'sample2', 'sample3', 'sample4', 'sample5', 'sample6']\n",
    "# df_test = df_test.fillna(\"\")\n",
    "# df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-29cc55d9702a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogreg_predictions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# df_test['metaphor0'] = logreg_predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sample0'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'metaphor0'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gabe/anaconda/lib/python3.5/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__nonzero__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    890\u001b[0m         raise ValueError(\"The truth value of a {0} is ambiguous. \"\n\u001b[1;32m    891\u001b[0m                          \u001b[0;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m                          .format(self.__class__.__name__))\n\u001b[0m\u001b[1;32m    893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m     \u001b[0m__bool__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "arr_test_feature_sparse = vec.transform(df_test['sample0'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "# df_test['metaphor0'] = logreg_predictions\n",
    "if df_test['sample0'] == '':\n",
    "    df_test['metaphor0'] = 0\n",
    "else:\n",
    "    df_test['metaphor0'] = logreg_predictions\n",
    "\n",
    "arr_test_feature_sparse = vec.transform(df_test['sample1'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "# df_test['metaphor1'] = logreg_predictions\n",
    "if df_test['sample1'] == '':\n",
    "    df_test['metaphor1'] = 0\n",
    "else:\n",
    "    df_test['metaphor1'] = logreg_predictions\n",
    "\n",
    "arr_test_feature_sparse = vec.transform(df_test['sample2'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "# df_test['metaphor2'] = logreg_predictions\n",
    "if df_test['sample2'] == '':\n",
    "    df_test['metaphor2'] = 0\n",
    "else:\n",
    "    df_test['metaphor2'] = logreg_predictions\n",
    "\n",
    "arr_test_feature_sparse = vec.transform(df_test['sample3'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "# df_test['metaphor3'] = logreg_predictions\n",
    "if df_test['sample3'] == '':\n",
    "    df_test['metaphor3'] = 0\n",
    "else:\n",
    "    df_test['metaphor3'] = logreg_predictions\n",
    "\n",
    "arr_test_feature_sparse = vec.transform(df_test['sample4'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "# df_test['metaphor4'] = logreg_predictions\n",
    "if df_test['sample4'] == '':\n",
    "    df_test['metaphor4'] = 0\n",
    "else:\n",
    "    df_test['metaphor4'] = logreg_predictions\n",
    "\n",
    "arr_test_feature_sparse = vec.transform(df_test['sample5'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "# df_test['metaphor5'] = logreg_predictions\n",
    "if df_test['sample5'] == '':\n",
    "    df_test['metaphor5'] = 0\n",
    "else:\n",
    "    df_test['metaphor5'] = logreg_predictions\n",
    "\n",
    "arr_test_feature_sparse = vec.transform(df_test['sample6'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "# df_test['metaphor6'] = logreg_predictions\n",
    "if df_test['sample6'] == '':\n",
    "    df_test['metaphor6'] = 0\n",
    "else:\n",
    "    df_test['metaphor6'] = logreg_predictions\n",
    "\n",
    "df_test['sum_metaphor'] = df_test['metaphor0'] + df_test['metaphor1'] + df_test['metaphor2'] + df_test['metaphor3'] +df_test['metaphor4'] +df_test['metaphor5'] + df_test['metaphor6'] \n",
    "\n",
    "# def count_metaphors():\n",
    "#     if sum(df_test['sum_metaphor']) >= 1:\n",
    "#         df_test['sum_metaphor'] = 1\n",
    "#     else:\n",
    "#         df_test['sum_metaphor'] = 0\n",
    "# count_metaphors()\n",
    "\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(df_test['sample3'] == '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_test['sample3'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"an_nonmets.txt\", \"r\") as testset:\n",
    "    filtered_words = []\n",
    "    testmeta = []\n",
    "    testmeta1 = []\n",
    "    word_list = ''\n",
    "    for line in testset:\n",
    "        word_list = line.split()\n",
    "        filtered_words = [word for word in word_list if word not in stopwords.words('english')]\n",
    "        testmeta1.append(' '.join(x) for x in bigrams(filtered_words))\n",
    "df_test = pd.DataFrame(testmeta1)\n",
    "df_test\n",
    "df_test.columns = ['sample0']\n",
    "df_test = df_test.fillna(\"\")\n",
    "df_test\n",
    "\n",
    "arr_test_feature_sparse = vec.transform(df_test['sample0'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "df_test['metaphor0'] = logreg_predictions\n",
    "\n",
    "sum(df_test['metaphor0'])\n",
    "#len(df_test['metaphor0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"sciencearticle_line.txt\", \"r\") as testset:\n",
    "    filtered_words = []\n",
    "    testmeta = []\n",
    "    testmeta1 = []\n",
    "    word_list = ''\n",
    "    for line in testset:\n",
    "        word_list = line.split()\n",
    "        filtered_words = [word for word in word_list if word not in stopwords.words('english')]\n",
    "        testmeta1.append(' '.join(x) for x in trigrams(filtered_words))\n",
    "df_test = pd.DataFrame(testmeta1)\n",
    "# df_test = df_test.ix[:, 0:6]\n",
    "# df_test.columns = ['sample0', 'sample1', 'sample2', 'sample3', 'sample4', 'sample5', 'sample6']\n",
    "# df_test = df_test.fillna(\"\")\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arr_test_feature_sparse = vec.transform(df_test['sample0'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "df_test['metaphor0'] = logreg_predictions\n",
    "\n",
    "arr_test_feature_sparse = vec.transform(df_test['sample1'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "df_test['metaphor1'] = logreg_predictions\n",
    "\n",
    "arr_test_feature_sparse = vec.transform(df_test['sample2'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "df_test['metaphor2'] = logreg_predictions\n",
    "\n",
    "arr_test_feature_sparse = vec.transform(df_test['sample3'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "df_test['metaphor3'] = logreg_predictions\n",
    "\n",
    "arr_test_feature_sparse = vec.transform(df_test['sample4'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "df_test['metaphor4'] = logreg_predictions\n",
    "\n",
    "arr_test_feature_sparse = vec.transform(df_test['sample5'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "df_test['metaphor5'] = logreg_predictions\n",
    "\n",
    "arr_test_feature_sparse = vec.transform(df_test['sample6'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "df_test['metaphor6'] = logreg_predictions\n",
    "\n",
    "df_test['sum_metaphor'] = df_test['metaphor1'] + df_test['metaphor2'] + df_test['metaphor3'] +df_test['metaphor4'] +df_test['metaphor5'] \n",
    "\n",
    "if sum(df_test['sum_metaphor']) >= 1:\n",
    "    df_test['sum_metaphor'] = 1\n",
    "sum(df_test['sum_metaphor'])\n",
    "#len(df_test['sum_metaphor'])\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"sciencearticle_line.txt\", \"r\") as testset:\n",
    "    filtered_words = []\n",
    "    testmeta = []\n",
    "    testmeta1 = []\n",
    "    word_list = ''\n",
    "    for line in testset:\n",
    "        word_list = line.split()\n",
    "        filtered_words = ' '.join([word.lower() for word in word_list if word not in stopwords.words('english')])\n",
    "        testmeta1.append(filtered_words)\n",
    "df_test = pd.DataFrame(testmeta1)\n",
    "df_test\n",
    "# df_test = df_test.ix[:, 0:6]\n",
    "df_test.columns = ['sample0']\n",
    "df_test = df_test.fillna(\"\")\n",
    "# df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arr_test_feature_sparse = vec.transform(df_test['sample0'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "df_test['metaphor0'] = logreg_predictions\n",
    "\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_test.ix[:, 0:5].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(df_test.columns)\n",
    "#df_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"sciencearticle_line.txt\", \"r\") as testset:\n",
    "    filtered_words = []\n",
    "    testmeta = []\n",
    "    testmeta1 = []\n",
    "    word_list = ''\n",
    "    for line in testset:\n",
    "        word_list = line.split()\n",
    "        filtered_words = ' '.join([word.lower() for word in word_list if word not in stopwords.words('english')])\n",
    "        testmeta1.append(filtered_words)\n",
    "df_test = pd.DataFrame(testmeta1)\n",
    "df_test\n",
    "# df_test = df_test.ix[:, 0:6]\n",
    "df_test.columns = ['sample0']\n",
    "df_test = df_test.fillna(\"\")\n",
    "# df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk, re\n",
    "from nltk.corpus import brown\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_data_sets(sentences):\n",
    "    size = int(len(sentences) * 0.9)\n",
    "    train_sents = sentences[:size]\n",
    "    test_sents = sentences[size:]\n",
    "    return train_sents, test_sents\n",
    "\n",
    "def build_backoff_tagger (train_sents):\n",
    "    t0 = nltk.DefaultTagger('NN')\n",
    "    t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
    "    t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
    "    return t2\n",
    "\n",
    "\n",
    "def train_tagger(already_tagged_sents):\n",
    "    train_sents, test_sents = create_data_sets(already_tagged_sents)\n",
    "    ngram_tagger = build_backoff_tagger(train_sents)\n",
    "    print (\"%0.3f pos accuracy on test set\" % ngram_tagger.evaluate(test_sents))\n",
    "    return ngram_tagger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_tagger_on_brown():\n",
    "    brown_tagged_sents = brown.tagged_sents(categories=['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies',\n",
    "    'humor', 'learned', 'lore', 'mystery', 'religion', 'reviews', 'romance','science_fiction'])\n",
    "    return train_tagger(brown_tagged_sents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_text(corpus):\n",
    "    sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sents = sent_tokenizer.tokenize(corpus) # Split text into sentences    \n",
    "    return [nltk.word_tokenize(word) for word in raw_sents]\n",
    "\n",
    "def create_corpus(f):\n",
    "    with open(f, 'r') as text_file:\n",
    "        new_corpus = text_file.read()\n",
    "    return new_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "brown_tagger = train_tagger_on_brown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"sciencearticle_line.txt\", \"r\") as testset:\n",
    "    filtered_words = []\n",
    "    testmeta = []\n",
    "    testmeta1 = []\n",
    "    word_list = ''\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    for line in testset:\n",
    "        word_list = line.split()\n",
    "#         filtered_words = ' '.join([word.lower() for word in word_list if word not in stopwords.words('english')])\n",
    "        filtered_words = ' '.join([word.lower() for word in word_list])\n",
    "        testmeta1.append(tokenizer.tokenize(filtered_words))\n",
    "testmeta1[0]\n",
    "\n",
    "def get_pos(sents, tagger):        \n",
    "    return [tagger.tag(sent) for sent in sents]\n",
    "\n",
    "pos = get_pos(testmeta1, brown_tagger)\n",
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_tagger_on_brown_augmented_with_additional_sents():\n",
    "\n",
    "    additional_sents = [[('colorful', 'JJ'), ('quilt', 'NN')],\n",
    "                        [('regions', 'NN'), ('represent', 'VB'), ('world', 'NN')],\n",
    "                        [('public', 'NN'), ('explore', 'VB'), ('brain', 'NN')]]\n",
    "\n",
    "\n",
    "    brown_tagged_sents = brown.tagged_sents(categories=['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies',\n",
    "    'humor', 'learned', 'lore', 'mystery', 'religion', 'reviews', 'romance', 'science_fiction'])\n",
    "    \n",
    "    #append hand-tagged cooking sentences to the front of the training data\n",
    "    all_tagged_sents = additional_sents + brown_tagged_sents\n",
    "    return train_tagger(all_tagged_sents)\n",
    "\n",
    "brown_and_additional_tagger = train_tagger_on_brown_augmented_with_cooking_sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "better_sentences = get_pos(testmeta1, brown_and_additional_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"test_meta_sent.txt\", \"r\") as testset:\n",
    "    filtered_words = []\n",
    "    testmeta = []\n",
    "    testmeta1 = []\n",
    "    word_list = ''\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    for line in testset:\n",
    "        word_list = line.split()\n",
    "#         filtered_words = ' '.join([word.lower() for word in word_list if word not in stopwords.words('english')])\n",
    "        filtered_words = ' '.join([word.lower() for word in word_list])\n",
    "        testmeta1.append(tokenizer.tokenize(filtered_words))\n",
    "testmeta1[0]\n",
    "\n",
    "def get_pos(sents, tagger):        \n",
    "    return [tagger.tag(sent) for sent in sents]\n",
    "\n",
    "pos = get_pos(testmeta1, brown_tagger)\n",
    "pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Switch to Classification Features with NLTK Classification Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import nltk\n",
    "from nltk.corpus import names\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('an_mets.txt', 'r')\n",
    "x = f.readlines()\n",
    "an_mets = [t.rstrip() for t in x[1::]]\n",
    "f = open('an_nonmets.txt', 'r')\n",
    "x = f.readlines()\n",
    "an_nonmets = [t.rstrip() for t in x[1::]]\n",
    "f = open('svo_mets.txt', 'r')\n",
    "x = f.readlines()\n",
    "svo_mets = [t.rstrip() for t in x[1::]]\n",
    "f = open('svo_nonmets.txt', 'r')\n",
    "x = f.readlines()\n",
    "svo_nonmets = [t.rstrip() for t in x[1::]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def double_letter(word):\n",
    "#     letter_list = []\n",
    "#     for letter in word:\n",
    "#         if word.count(letter) > 1:\n",
    "#             letter_list.append(True)\n",
    "#         else:\n",
    "#             letter_list.append(False)\n",
    "#     if True in letter_list:\n",
    "#         return True\n",
    "#     else:\n",
    "#         return False\n",
    "    \n",
    "def metaphor_features(word):\n",
    "    features = {}\n",
    "    word = word.lower()\n",
    "    features['POS'] = word[0]\n",
    "#     features['last'] = word[-1]\n",
    "#     features['last 2'] = word[-2]\n",
    "#     features['first 3'] = word[:3]\n",
    "#     features['first'] = word[:1]\n",
    "#     features['length'] = len(word)\n",
    "#     features['starts with K'] = word.startswith('k')\n",
    "#     features['ends with i'] = word.endswith('i')\n",
    "#     features['ends with a'] = word.endswith('a')\n",
    "#     features['double letter'] = double_letter(word)\n",
    "    return features\n",
    "print(str(metaphor_features('Michelle')))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_an_data():\n",
    "    an_mets_tag = [(re.sub(r'[^\\w\\s]','',line.lower()), 'metaphor') for line in an_mets]\n",
    "    an_nonmets_tag = [(re.sub(r'[^\\w\\s]','',line.lower()), 'NOT metaphor') for line in an_nonmets]\n",
    "    svo_mets_tag = [(re.sub(r'[^\\w\\s]','',line.lower()), 'metaphor') for line in svo_mets]\n",
    "    svo_nonmets_tag = [(re.sub(r'[^\\w\\s]','',line.lower()), 'NOT metaphor') for line in svo_nonmets]\n",
    "    all_an = an_mets_tag + an_nonmets_tag + svo_mets_tag + svo_nonmets_tag\n",
    "    \n",
    "    # Randomize the order of male and female names, and de-alphabatize\n",
    "    random.shuffle(all_an)\n",
    "    return all_an\n",
    "\n",
    "all_an_data = create_an_data()\n",
    "# print(all_an_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function allows experimentation with different feature definitions\n",
    "# items is a list of (key, value) pairs from which features are extracted and training sets are made\n",
    "# Feature sets returned are dictionaries of features\n",
    "\n",
    "# This function also optionally returns the names of the training, development, \n",
    "# and test data for the purposes of error checking\n",
    "\n",
    "def create_training_sets (feature_function, items, return_items=False):\n",
    "    # Create the features sets.  Call the function that was passed in.\n",
    "    # For names data, key is the name, and value is the gender\n",
    "    featuresets = [(feature_function(key), value) for (key, value) in items]\n",
    "    \n",
    "    # Divided training and testing in thirds.  Could divide in other proportions instead.\n",
    "    third = int(float(len(featuresets)) / 3.0)\n",
    "    \n",
    "    train_set, dev_set, test_set = featuresets[0:third], featuresets[third:third*2], featuresets[third*2:]\n",
    "    train_items, dev_items, test_items = items[0:third], items[third:third*2], items[third*2:]\n",
    "    if return_items == True:\n",
    "        return train_set, dev_set, test_set, train_items, dev_items, test_items\n",
    "    else:\n",
    "        return train_set, dev_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# note that this passes in a function name as an argument (gender_features)\n",
    "\n",
    "train_set, dev_set, test_set = create_training_sets(metaphor_features, all_an_data)\n",
    "cl = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (\"'white blanket': \" + cl.classify(metaphor_features('white blanket')))\n",
    "print (\"'snow white': \" + cl.classify(metaphor_features('snow white')))\n",
    "print (\"'white as snow': \" + cl.classify(metaphor_features('white as snow')))\n",
    "print (\"'happy customer': \" + cl.classify(metaphor_features('happy customer')))\n",
    "print (\"'boiling rage': \" + cl.classify(metaphor_features('boiling rage')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testsample = 'one specific aspect'\n",
    "print (testsample + \": \" + cl.classify(metaphor_features(testsample)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (\"%.3f\" % nltk.classify.accuracy(cl, dev_set))\n",
    "cl.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "with open(\"sciencearticle_line.txt\", \"r\") as testset:\n",
    "    filtered_words = []\n",
    "    testmeta = []\n",
    "    word_list = ''\n",
    "    for line in testset:\n",
    "        word_list = line.split()\n",
    "        filtered_words = [re.sub(r'[^\\w\\s]','',word.lower()) for word in word_list if word.lower() not in stopwords.words('english')]\n",
    "        testmeta.append([' '.join(x) for x in trigrams(filtered_words)])\n",
    "print(testmeta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"test_meta_sent.txt\", \"r\") as testset:\n",
    "    filtered_words = []\n",
    "    testmeta = []\n",
    "    word_list = ''\n",
    "    for line in testset:\n",
    "        word_list = line.split()\n",
    "        filtered_words = [re.sub(r'[^\\w\\s]','',word.lower()) for word in word_list if word.lower() not in stopwords.words('english')]\n",
    "        testmeta.append([' '.join(x) for x in trigrams(filtered_words)])\n",
    "print(testmeta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metaphor_count = 0\n",
    "nonmetaphor_count = 0\n",
    "for x in testmeta:\n",
    "    for y in x:\n",
    "        testsample = y\n",
    "        print (testsample + \": \" + cl.classify(metaphor_features(testsample)))\n",
    "        if cl.classify(metaphor_features(testsample)) == 'metaphor':\n",
    "            metaphor_count += 1\n",
    "        elif cl.classify(metaphor_features(testsample)) == 'NOT metaphor':\n",
    "            nonmetaphor_count += 1\n",
    "            \n",
    "print('metaphor_count:' + str(metaphor_count))\n",
    "print('nonmetaphor_count:' + str(nonmetaphor_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_article_for_metaphor(article):\n",
    "    with open(\"sciencearticle.txt\", \"r\") as testset:\n",
    "        filtered_words = []\n",
    "        testmeta = []\n",
    "        word_list = ''\n",
    "        para_index = 0\n",
    "    #     text = ''.join(testset.readlines())\n",
    "    #     sentences = re.split(r' *[\\.\\?!][\\'\"\\)\\]]* *', text)\n",
    "    #     sentences = [t.rstrip() for t in sentences]\n",
    "    #     print(sentences)\n",
    "        for line in testset:\n",
    "            para_index += 1\n",
    "            word_list = line.split()\n",
    "            filtered_words = [re.sub(r'[^\\w\\s]','',word.lower()) for word in word_list if word not in stopwords.words('english')]\n",
    "            testmeta.append([' '.join(x) for x in trigrams(filtered_words)])\n",
    "            print(para_index)\n",
    "    # print(testmeta)\n",
    "\n",
    "    metaphor_count = 0\n",
    "    nonmetaphor_count = 0\n",
    "    metaphor_list = []\n",
    "    for x in testmeta:\n",
    "        for y in x:\n",
    "            testsample = y\n",
    "            print (testsample + \": \" + cl.classify(metaphor_features(testsample)))\n",
    "            if cl.classify(metaphor_features(testsample)) == 'metaphor':\n",
    "                metaphor_count += 1\n",
    "                metaphor_list.append(testsample)\n",
    "            elif cl.classify(metaphor_features(testsample)) == 'NOT metaphor':\n",
    "                nonmetaphor_count += 1\n",
    "\n",
    "    print('metaphor_count:' + str(metaphor_count))\n",
    "    print('nonmetaphor_count:' + str(nonmetaphor_count))\n",
    "    print(metaphor_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"sciencearticle.txt\", \"r\") as testset:\n",
    "    word_list = ''\n",
    "    highlight_bucket = []\n",
    "    for line in testset:\n",
    "        print(line)\n",
    "        for trigram in metaphor_list:\n",
    "            trigram_list = trigram.split()\n",
    "            index = line.find(trigram_list[0])\n",
    "            index_len = len(trigram)\n",
    "            highlight_bucket.append(line[index:index+index_len])\n",
    "print(highlight_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sample = ['the', 'methods', 'i', 'am', 'michelle']\n",
    "# word_list\n",
    "word_str = ' '.join(word_list)\n",
    "# print(word_str)\n",
    "for word in sample:\n",
    "    print(word_str.find(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_regex_strings():\n",
    "    stopwords_regex = \"(\"\n",
    "    # print(stopwords.words('english'))\n",
    "    for word in stopwords.words('english'):\n",
    "        stopwords_regex = stopwords_regex + word + \"|\"\n",
    "    stopwords_regex = stopwords_regex[:-1] + \"|an)\"\n",
    "#     print(stopwords_regex)\n",
    "    punctuation_regex = r\"[â€œâ€!\\\"#$%&'\\(\\)*+,-./:;<=>?@^_`{}~\\s]*\"\n",
    "    return stopwords_regex, punctuation_regex\n",
    "\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with open(\"sciencearticle.txt\", \"r\") as testset:\n",
    "def Rule1_metaphor(article):\n",
    "    article = article.lower()\n",
    "    word_list = ''\n",
    "    highlight_bucket = []\n",
    "    tag_list = []\n",
    "    stopwords_regex, punctuation_regex = create_regex_strings()\n",
    "    \n",
    "    for trigram in metaphor_list:\n",
    "#         print(trigram)\n",
    "        trigram_list = trigram.split(\" \")\n",
    "        regex_string = \"(\" + trigram_list[0] + punctuation_regex + \"(\" + stopwords_regex + punctuation_regex + \")*\" + trigram_list[1] + punctuation_regex + \"(\" + stopwords_regex + punctuation_regex + \")*\" + trigram_list[2] + \")\" \n",
    "#         print(regex_string)\n",
    "#         print(article)\n",
    "        match = re.search(regex_string, article)\n",
    "        if match is not None:\n",
    "            complete_trigram = match.group(0)\n",
    "#             print(trigram, complete_trigram)\n",
    "            index = article.find(complete_trigram)\n",
    "#             print(index, complete_trigram)\n",
    "            index_len = len(trigram)\n",
    "            highlight_bucket.append(article[index:index+index_len])\n",
    "            tag_list.append((index, index_len))\n",
    "#             article = article[index+index_len:]\n",
    "        else:\n",
    "             print(\"Not found: \" + trigram)\n",
    "    \n",
    "#     for line in testset:\n",
    "# #         print(line)\n",
    "#         for trigram in metaphor_list:\n",
    "#             trigram_list = trigram.split()\n",
    "#             index = line.find(trigram_list[0])\n",
    "#             index_len = len(trigram)\n",
    "#             highlight_bucket.append(line[index:index+index_len])\n",
    "#             line = line[index+index_len:]\n",
    "#         print(highlight_bucket)\n",
    "#         len(highlight_bucket)\n",
    "        \n",
    "    return tag_list\n",
    "with open(\"sciencearticle.txt\", \"r\") as testset:   \n",
    "#     print(metaphor_list)\n",
    "    print(Rule1_metaphor(testset.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"sciencearticle.txt\", \"r\") as testset:\n",
    "    filtered_words = []\n",
    "    testmeta = []\n",
    "    word_list = ''\n",
    "    para_index = 0\n",
    "#     text = ''.join(testset.readlines())\n",
    "#     sentences = re.split(r' *[\\.\\?!][\\'\"\\)\\]]* *', text)\n",
    "#     sentences = [t.rstrip() for t in sentences]\n",
    "#     print(sentences)\n",
    "    for line in testset:\n",
    "        para_index += 1\n",
    "        word_list = line.split()\n",
    "        filtered_words = [re.sub(r'[^\\w\\s]','',word.lower()) for word in word_list if word not in stopwords.words('english')]\n",
    "        testmeta.append([' '.join(x) for x in trigrams(filtered_words)])\n",
    "        print(para_index)\n",
    "# print(testmeta)\n",
    "\n",
    "metaphor_count = 0\n",
    "nonmetaphor_count = 0\n",
    "metaphor_list = []\n",
    "for x in testmeta:\n",
    "    for y in x:\n",
    "        testsample = y\n",
    "        print (testsample + \": \" + cl.classify(metaphor_features(testsample)))\n",
    "        if cl.classify(metaphor_features(testsample)) == 'metaphor':\n",
    "            metaphor_count += 1\n",
    "            metaphor_list.append(testsample)\n",
    "        elif cl.classify(metaphor_features(testsample)) == 'NOT metaphor':\n",
    "            nonmetaphor_count += 1\n",
    "            \n",
    "print('metaphor_count:' + str(metaphor_count))\n",
    "print('nonmetaphor_count:' + str(nonmetaphor_count))\n",
    "print(metaphor_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
