{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at https://github.com/ytsvetko/metaphor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##all the imports...\n",
    "#%pylab inline\n",
    "import re\n",
    "import math\n",
    "import string\n",
    "import nltk\n",
    "import pprint\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "sent_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "from collections import Counter\n",
    "from __future__ import division\n",
    "from nltk.collocations import *\n",
    "import string, random\n",
    "from nltk.corpus import brown\n",
    "from nltk.collocations import *\n",
    "from string import punctuation\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk import bigrams\n",
    "from nltk import collocations\n",
    "from nltk import trigrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start by loading the .csv files and setting the columns.\n",
    "### For these .csv files, downloaded from https://github.com/ytsvetko/metaphor inputs and saved as .csv with the first row added as \"sample\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>angry welt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bald assertion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bare outline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>black humor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blind alley</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           sample\n",
       "0      angry welt\n",
       "1  bald assertion\n",
       "2    bare outline\n",
       "3     black humor\n",
       "4     blind alley"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_anmet = pd.read_csv(\"an_mets.csv\", low_memory=False)\n",
    "df_anmet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_anmet = pd.read_csv(\"an_mets.csv\", low_memory=False)\n",
    "df_anmet['metaphor'] = 1\n",
    "df_anmet['an'] = 1\n",
    "df_anmet['svo'] = 0\n",
    "df_anmet['metanet'] = 0\n",
    "df_anmet.head()\n",
    "len(df_anmet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_annonmet = pd.read_csv(\"an_nonmets.csv\", low_memory=False)\n",
    "df_annonmet['metaphor'] = 0\n",
    "df_annonmet['an'] = 1\n",
    "df_annonmet['svo'] = 0\n",
    "df_annonmet['metanet'] = 0\n",
    "df_annonmet.head()\n",
    "len(df_annonmet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_svomet = pd.read_csv(\"svo_mets.csv\", low_memory=False)\n",
    "df_svomet['metaphor'] = 1\n",
    "df_svomet['an'] = 0\n",
    "df_svomet['svo'] = 1\n",
    "df_svomet['metanet'] = 0\n",
    "df_svomet.head()\n",
    "len(df_svomet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_svononmet = pd.read_csv(\"svo_nonmets.csv\", low_memory=False)\n",
    "df_svononmet['metaphor'] = 0\n",
    "df_svononmet['an'] = 0\n",
    "df_svononmet['svo'] = 1\n",
    "df_svononmet['metanet'] = 0\n",
    "df_svononmet.head()\n",
    "len(df_svononmet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>metaphor</th>\n",
       "      <th>an</th>\n",
       "      <th>svo</th>\n",
       "      <th>metanet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ability to evaluate government is ability to see</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ability to evaluate is ability to see</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ability to know is ability to see</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abusive political leaders are physical bullies</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>accepting is swallowing</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sample  metaphor  an  svo  \\\n",
       "0  ability to evaluate government is ability to see         1   0    0   \n",
       "1             ability to evaluate is ability to see         1   0    0   \n",
       "2                 ability to know is ability to see         1   0    0   \n",
       "3    abusive political leaders are physical bullies         1   0    0   \n",
       "4                           accepting is swallowing         1   0    0   \n",
       "\n",
       "   metanet  \n",
       "0        1  \n",
       "1        1  \n",
       "2        1  \n",
       "3        1  \n",
       "4        1  "
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metanet = pd.read_csv(\"metanet.csv\", low_memory=False)\n",
    "df_metanet['metaphor'] = 1\n",
    "df_metanet['an'] = 0\n",
    "df_metanet['svo'] = 0\n",
    "df_metanet['metanet'] = 1\n",
    "df_metanet.head()\n",
    "# len(df_metanet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>metaphor</th>\n",
       "      <th>an</th>\n",
       "      <th>svo</th>\n",
       "      <th>metanet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>conversation turn subject</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>resumption bring relief</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>economy move direction</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>service meet expectation</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>material live dream</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>unemployment stand *none</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>action talk *none</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>statement sit *none</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>income fall *none</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>silence speak volume</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>car decide *none</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>battery die *none</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>accident wait *none</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>Texan break record</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>temperature break *number</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>insurance cover care</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>state cut spending</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>Hawaii kill  proposal</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>electronics drive innovation</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>teenager wear attitude</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>*pronoun catch flight</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>envy eat *none</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>data point pain</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>advertiser pull ad</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>*pronoun close deal</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>police close investigation</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>bicycle suffer damage</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>Tori throw tantrum</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>fortune smile *pronoun</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>excitement fill street</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>analysis of social problems is diagnosis of af...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>analyzing is dissecting</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>anger is fire</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>anger is heat</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>anger is insanity</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>anger is pressure in a container</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>anger is the heat of fluid in a container</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>argument is physical combat</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>argument is war</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>arithmetic is object construction</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>assessing is measuring</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>assistance is support</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>attaining control is gaining a possession</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>attributes are entities</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>attributes are possessions</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>attributes of government are entities</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>augmenting economic assets is creating objects</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>bad is stinky</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>becoming impoverished is moving downwards</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>being alive is being physically at this location</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>being good is being upright</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>being immoral is being low</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>being impoverished is being at a low location</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>being impoverished is being in a bounded region</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>being in a high social class is being high on ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>being in a low social class is being low on a ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>being in a middle class is being in the middle...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>being in a state is being at a point on a line...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>being poised to know is being positioned to se...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>being wealthy is being at a high location</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sample  metaphor  an  svo  \\\n",
       "200                          conversation turn subject         1   0    1   \n",
       "201                            resumption bring relief         1   0    1   \n",
       "202                             economy move direction         1   0    1   \n",
       "203                           service meet expectation         1   0    1   \n",
       "204                                material live dream         1   0    1   \n",
       "205                           unemployment stand *none         1   0    1   \n",
       "206                                  action talk *none         1   0    1   \n",
       "207                                statement sit *none         1   0    1   \n",
       "208                                  income fall *none         1   0    1   \n",
       "209                               silence speak volume         1   0    1   \n",
       "210                                   car decide *none         1   0    1   \n",
       "211                                  battery die *none         1   0    1   \n",
       "212                                accident wait *none         1   0    1   \n",
       "213                                 Texan break record         1   0    1   \n",
       "214                          temperature break *number         1   0    1   \n",
       "215                               insurance cover care         1   0    1   \n",
       "216                                 state cut spending         1   0    1   \n",
       "217                              Hawaii kill  proposal         1   0    1   \n",
       "218                       electronics drive innovation         1   0    1   \n",
       "219                             teenager wear attitude         1   0    1   \n",
       "220                              *pronoun catch flight         1   0    1   \n",
       "221                                     envy eat *none         1   0    1   \n",
       "222                                    data point pain         1   0    1   \n",
       "223                                 advertiser pull ad         1   0    1   \n",
       "224                                *pronoun close deal         1   0    1   \n",
       "225                         police close investigation         1   0    1   \n",
       "226                              bicycle suffer damage         1   0    1   \n",
       "227                                 Tori throw tantrum         1   0    1   \n",
       "228                             fortune smile *pronoun         1   0    1   \n",
       "229                             excitement fill street         1   0    1   \n",
       "..                                                 ...       ...  ..  ...   \n",
       "470  analysis of social problems is diagnosis of af...         1   0    0   \n",
       "471                            analyzing is dissecting         1   0    0   \n",
       "472                                      anger is fire         1   0    0   \n",
       "473                                      anger is heat         1   0    0   \n",
       "474                                  anger is insanity         1   0    0   \n",
       "475                   anger is pressure in a container         1   0    0   \n",
       "476          anger is the heat of fluid in a container         1   0    0   \n",
       "477                        argument is physical combat         1   0    0   \n",
       "478                                    argument is war         1   0    0   \n",
       "479                  arithmetic is object construction         1   0    0   \n",
       "480                             assessing is measuring         1   0    0   \n",
       "481                              assistance is support         1   0    0   \n",
       "482          attaining control is gaining a possession         1   0    0   \n",
       "483                            attributes are entities         1   0    0   \n",
       "484                         attributes are possessions         1   0    0   \n",
       "485              attributes of government are entities         1   0    0   \n",
       "486     augmenting economic assets is creating objects         1   0    0   \n",
       "487                                      bad is stinky         1   0    0   \n",
       "488          becoming impoverished is moving downwards         1   0    0   \n",
       "489   being alive is being physically at this location         1   0    0   \n",
       "490                        being good is being upright         1   0    0   \n",
       "491                         being immoral is being low         1   0    0   \n",
       "492      being impoverished is being at a low location         1   0    0   \n",
       "493    being impoverished is being in a bounded region         1   0    0   \n",
       "494  being in a high social class is being high on ...         1   0    0   \n",
       "495  being in a low social class is being low on a ...         1   0    0   \n",
       "496  being in a middle class is being in the middle...         1   0    0   \n",
       "497  being in a state is being at a point on a line...         1   0    0   \n",
       "498  being poised to know is being positioned to se...         1   0    0   \n",
       "499          being wealthy is being at a high location         1   0    0   \n",
       "\n",
       "     metanet  \n",
       "200        0  \n",
       "201        0  \n",
       "202        0  \n",
       "203        0  \n",
       "204        0  \n",
       "205        0  \n",
       "206        0  \n",
       "207        0  \n",
       "208        0  \n",
       "209        0  \n",
       "210        0  \n",
       "211        0  \n",
       "212        0  \n",
       "213        0  \n",
       "214        0  \n",
       "215        0  \n",
       "216        0  \n",
       "217        0  \n",
       "218        0  \n",
       "219        0  \n",
       "220        0  \n",
       "221        0  \n",
       "222        0  \n",
       "223        0  \n",
       "224        0  \n",
       "225        0  \n",
       "226        0  \n",
       "227        0  \n",
       "228        0  \n",
       "229        0  \n",
       "..       ...  \n",
       "470        1  \n",
       "471        1  \n",
       "472        1  \n",
       "473        1  \n",
       "474        1  \n",
       "475        1  \n",
       "476        1  \n",
       "477        1  \n",
       "478        1  \n",
       "479        1  \n",
       "480        1  \n",
       "481        1  \n",
       "482        1  \n",
       "483        1  \n",
       "484        1  \n",
       "485        1  \n",
       "486        1  \n",
       "487        1  \n",
       "488        1  \n",
       "489        1  \n",
       "490        1  \n",
       "491        1  \n",
       "492        1  \n",
       "493        1  \n",
       "494        1  \n",
       "495        1  \n",
       "496        1  \n",
       "497        1  \n",
       "498        1  \n",
       "499        1  \n",
       "\n",
       "[300 rows x 5 columns]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#combine into one df\n",
    "frames = [df_anmet, df_annonmet, df_svomet, df_svononmet, df_metanet]\n",
    "df_combo = pd.concat(frames)\n",
    "df_combo.reset_index(drop=True, inplace=True)\n",
    "df_combo.shape\n",
    "# df_combo.replace(to_replace='none', value=\"\")\n",
    "# #         tokenizer = RegexpTokenizer(r'\\w+')\n",
    "# #         word_list = tokenizer.tokenize(line)\n",
    "# #         filtered_words = [word for word in word_list if word not in stopwords.words('english')]\n",
    "# df_combo.replace(to_replace='is', value=\"\")\n",
    "stop = ['*none', '.', 'is']\n",
    "df_combo['sample'].apply(lambda x: [item for item in x if item not in stop])\n",
    "df_combo[200:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Break down into training, dev, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1606"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_index = np.random.permutation(df_combo.index)\n",
    "df_combo.ix[random_index, ['sample', 'metaphor', 'an', 'svo', 'metanet']]\n",
    "df_shuffled = df_combo.ix[random_index, ['sample', 'metaphor', 'an', 'svo', 'metanet']]\n",
    "df_shuffled.reset_index(drop=True, inplace=True)\n",
    "len(df_shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 1606\n",
      "Columns: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 5)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows, columns = df_shuffled.shape\n",
    "print(\"Rows:\", rows)\n",
    "print(\"Columns:\", columns)\n",
    "#train_size = round(rows*.6)\n",
    "train_size = round(rows*.9)\n",
    "#dev_size   = round(rows*.2)\n",
    "dev_size   = round(rows*.1)\n",
    "df_train = df_shuffled.loc[:train_size]\n",
    "df_train.shape\n",
    "df_dev = df_shuffled.loc[train_size:dev_size+train_size].reset_index(drop=True)\n",
    "df_dev.shape\n",
    "df_test = df_shuffled.loc[dev_size+train_size:].reset_index(drop=True)\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer\n",
    "### Use count vecotrizer with df = 3 for unigram and bigrams to predict dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = list(set(brown.words()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# vec = CountVectorizer(ngram_range=(1, 3), token_pattern=r'\\b\\w+\\b', analyzer=u'word', min_df=1, vocabulary=vocab)\n",
    "# df_train = df_train.fillna(\"\")\n",
    "# df_dev = df_dev.fillna(\"\")\n",
    "# df_test = df_test.fillna(\"\")\n",
    "\n",
    "\n",
    "vec = TfidfVectorizer(ngram_range=(2, 3), token_pattern=r'\\b\\w+\\b', analyzer=u'char', min_df=1)\n",
    "df_train = df_train.fillna(\"\")\n",
    "df_dev = df_dev.fillna(\"\")\n",
    "df_test = df_test.fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arr_train_feature_sparse = vec.fit_transform(df_train['sample'])\n",
    "arr_train_feature_sparse\n",
    "arr_train_feature = arr_train_feature_sparse.toarray()\n",
    "feature_labels = vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arr_dev_feature_sparse = vec.transform(df_dev[\"sample\"])\n",
    "arr_dev_feature = arr_dev_feature_sparse.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88198757763975155"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor']) #defining features (from reviews) and passing in Category label\n",
    "logreg_predictions = logreg_model.predict(arr_dev_feature)\n",
    "accuracy_score(df_dev['metaphor'], logreg_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:5: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>s</th>\n",
       "      <td>55.478196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>54.085359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>51.684293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>50.882446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>49.849230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ti</th>\n",
       "      <td>49.823581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on</th>\n",
       "      <td>49.616411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>45.124110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>44.978664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ng</th>\n",
       "      <td>41.920643</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        counts\n",
       "s    55.478196\n",
       "in   54.085359\n",
       " i   51.684293\n",
       "is   50.882446\n",
       " a   49.849230\n",
       "ti   49.823581\n",
       "on   49.616411\n",
       "is   45.124110\n",
       " is  44.978664\n",
       "ng   41.920643"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_sum = arr_train_feature.sum(axis=0)   #sum the counts of each feature\n",
    "\n",
    "df_feature_sum = pd.DataFrame({'counts': feature_sum})\n",
    "df_feature_sum.index = vec.get_feature_names()\n",
    "df_feature_sum.sort('counts', ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## make a file of sample sentences to practice on\n",
    "## loop through sentence and .split()\n",
    "## use nltk bigrams\n",
    "##\n",
    "## put into pandas as df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jamal', 'pig', 'dinner']\n"
     ]
    }
   ],
   "source": [
    "word_list = 'Jamal was a pig at dinner'.split()\n",
    "filtered_words = [word for word in word_list if word not in stopwords.words('english')]\n",
    "filtered_words\n",
    "testmeta = list(trigrams(filtered_words))\n",
    "testmeta\n",
    "testmeta1 = [' '.join(x) for x in testmeta]\n",
    "testmeta1\n",
    "df_test = pd.DataFrame(testmeta1)\n",
    "df_test\n",
    "df_test.columns = ['sample']\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_test_feature_sparse = vec.transform(df_test['sample'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "logreg_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jamal pig dinner</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0\n",
       "0  Jamal pig dinner"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.DataFrame(testmeta1)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"test_meta_sent.txt\", \"r\") as testset:\n",
    "    filtered_words = []\n",
    "    testmeta = []\n",
    "    testmeta1 = []\n",
    "    word_list = ''\n",
    "    for line in testset:\n",
    "#         word_list = line.split()\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        word_list = tokenizer.tokenize(line)\n",
    "        filtered_words = [word for word in word_list if word not in stopwords.words('english')]\n",
    "#         testmeta1.append(' '.join(filtered_words))\n",
    "        testmeta1.append(' '.join(x) for x in bigrams(filtered_words))\n",
    "#         testmeta1.append(' '.join(x) for x in filtered_words)\n",
    "# print(testmeta1)\n",
    "\n",
    "df_test =pd.DataFrame(testmeta1)\n",
    "df_test\n",
    "df_test = df_test.fillna(\"\")\n",
    "df_test\n",
    "# df_test.columns = ['sample']\n",
    "# df_test = pd.DataFrame(testmeta1)\n",
    "# df_test\n",
    "df_test.columns = ['sample0', 'sample1', 'sample2', 'sample3', 'sample4', 'sample5', 'sample6']\n",
    "# df_test = df_test.fillna(\"\")\n",
    "# df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-216-29cc55d9702a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogreg_predictions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# df_test['metaphor0'] = logreg_predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sample0'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'metaphor0'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__nonzero__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    890\u001b[0m         raise ValueError(\"The truth value of a {0} is ambiguous. \"\n\u001b[1;32m    891\u001b[0m                          \u001b[0;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m                          .format(self.__class__.__name__))\n\u001b[0m\u001b[1;32m    893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m     \u001b[0m__bool__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "arr_test_feature_sparse = vec.transform(df_test['sample0'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "# df_test['metaphor0'] = logreg_predictions\n",
    "if df_test['sample0'] == '':\n",
    "    df_test['metaphor0'] = 0\n",
    "else:\n",
    "    df_test['metaphor0'] = logreg_predictions\n",
    "\n",
    "arr_test_feature_sparse = vec.transform(df_test['sample1'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "# df_test['metaphor1'] = logreg_predictions\n",
    "if df_test['sample1'] == '':\n",
    "    df_test['metaphor1'] = 0\n",
    "else:\n",
    "    df_test['metaphor1'] = logreg_predictions\n",
    "\n",
    "arr_test_feature_sparse = vec.transform(df_test['sample2'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "# df_test['metaphor2'] = logreg_predictions\n",
    "if df_test['sample2'] == '':\n",
    "    df_test['metaphor2'] = 0\n",
    "else:\n",
    "    df_test['metaphor2'] = logreg_predictions\n",
    "\n",
    "arr_test_feature_sparse = vec.transform(df_test['sample3'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "# df_test['metaphor3'] = logreg_predictions\n",
    "if df_test['sample3'] == '':\n",
    "    df_test['metaphor3'] = 0\n",
    "else:\n",
    "    df_test['metaphor3'] = logreg_predictions\n",
    "\n",
    "arr_test_feature_sparse = vec.transform(df_test['sample4'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "# df_test['metaphor4'] = logreg_predictions\n",
    "if df_test['sample4'] == '':\n",
    "    df_test['metaphor4'] = 0\n",
    "else:\n",
    "    df_test['metaphor4'] = logreg_predictions\n",
    "\n",
    "arr_test_feature_sparse = vec.transform(df_test['sample5'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "# df_test['metaphor5'] = logreg_predictions\n",
    "if df_test['sample5'] == '':\n",
    "    df_test['metaphor5'] = 0\n",
    "else:\n",
    "    df_test['metaphor5'] = logreg_predictions\n",
    "\n",
    "arr_test_feature_sparse = vec.transform(df_test['sample6'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "# df_test['metaphor6'] = logreg_predictions\n",
    "if df_test['sample6'] == '':\n",
    "    df_test['metaphor6'] = 0\n",
    "else:\n",
    "    df_test['metaphor6'] = logreg_predictions\n",
    "\n",
    "df_test['sum_metaphor'] = df_test['metaphor0'] + df_test['metaphor1'] + df_test['metaphor2'] + df_test['metaphor3'] +df_test['metaphor4'] +df_test['metaphor5'] + df_test['metaphor6'] \n",
    "\n",
    "# def count_metaphors():\n",
    "#     if sum(df_test['sum_metaphor']) >= 1:\n",
    "#         df_test['sum_metaphor'] = 1\n",
    "#     else:\n",
    "#         df_test['sum_metaphor'] = 0\n",
    "# count_metaphors()\n",
    "\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      True\n",
      "1      True\n",
      "2      True\n",
      "3      True\n",
      "4     False\n",
      "5      True\n",
      "6     False\n",
      "7      True\n",
      "8     False\n",
      "9      True\n",
      "10     True\n",
      "11     True\n",
      "12     True\n",
      "13     True\n",
      "14     True\n",
      "15    False\n",
      "16    False\n",
      "17     True\n",
      "18    False\n",
      "19    False\n",
      "20     True\n",
      "21     True\n",
      "22     True\n",
      "23     True\n",
      "24    False\n",
      "25     True\n",
      "26    False\n",
      "27    False\n",
      "28     True\n",
      "29     True\n",
      "      ...  \n",
      "41     True\n",
      "42     True\n",
      "43     True\n",
      "44     True\n",
      "45     True\n",
      "46     True\n",
      "47    False\n",
      "48    False\n",
      "49     True\n",
      "50     True\n",
      "51    False\n",
      "52     True\n",
      "53     True\n",
      "54     True\n",
      "55     True\n",
      "56     True\n",
      "57     True\n",
      "58    False\n",
      "59     True\n",
      "60     True\n",
      "61     True\n",
      "62    False\n",
      "63    False\n",
      "64    False\n",
      "65     True\n",
      "66    False\n",
      "67     True\n",
      "68     True\n",
      "69    False\n",
      "70    False\n",
      "Name: sample3, dtype: bool\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_test['sample3'] == '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4     10\n",
       "5      0\n",
       "6     13\n",
       "7      0\n",
       "8      9\n",
       "9      0\n",
       "10     0\n",
       "11     0\n",
       "12     0\n",
       "13     0\n",
       "14     0\n",
       "15    13\n",
       "16    12\n",
       "17     0\n",
       "18    13\n",
       "19    14\n",
       "20     0\n",
       "21     0\n",
       "22     0\n",
       "23     0\n",
       "24    11\n",
       "25     0\n",
       "26    14\n",
       "27    13\n",
       "28     0\n",
       "29     0\n",
       "      ..\n",
       "41     0\n",
       "42     0\n",
       "43     0\n",
       "44     0\n",
       "45     0\n",
       "46     0\n",
       "47    13\n",
       "48    10\n",
       "49     0\n",
       "50     0\n",
       "51    11\n",
       "52     0\n",
       "53     0\n",
       "54     0\n",
       "55     0\n",
       "56     0\n",
       "57     0\n",
       "58    11\n",
       "59     0\n",
       "60     0\n",
       "61     0\n",
       "62    12\n",
       "63    17\n",
       "64    14\n",
       "65     0\n",
       "66    10\n",
       "67     0\n",
       "68     0\n",
       "69    11\n",
       "70    14\n",
       "Name: sample3, dtype: int64"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['sample3'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"an_nonmets.txt\", \"r\") as testset:\n",
    "    filtered_words = []\n",
    "    testmeta = []\n",
    "    testmeta1 = []\n",
    "    word_list = ''\n",
    "    for line in testset:\n",
    "        word_list = line.split()\n",
    "        filtered_words = [word for word in word_list if word not in stopwords.words('english')]\n",
    "        testmeta1.append(' '.join(x) for x in bigrams(filtered_words))\n",
    "df_test = pd.DataFrame(testmeta1)\n",
    "df_test\n",
    "df_test.columns = ['sample0']\n",
    "df_test = df_test.fillna(\"\")\n",
    "df_test\n",
    "\n",
    "arr_test_feature_sparse = vec.transform(df_test['sample0'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "df_test['metaphor0'] = logreg_predictions\n",
    "\n",
    "sum(df_test['metaphor0'])\n",
    "#len(df_test['metaphor0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Scientists created â€œatlas</td>\n",
       "      <td>created â€œatlas brainâ€</td>\n",
       "      <td>â€œatlas brainâ€ reveals</td>\n",
       "      <td>brainâ€ reveals meanings</td>\n",
       "      <td>reveals meanings words</td>\n",
       "      <td>meanings words arranged</td>\n",
       "      <td>words arranged across</td>\n",
       "      <td>arranged across different</td>\n",
       "      <td>across different regions</td>\n",
       "      <td>different regions organ.</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Like colourful quilt</td>\n",
       "      <td>colourful quilt laid</td>\n",
       "      <td>quilt laid cortex,</td>\n",
       "      <td>laid cortex, atlas</td>\n",
       "      <td>cortex, atlas displays</td>\n",
       "      <td>atlas displays rainbow</td>\n",
       "      <td>displays rainbow hues</td>\n",
       "      <td>rainbow hues individual</td>\n",
       "      <td>hues individual words</td>\n",
       "      <td>individual words concepts</td>\n",
       "      <td>...</td>\n",
       "      <td>concepts convey grouped</td>\n",
       "      <td>convey grouped together</td>\n",
       "      <td>grouped together clumps</td>\n",
       "      <td>together clumps white</td>\n",
       "      <td>clumps white matter.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>â€œOur goal build</td>\n",
       "      <td>goal build giant</td>\n",
       "      <td>build giant atlas</td>\n",
       "      <td>giant atlas shows</td>\n",
       "      <td>atlas shows one</td>\n",
       "      <td>shows one specific</td>\n",
       "      <td>one specific aspect</td>\n",
       "      <td>specific aspect language</td>\n",
       "      <td>aspect language represented</td>\n",
       "      <td>language represented brain,</td>\n",
       "      <td>...</td>\n",
       "      <td>brain, case semantics,</td>\n",
       "      <td>case semantics, meanings</td>\n",
       "      <td>semantics, meanings words,â€</td>\n",
       "      <td>meanings words,â€ said</td>\n",
       "      <td>words,â€ said Jack</td>\n",
       "      <td>said Jack Gallant,</td>\n",
       "      <td>Jack Gallant, neuroscientist</td>\n",
       "      <td>Gallant, neuroscientist University</td>\n",
       "      <td>neuroscientist University California,</td>\n",
       "      <td>University California, Berkeley.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No single brain</td>\n",
       "      <td>single brain region</td>\n",
       "      <td>brain region holds</td>\n",
       "      <td>region holds one</td>\n",
       "      <td>holds one word</td>\n",
       "      <td>one word concept.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A single brain</td>\n",
       "      <td>single brain spot</td>\n",
       "      <td>brain spot associated</td>\n",
       "      <td>spot associated number</td>\n",
       "      <td>associated number related</td>\n",
       "      <td>number related words.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>And single word</td>\n",
       "      <td>single word lights</td>\n",
       "      <td>word lights many</td>\n",
       "      <td>lights many different</td>\n",
       "      <td>many different brain</td>\n",
       "      <td>different brain spots.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Together make networks</td>\n",
       "      <td>make networks represent</td>\n",
       "      <td>networks represent meanings</td>\n",
       "      <td>represent meanings word</td>\n",
       "      <td>meanings word use:</td>\n",
       "      <td>word use: life</td>\n",
       "      <td>use: life love;</td>\n",
       "      <td>life love; death</td>\n",
       "      <td>love; death taxes;</td>\n",
       "      <td>death taxes; clouds,</td>\n",
       "      <td>...</td>\n",
       "      <td>clouds, Florida bra.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>All light networks.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Described â€œtour de</td>\n",
       "      <td>â€œtour de forceâ€</td>\n",
       "      <td>de forceâ€ one</td>\n",
       "      <td>forceâ€ one researcher</td>\n",
       "      <td>one researcher involved</td>\n",
       "      <td>researcher involved study,</td>\n",
       "      <td>involved study, atlas</td>\n",
       "      <td>study, atlas demonstrates</td>\n",
       "      <td>atlas demonstrates modern</td>\n",
       "      <td>demonstrates modern imaging</td>\n",
       "      <td>...</td>\n",
       "      <td>imaging transform knowledge</td>\n",
       "      <td>transform knowledge brain</td>\n",
       "      <td>knowledge brain performs</td>\n",
       "      <td>brain performs important</td>\n",
       "      <td>performs important tasks.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>With advances, technology</td>\n",
       "      <td>advances, technology could</td>\n",
       "      <td>technology could profound</td>\n",
       "      <td>could profound impact</td>\n",
       "      <td>profound impact medicine</td>\n",
       "      <td>impact medicine fields.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>â€œIt possible approach</td>\n",
       "      <td>possible approach could</td>\n",
       "      <td>approach could used</td>\n",
       "      <td>could used decode</td>\n",
       "      <td>used decode information</td>\n",
       "      <td>decode information words</td>\n",
       "      <td>information words person</td>\n",
       "      <td>words person hearing,</td>\n",
       "      <td>person hearing, reading,</td>\n",
       "      <td>hearing, reading, possibly</td>\n",
       "      <td>...</td>\n",
       "      <td>possibly even thinking,â€</td>\n",
       "      <td>even thinking,â€ said</td>\n",
       "      <td>thinking,â€ said Alexander</td>\n",
       "      <td>said Alexander Huth,</td>\n",
       "      <td>Alexander Huth, first</td>\n",
       "      <td>Huth, first author</td>\n",
       "      <td>first author study.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>One potential use</td>\n",
       "      <td>potential use would</td>\n",
       "      <td>use would language</td>\n",
       "      <td>would language decoder</td>\n",
       "      <td>language decoder could</td>\n",
       "      <td>decoder could allow</td>\n",
       "      <td>could allow people</td>\n",
       "      <td>allow people silenced</td>\n",
       "      <td>people silenced motor</td>\n",
       "      <td>silenced motor neurone</td>\n",
       "      <td>...</td>\n",
       "      <td>neurone disease locked-in</td>\n",
       "      <td>disease locked-in syndrome</td>\n",
       "      <td>locked-in syndrome speak</td>\n",
       "      <td>syndrome speak computer.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>To create atlas,</td>\n",
       "      <td>create atlas, scientists</td>\n",
       "      <td>atlas, scientists recorded</td>\n",
       "      <td>scientists recorded peopleâ€™s</td>\n",
       "      <td>recorded peopleâ€™s brain</td>\n",
       "      <td>peopleâ€™s brain activity</td>\n",
       "      <td>brain activity listened</td>\n",
       "      <td>activity listened stories</td>\n",
       "      <td>listened stories read</td>\n",
       "      <td>stories read The</td>\n",
       "      <td>...</td>\n",
       "      <td>The Moth Radio</td>\n",
       "      <td>Moth Radio Hour,</td>\n",
       "      <td>Radio Hour, US</td>\n",
       "      <td>Hour, US radio</td>\n",
       "      <td>US radio show.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>They matched transcripts</td>\n",
       "      <td>matched transcripts stories</td>\n",
       "      <td>transcripts stories brain</td>\n",
       "      <td>stories brain activity</td>\n",
       "      <td>brain activity data</td>\n",
       "      <td>activity data show</td>\n",
       "      <td>data show groups</td>\n",
       "      <td>show groups related</td>\n",
       "      <td>groups related words</td>\n",
       "      <td>related words triggered</td>\n",
       "      <td>...</td>\n",
       "      <td>triggered neural responses</td>\n",
       "      <td>neural responses 50,000</td>\n",
       "      <td>responses 50,000 80,000</td>\n",
       "      <td>50,000 80,000 pea-sized</td>\n",
       "      <td>80,000 pea-sized spots</td>\n",
       "      <td>pea-sized spots cerebral</td>\n",
       "      <td>spots cerebral cortex.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Huth used stories</td>\n",
       "      <td>used stories The</td>\n",
       "      <td>stories The Moth</td>\n",
       "      <td>The Moth Radio</td>\n",
       "      <td>Moth Radio Hour</td>\n",
       "      <td>Radio Hour short</td>\n",
       "      <td>Hour short compelling.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>The enthralling stories,</td>\n",
       "      <td>enthralling stories, confident</td>\n",
       "      <td>stories, confident scientists</td>\n",
       "      <td>confident scientists could</td>\n",
       "      <td>scientists could people</td>\n",
       "      <td>could people scanned</td>\n",
       "      <td>people scanned focusing</td>\n",
       "      <td>scanned focusing words</td>\n",
       "      <td>focusing words drifting</td>\n",
       "      <td>words drifting off.</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Seven people listened</td>\n",
       "      <td>people listened two</td>\n",
       "      <td>listened two hours</td>\n",
       "      <td>two hours stories</td>\n",
       "      <td>hours stories each.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Per person, amounted</td>\n",
       "      <td>person, amounted hearing</td>\n",
       "      <td>amounted hearing roughly</td>\n",
       "      <td>hearing roughly 25,000</td>\n",
       "      <td>roughly 25,000 words-</td>\n",
       "      <td>25,000 words- 3,000</td>\n",
       "      <td>words- 3,000 different</td>\n",
       "      <td>3,000 different words</td>\n",
       "      <td>different words -</td>\n",
       "      <td>words - lay</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>The atlas shows</td>\n",
       "      <td>atlas shows words</td>\n",
       "      <td>shows words related</td>\n",
       "      <td>words related terms</td>\n",
       "      <td>related terms exercise</td>\n",
       "      <td>terms exercise regions</td>\n",
       "      <td>exercise regions brain.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>For example, left-hand</td>\n",
       "      <td>example, left-hand side</td>\n",
       "      <td>left-hand side brain,</td>\n",
       "      <td>side brain, ear,</td>\n",
       "      <td>brain, ear, one</td>\n",
       "      <td>ear, one tiny</td>\n",
       "      <td>one tiny regions</td>\n",
       "      <td>tiny regions represents</td>\n",
       "      <td>regions represents word</td>\n",
       "      <td>represents word â€œvictimâ€.</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>The region responds</td>\n",
       "      <td>region responds â€œkilledâ€,</td>\n",
       "      <td>responds â€œkilledâ€, â€œconvictedâ€,</td>\n",
       "      <td>â€œkilledâ€, â€œconvictedâ€, â€œmurderedâ€</td>\n",
       "      <td>â€œconvictedâ€, â€œmurderedâ€ â€œconfessedâ€.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>On brainâ€™s right-hand</td>\n",
       "      <td>brainâ€™s right-hand side,</td>\n",
       "      <td>right-hand side, near</td>\n",
       "      <td>side, near top</td>\n",
       "      <td>near top head,</td>\n",
       "      <td>top head, one</td>\n",
       "      <td>head, one brain</td>\n",
       "      <td>one brain spots</td>\n",
       "      <td>brain spots activated</td>\n",
       "      <td>spots activated family</td>\n",
       "      <td>...</td>\n",
       "      <td>family terms: â€œwifeâ€,</td>\n",
       "      <td>terms: â€œwifeâ€, â€œhusbandâ€,</td>\n",
       "      <td>â€œwifeâ€, â€œhusbandâ€, â€œchildrenâ€,</td>\n",
       "      <td>â€œhusbandâ€, â€œchildrenâ€, â€œparentsâ€.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Each word represented</td>\n",
       "      <td>word represented one</td>\n",
       "      <td>represented one spot</td>\n",
       "      <td>one spot words</td>\n",
       "      <td>spot words tend</td>\n",
       "      <td>words tend several</td>\n",
       "      <td>tend several meanings.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>One part brain,</td>\n",
       "      <td>part brain, example,</td>\n",
       "      <td>brain, example, reliably</td>\n",
       "      <td>example, reliably responds</td>\n",
       "      <td>reliably responds word</td>\n",
       "      <td>responds word â€œtopâ€,</td>\n",
       "      <td>word â€œtopâ€, along</td>\n",
       "      <td>â€œtopâ€, along words</td>\n",
       "      <td>along words describe</td>\n",
       "      <td>words describe clothing.</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>But word â€œtopâ€</td>\n",
       "      <td>word â€œtopâ€ activates</td>\n",
       "      <td>â€œtopâ€ activates many</td>\n",
       "      <td>activates many regions.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>One responds numbers</td>\n",
       "      <td>responds numbers measurements,</td>\n",
       "      <td>numbers measurements, another</td>\n",
       "      <td>measurements, another buildings</td>\n",
       "      <td>another buildings places.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>The scientists created</td>\n",
       "      <td>scientists created interactive</td>\n",
       "      <td>created interactive website</td>\n",
       "      <td>interactive website public</td>\n",
       "      <td>website public explore</td>\n",
       "      <td>public explore brain</td>\n",
       "      <td>explore brain atlas.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Strikingly, brain atlases</td>\n",
       "      <td>brain atlases similar</td>\n",
       "      <td>atlases similar participants,</td>\n",
       "      <td>similar participants, suggesting</td>\n",
       "      <td>participants, suggesting brains</td>\n",
       "      <td>suggesting brains organised</td>\n",
       "      <td>brains organised meanings</td>\n",
       "      <td>organised meanings words</td>\n",
       "      <td>meanings words way.</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>The scientists scanned</td>\n",
       "      <td>scientists scanned five</td>\n",
       "      <td>scanned five men</td>\n",
       "      <td>five men two</td>\n",
       "      <td>men two women,</td>\n",
       "      <td>two women, however.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>All native English</td>\n",
       "      <td>native English speakers,</td>\n",
       "      <td>English speakers, two</td>\n",
       "      <td>speakers, two authors</td>\n",
       "      <td>two authors study</td>\n",
       "      <td>authors study published</td>\n",
       "      <td>study published Nature.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>It highly possible</td>\n",
       "      <td>highly possible people</td>\n",
       "      <td>possible people different</td>\n",
       "      <td>people different backgrounds</td>\n",
       "      <td>different backgrounds cultures</td>\n",
       "      <td>backgrounds cultures different</td>\n",
       "      <td>cultures different semantic</td>\n",
       "      <td>different semantic brain</td>\n",
       "      <td>semantic brain atlases.</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Armed atlas, researchers</td>\n",
       "      <td>atlas, researchers piece</td>\n",
       "      <td>researchers piece together</td>\n",
       "      <td>piece together brain</td>\n",
       "      <td>together brain networks</td>\n",
       "      <td>brain networks represent</td>\n",
       "      <td>networks represent wildly</td>\n",
       "      <td>represent wildly different</td>\n",
       "      <td>wildly different concepts,</td>\n",
       "      <td>different concepts, numbers</td>\n",
       "      <td>...</td>\n",
       "      <td>numbers murder religion.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>â€œThe idea murder</td>\n",
       "      <td>idea murder represented</td>\n",
       "      <td>murder represented lot</td>\n",
       "      <td>represented lot brain,â€</td>\n",
       "      <td>lot brain,â€ Gallant</td>\n",
       "      <td>brain,â€ Gallant said.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Using haul data,</td>\n",
       "      <td>haul data, group</td>\n",
       "      <td>data, group begun</td>\n",
       "      <td>group begun work</td>\n",
       "      <td>begun work new</td>\n",
       "      <td>work new atlases</td>\n",
       "      <td>new atlases show</td>\n",
       "      <td>atlases show brain</td>\n",
       "      <td>show brain holds</td>\n",
       "      <td>brain holds information</td>\n",
       "      <td>...</td>\n",
       "      <td>information aspects language,</td>\n",
       "      <td>aspects language, phonemes</td>\n",
       "      <td>language, phonemes syntax.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>A brain atlas</td>\n",
       "      <td>brain atlas narrative</td>\n",
       "      <td>atlas narrative structure</td>\n",
       "      <td>narrative structure far</td>\n",
       "      <td>structure far proved</td>\n",
       "      <td>far proved elusive,</td>\n",
       "      <td>proved elusive, however.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>â€œEvery time come</td>\n",
       "      <td>time come set</td>\n",
       "      <td>come set narrative</td>\n",
       "      <td>set narrative features,</td>\n",
       "      <td>narrative features, get</td>\n",
       "      <td>features, get told</td>\n",
       "      <td>get told arenâ€™t</td>\n",
       "      <td>told arenâ€™t right</td>\n",
       "      <td>arenâ€™t right set</td>\n",
       "      <td>right set narrative</td>\n",
       "      <td>...</td>\n",
       "      <td>narrative features,â€ said</td>\n",
       "      <td>features,â€ said Gallant.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Uri Hasson, neuroscientist</td>\n",
       "      <td>Hasson, neuroscientist Princeton</td>\n",
       "      <td>neuroscientist Princeton University,</td>\n",
       "      <td>Princeton University, praised</td>\n",
       "      <td>University, praised work.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Unlike many studies</td>\n",
       "      <td>many studies looked</td>\n",
       "      <td>studies looked brain</td>\n",
       "      <td>looked brain activity</td>\n",
       "      <td>brain activity isolated</td>\n",
       "      <td>activity isolated word</td>\n",
       "      <td>isolated word sentence</td>\n",
       "      <td>word sentence spoken,</td>\n",
       "      <td>sentence spoken, Gallantâ€™s</td>\n",
       "      <td>spoken, Gallantâ€™s team</td>\n",
       "      <td>...</td>\n",
       "      <td>team shed light</td>\n",
       "      <td>shed light brain</td>\n",
       "      <td>light brain worked</td>\n",
       "      <td>brain worked real-world</td>\n",
       "      <td>worked real-world scenario,</td>\n",
       "      <td>real-world scenario, said.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>The next step,</td>\n",
       "      <td>next step, said,</td>\n",
       "      <td>step, said, create</td>\n",
       "      <td>said, create comprehensive</td>\n",
       "      <td>create comprehensive precise</td>\n",
       "      <td>comprehensive precise semantic</td>\n",
       "      <td>precise semantic brain</td>\n",
       "      <td>semantic brain atlas.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Ultimately, Hasson believes</td>\n",
       "      <td>Hasson believes possible</td>\n",
       "      <td>believes possible reconstruct</td>\n",
       "      <td>possible reconstruct words</td>\n",
       "      <td>reconstruct words person</td>\n",
       "      <td>words person thinking</td>\n",
       "      <td>person thinking brain</td>\n",
       "      <td>thinking brain activity.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>The ethical implications</td>\n",
       "      <td>ethical implications enormous.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>One benign use</td>\n",
       "      <td>benign use would</td>\n",
       "      <td>use would see</td>\n",
       "      <td>would see brain</td>\n",
       "      <td>see brain activity</td>\n",
       "      <td>brain activity used</td>\n",
       "      <td>activity used assess</td>\n",
       "      <td>used assess whether</td>\n",
       "      <td>assess whether political</td>\n",
       "      <td>whether political messages</td>\n",
       "      <td>...</td>\n",
       "      <td>messages effectively communicated</td>\n",
       "      <td>effectively communicated public.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>â€œThere many implications,</td>\n",
       "      <td>many implications, barely</td>\n",
       "      <td>implications, barely touching</td>\n",
       "      <td>barely touching surface,â€</td>\n",
       "      <td>touching surface,â€ said.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Lorraine Tyler, cognitive</td>\n",
       "      <td>Tyler, cognitive neuroscientist</td>\n",
       "      <td>cognitive neuroscientist head</td>\n",
       "      <td>neuroscientist head Centre</td>\n",
       "      <td>head Centre Speech,</td>\n",
       "      <td>Centre Speech, Language</td>\n",
       "      <td>Speech, Language Brain</td>\n",
       "      <td>Language Brain Cambridge</td>\n",
       "      <td>Brain Cambridge University</td>\n",
       "      <td>Cambridge University said</td>\n",
       "      <td>...</td>\n",
       "      <td>said research â€œtour</td>\n",
       "      <td>research â€œtour de</td>\n",
       "      <td>â€œtour de force</td>\n",
       "      <td>de force scope</td>\n",
       "      <td>force scope methodsâ€.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>But brain atlas</td>\n",
       "      <td>brain atlas current</td>\n",
       "      <td>atlas current form</td>\n",
       "      <td>current form capture</td>\n",
       "      <td>form capture fine</td>\n",
       "      <td>capture fine differences</td>\n",
       "      <td>fine differences word</td>\n",
       "      <td>differences word meanings.</td>\n",
       "      <td>word meanings. Take</td>\n",
       "      <td>meanings. Take word</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>It member many</td>\n",
       "      <td>member many different</td>\n",
       "      <td>many different groups,</td>\n",
       "      <td>different groups, says</td>\n",
       "      <td>groups, says Tyler.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>â€œIt something eat</td>\n",
       "      <td>something eat off,</td>\n",
       "      <td>eat off, things</td>\n",
       "      <td>off, things made</td>\n",
       "      <td>things made wood,</td>\n",
       "      <td>made wood, things</td>\n",
       "      <td>wood, things heavy,</td>\n",
       "      <td>things heavy, things</td>\n",
       "      <td>heavy, things four</td>\n",
       "      <td>things four legs,</td>\n",
       "      <td>...</td>\n",
       "      <td>legs, non-animate objects,</td>\n",
       "      <td>non-animate objects, on.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>This kind detailed</td>\n",
       "      <td>kind detailed semantic</td>\n",
       "      <td>detailed semantic information</td>\n",
       "      <td>semantic information enables</td>\n",
       "      <td>information enables words</td>\n",
       "      <td>enables words used</td>\n",
       "      <td>words used flexibly</td>\n",
       "      <td>used flexibly lost</td>\n",
       "      <td>flexibly lost analysis,â€</td>\n",
       "      <td>lost analysis,â€ said.</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>â€œWhile research path-breaking</td>\n",
       "      <td>research path-breaking scope,</td>\n",
       "      <td>path-breaking scope, still</td>\n",
       "      <td>scope, still lot</td>\n",
       "      <td>still lot learn</td>\n",
       "      <td>lot learn semantics</td>\n",
       "      <td>learn semantics represented</td>\n",
       "      <td>semantics represented brain.â€</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               0                                 1   \\\n",
       "0       Scientists created â€œatlas             created â€œatlas brainâ€   \n",
       "1            Like colourful quilt              colourful quilt laid   \n",
       "2                 â€œOur goal build                  goal build giant   \n",
       "3                 No single brain               single brain region   \n",
       "4                  A single brain                 single brain spot   \n",
       "5                 And single word                single word lights   \n",
       "6          Together make networks           make networks represent   \n",
       "7             All light networks.                              None   \n",
       "8              Described â€œtour de                   â€œtour de forceâ€   \n",
       "9       With advances, technology        advances, technology could   \n",
       "10          â€œIt possible approach           possible approach could   \n",
       "11              One potential use               potential use would   \n",
       "12               To create atlas,          create atlas, scientists   \n",
       "13       They matched transcripts       matched transcripts stories   \n",
       "14              Huth used stories                  used stories The   \n",
       "15       The enthralling stories,    enthralling stories, confident   \n",
       "16          Seven people listened               people listened two   \n",
       "17           Per person, amounted          person, amounted hearing   \n",
       "18                The atlas shows                 atlas shows words   \n",
       "19         For example, left-hand           example, left-hand side   \n",
       "20            The region responds         region responds â€œkilledâ€,   \n",
       "21          On brainâ€™s right-hand          brainâ€™s right-hand side,   \n",
       "22          Each word represented              word represented one   \n",
       "23                One part brain,              part brain, example,   \n",
       "24                 But word â€œtopâ€              word â€œtopâ€ activates   \n",
       "25           One responds numbers    responds numbers measurements,   \n",
       "26         The scientists created    scientists created interactive   \n",
       "27      Strikingly, brain atlases             brain atlases similar   \n",
       "28         The scientists scanned           scientists scanned five   \n",
       "29             All native English          native English speakers,   \n",
       "30             It highly possible            highly possible people   \n",
       "31       Armed atlas, researchers          atlas, researchers piece   \n",
       "32               â€œThe idea murder           idea murder represented   \n",
       "33               Using haul data,                  haul data, group   \n",
       "34                  A brain atlas             brain atlas narrative   \n",
       "35               â€œEvery time come                     time come set   \n",
       "36     Uri Hasson, neuroscientist  Hasson, neuroscientist Princeton   \n",
       "37            Unlike many studies               many studies looked   \n",
       "38                 The next step,                  next step, said,   \n",
       "39    Ultimately, Hasson believes          Hasson believes possible   \n",
       "40       The ethical implications    ethical implications enormous.   \n",
       "41                 One benign use                  benign use would   \n",
       "42      â€œThere many implications,         many implications, barely   \n",
       "43      Lorraine Tyler, cognitive   Tyler, cognitive neuroscientist   \n",
       "44                But brain atlas               brain atlas current   \n",
       "45                 It member many             member many different   \n",
       "46              â€œIt something eat                something eat off,   \n",
       "47             This kind detailed            kind detailed semantic   \n",
       "48  â€œWhile research path-breaking     research path-breaking scope,   \n",
       "\n",
       "                                      2                                  3   \\\n",
       "0                  â€œatlas brainâ€ reveals            brainâ€ reveals meanings   \n",
       "1                     quilt laid cortex,                 laid cortex, atlas   \n",
       "2                      build giant atlas                  giant atlas shows   \n",
       "3                     brain region holds                   region holds one   \n",
       "4                  brain spot associated             spot associated number   \n",
       "5                       word lights many              lights many different   \n",
       "6            networks represent meanings            represent meanings word   \n",
       "7                                   None                               None   \n",
       "8                          de forceâ€ one              forceâ€ one researcher   \n",
       "9              technology could profound              could profound impact   \n",
       "10                   approach could used                  could used decode   \n",
       "11                    use would language             would language decoder   \n",
       "12            atlas, scientists recorded       scientists recorded peopleâ€™s   \n",
       "13             transcripts stories brain             stories brain activity   \n",
       "14                      stories The Moth                     The Moth Radio   \n",
       "15         stories, confident scientists         confident scientists could   \n",
       "16                    listened two hours                  two hours stories   \n",
       "17              amounted hearing roughly             hearing roughly 25,000   \n",
       "18                   shows words related                words related terms   \n",
       "19                 left-hand side brain,                   side brain, ear,   \n",
       "20       responds â€œkilledâ€, â€œconvictedâ€,  â€œkilledâ€, â€œconvictedâ€, â€œmurderedâ€   \n",
       "21                 right-hand side, near                     side, near top   \n",
       "22                  represented one spot                     one spot words   \n",
       "23              brain, example, reliably         example, reliably responds   \n",
       "24                  â€œtopâ€ activates many            activates many regions.   \n",
       "25         numbers measurements, another    measurements, another buildings   \n",
       "26           created interactive website         interactive website public   \n",
       "27         atlases similar participants,   similar participants, suggesting   \n",
       "28                      scanned five men                       five men two   \n",
       "29                 English speakers, two              speakers, two authors   \n",
       "30             possible people different       people different backgrounds   \n",
       "31            researchers piece together               piece together brain   \n",
       "32                murder represented lot            represented lot brain,â€   \n",
       "33                     data, group begun                   group begun work   \n",
       "34             atlas narrative structure            narrative structure far   \n",
       "35                    come set narrative            set narrative features,   \n",
       "36  neuroscientist Princeton University,      Princeton University, praised   \n",
       "37                  studies looked brain              looked brain activity   \n",
       "38                    step, said, create         said, create comprehensive   \n",
       "39         believes possible reconstruct         possible reconstruct words   \n",
       "40                                  None                               None   \n",
       "41                         use would see                    would see brain   \n",
       "42         implications, barely touching          barely touching surface,â€   \n",
       "43         cognitive neuroscientist head         neuroscientist head Centre   \n",
       "44                    atlas current form               current form capture   \n",
       "45                many different groups,             different groups, says   \n",
       "46                       eat off, things                   off, things made   \n",
       "47         detailed semantic information       semantic information enables   \n",
       "48            path-breaking scope, still                   scope, still lot   \n",
       "\n",
       "                                      4                               5   \\\n",
       "0                 reveals meanings words         meanings words arranged   \n",
       "1                 cortex, atlas displays          atlas displays rainbow   \n",
       "2                        atlas shows one              shows one specific   \n",
       "3                         holds one word               one word concept.   \n",
       "4              associated number related           number related words.   \n",
       "5                   many different brain          different brain spots.   \n",
       "6                     meanings word use:                  word use: life   \n",
       "7                                   None                            None   \n",
       "8                one researcher involved      researcher involved study,   \n",
       "9               profound impact medicine         impact medicine fields.   \n",
       "10               used decode information        decode information words   \n",
       "11                language decoder could             decoder could allow   \n",
       "12               recorded peopleâ€™s brain         peopleâ€™s brain activity   \n",
       "13                   brain activity data              activity data show   \n",
       "14                       Moth Radio Hour                Radio Hour short   \n",
       "15               scientists could people            could people scanned   \n",
       "16                   hours stories each.                            None   \n",
       "17                 roughly 25,000 words-             25,000 words- 3,000   \n",
       "18                related terms exercise          terms exercise regions   \n",
       "19                       brain, ear, one                   ear, one tiny   \n",
       "20  â€œconvictedâ€, â€œmurderedâ€ â€œconfessedâ€.                            None   \n",
       "21                        near top head,                   top head, one   \n",
       "22                       spot words tend              words tend several   \n",
       "23                reliably responds word            responds word â€œtopâ€,   \n",
       "24                                  None                            None   \n",
       "25             another buildings places.                            None   \n",
       "26                website public explore            public explore brain   \n",
       "27       participants, suggesting brains     suggesting brains organised   \n",
       "28                        men two women,             two women, however.   \n",
       "29                     two authors study         authors study published   \n",
       "30        different backgrounds cultures  backgrounds cultures different   \n",
       "31               together brain networks        brain networks represent   \n",
       "32                   lot brain,â€ Gallant           brain,â€ Gallant said.   \n",
       "33                        begun work new                work new atlases   \n",
       "34                  structure far proved             far proved elusive,   \n",
       "35               narrative features, get              features, get told   \n",
       "36             University, praised work.                            None   \n",
       "37               brain activity isolated          activity isolated word   \n",
       "38          create comprehensive precise  comprehensive precise semantic   \n",
       "39              reconstruct words person           words person thinking   \n",
       "40                                  None                            None   \n",
       "41                    see brain activity             brain activity used   \n",
       "42              touching surface,â€ said.                            None   \n",
       "43                   head Centre Speech,         Centre Speech, Language   \n",
       "44                     form capture fine        capture fine differences   \n",
       "45                   groups, says Tyler.                            None   \n",
       "46                     things made wood,               made wood, things   \n",
       "47             information enables words              enables words used   \n",
       "48                       still lot learn             lot learn semantics   \n",
       "\n",
       "                             6                              7   \\\n",
       "0         words arranged across      arranged across different   \n",
       "1         displays rainbow hues        rainbow hues individual   \n",
       "2           one specific aspect       specific aspect language   \n",
       "3                          None                           None   \n",
       "4                          None                           None   \n",
       "5                          None                           None   \n",
       "6               use: life love;               life love; death   \n",
       "7                          None                           None   \n",
       "8         involved study, atlas      study, atlas demonstrates   \n",
       "9                          None                           None   \n",
       "10     information words person          words person hearing,   \n",
       "11           could allow people          allow people silenced   \n",
       "12      brain activity listened      activity listened stories   \n",
       "13             data show groups            show groups related   \n",
       "14       Hour short compelling.                           None   \n",
       "15      people scanned focusing         scanned focusing words   \n",
       "16                         None                           None   \n",
       "17       words- 3,000 different          3,000 different words   \n",
       "18      exercise regions brain.                           None   \n",
       "19             one tiny regions        tiny regions represents   \n",
       "20                         None                           None   \n",
       "21              head, one brain                one brain spots   \n",
       "22       tend several meanings.                           None   \n",
       "23            word â€œtopâ€, along             â€œtopâ€, along words   \n",
       "24                         None                           None   \n",
       "25                         None                           None   \n",
       "26         explore brain atlas.                           None   \n",
       "27    brains organised meanings       organised meanings words   \n",
       "28                         None                           None   \n",
       "29      study published Nature.                           None   \n",
       "30  cultures different semantic       different semantic brain   \n",
       "31    networks represent wildly     represent wildly different   \n",
       "32                         None                           None   \n",
       "33             new atlases show             atlases show brain   \n",
       "34     proved elusive, however.                           None   \n",
       "35              get told arenâ€™t              told arenâ€™t right   \n",
       "36                         None                           None   \n",
       "37       isolated word sentence          word sentence spoken,   \n",
       "38       precise semantic brain          semantic brain atlas.   \n",
       "39        person thinking brain       thinking brain activity.   \n",
       "40                         None                           None   \n",
       "41         activity used assess            used assess whether   \n",
       "42                         None                           None   \n",
       "43       Speech, Language Brain       Language Brain Cambridge   \n",
       "44        fine differences word     differences word meanings.   \n",
       "45                         None                           None   \n",
       "46          wood, things heavy,           things heavy, things   \n",
       "47          words used flexibly             used flexibly lost   \n",
       "48  learn semantics represented  semantics represented brain.â€   \n",
       "\n",
       "                             8                            9   \\\n",
       "0      across different regions     different regions organ.   \n",
       "1         hues individual words    individual words concepts   \n",
       "2   aspect language represented  language represented brain,   \n",
       "3                          None                         None   \n",
       "4                          None                         None   \n",
       "5                          None                         None   \n",
       "6            love; death taxes;         death taxes; clouds,   \n",
       "7                          None                         None   \n",
       "8     atlas demonstrates modern  demonstrates modern imaging   \n",
       "9                          None                         None   \n",
       "10     person hearing, reading,   hearing, reading, possibly   \n",
       "11        people silenced motor       silenced motor neurone   \n",
       "12        listened stories read             stories read The   \n",
       "13         groups related words      related words triggered   \n",
       "14                         None                         None   \n",
       "15      focusing words drifting          words drifting off.   \n",
       "16                         None                         None   \n",
       "17            different words -                  words - lay   \n",
       "18                         None                         None   \n",
       "19      regions represents word    represents word â€œvictimâ€.   \n",
       "20                         None                         None   \n",
       "21        brain spots activated       spots activated family   \n",
       "22                         None                         None   \n",
       "23         along words describe     words describe clothing.   \n",
       "24                         None                         None   \n",
       "25                         None                         None   \n",
       "26                         None                         None   \n",
       "27          meanings words way.                         None   \n",
       "28                         None                         None   \n",
       "29                         None                         None   \n",
       "30      semantic brain atlases.                         None   \n",
       "31   wildly different concepts,  different concepts, numbers   \n",
       "32                         None                         None   \n",
       "33             show brain holds      brain holds information   \n",
       "34                         None                         None   \n",
       "35             arenâ€™t right set          right set narrative   \n",
       "36                         None                         None   \n",
       "37   sentence spoken, Gallantâ€™s       spoken, Gallantâ€™s team   \n",
       "38                         None                         None   \n",
       "39                         None                         None   \n",
       "40                         None                         None   \n",
       "41     assess whether political   whether political messages   \n",
       "42                         None                         None   \n",
       "43   Brain Cambridge University    Cambridge University said   \n",
       "44          word meanings. Take          meanings. Take word   \n",
       "45                         None                         None   \n",
       "46           heavy, things four            things four legs,   \n",
       "47     flexibly lost analysis,â€        lost analysis,â€ said.   \n",
       "48                         None                         None   \n",
       "\n",
       "                  ...                                                11  \\\n",
       "0                 ...                                              None   \n",
       "1                 ...                           concepts convey grouped   \n",
       "2                 ...                            brain, case semantics,   \n",
       "3                 ...                                              None   \n",
       "4                 ...                                              None   \n",
       "5                 ...                                              None   \n",
       "6                 ...                              clouds, Florida bra.   \n",
       "7                 ...                                              None   \n",
       "8                 ...                       imaging transform knowledge   \n",
       "9                 ...                                              None   \n",
       "10                ...                          possibly even thinking,â€   \n",
       "11                ...                         neurone disease locked-in   \n",
       "12                ...                                    The Moth Radio   \n",
       "13                ...                        triggered neural responses   \n",
       "14                ...                                              None   \n",
       "15                ...                                              None   \n",
       "16                ...                                              None   \n",
       "17                ...                                              None   \n",
       "18                ...                                              None   \n",
       "19                ...                                              None   \n",
       "20                ...                                              None   \n",
       "21                ...                             family terms: â€œwifeâ€,   \n",
       "22                ...                                              None   \n",
       "23                ...                                              None   \n",
       "24                ...                                              None   \n",
       "25                ...                                              None   \n",
       "26                ...                                              None   \n",
       "27                ...                                              None   \n",
       "28                ...                                              None   \n",
       "29                ...                                              None   \n",
       "30                ...                                              None   \n",
       "31                ...                          numbers murder religion.   \n",
       "32                ...                                              None   \n",
       "33                ...                     information aspects language,   \n",
       "34                ...                                              None   \n",
       "35                ...                         narrative features,â€ said   \n",
       "36                ...                                              None   \n",
       "37                ...                                   team shed light   \n",
       "38                ...                                              None   \n",
       "39                ...                                              None   \n",
       "40                ...                                              None   \n",
       "41                ...                 messages effectively communicated   \n",
       "42                ...                                              None   \n",
       "43                ...                               said research â€œtour   \n",
       "44                ...                                              None   \n",
       "45                ...                                              None   \n",
       "46                ...                        legs, non-animate objects,   \n",
       "47                ...                                              None   \n",
       "48                ...                                              None   \n",
       "\n",
       "                                  12                              13  \\\n",
       "0                               None                            None   \n",
       "1            convey grouped together         grouped together clumps   \n",
       "2           case semantics, meanings     semantics, meanings words,â€   \n",
       "3                               None                            None   \n",
       "4                               None                            None   \n",
       "5                               None                            None   \n",
       "6                               None                            None   \n",
       "7                               None                            None   \n",
       "8          transform knowledge brain        knowledge brain performs   \n",
       "9                               None                            None   \n",
       "10              even thinking,â€ said       thinking,â€ said Alexander   \n",
       "11        disease locked-in syndrome        locked-in syndrome speak   \n",
       "12                  Moth Radio Hour,                  Radio Hour, US   \n",
       "13           neural responses 50,000         responses 50,000 80,000   \n",
       "14                              None                            None   \n",
       "15                              None                            None   \n",
       "16                              None                            None   \n",
       "17                              None                            None   \n",
       "18                              None                            None   \n",
       "19                              None                            None   \n",
       "20                              None                            None   \n",
       "21         terms: â€œwifeâ€, â€œhusbandâ€,  â€œwifeâ€, â€œhusbandâ€, â€œchildrenâ€,   \n",
       "22                              None                            None   \n",
       "23                              None                            None   \n",
       "24                              None                            None   \n",
       "25                              None                            None   \n",
       "26                              None                            None   \n",
       "27                              None                            None   \n",
       "28                              None                            None   \n",
       "29                              None                            None   \n",
       "30                              None                            None   \n",
       "31                              None                            None   \n",
       "32                              None                            None   \n",
       "33        aspects language, phonemes      language, phonemes syntax.   \n",
       "34                              None                            None   \n",
       "35          features,â€ said Gallant.                            None   \n",
       "36                              None                            None   \n",
       "37                  shed light brain              light brain worked   \n",
       "38                              None                            None   \n",
       "39                              None                            None   \n",
       "40                              None                            None   \n",
       "41  effectively communicated public.                            None   \n",
       "42                              None                            None   \n",
       "43                 research â€œtour de                  â€œtour de force   \n",
       "44                              None                            None   \n",
       "45                              None                            None   \n",
       "46          non-animate objects, on.                            None   \n",
       "47                              None                            None   \n",
       "48                              None                            None   \n",
       "\n",
       "                                   14                           15  \\\n",
       "0                                None                         None   \n",
       "1               together clumps white         clumps white matter.   \n",
       "2               meanings words,â€ said            words,â€ said Jack   \n",
       "3                                None                         None   \n",
       "4                                None                         None   \n",
       "5                                None                         None   \n",
       "6                                None                         None   \n",
       "7                                None                         None   \n",
       "8            brain performs important    performs important tasks.   \n",
       "9                                None                         None   \n",
       "10               said Alexander Huth,        Alexander Huth, first   \n",
       "11           syndrome speak computer.                         None   \n",
       "12                     Hour, US radio               US radio show.   \n",
       "13            50,000 80,000 pea-sized       80,000 pea-sized spots   \n",
       "14                               None                         None   \n",
       "15                               None                         None   \n",
       "16                               None                         None   \n",
       "17                               None                         None   \n",
       "18                               None                         None   \n",
       "19                               None                         None   \n",
       "20                               None                         None   \n",
       "21  â€œhusbandâ€, â€œchildrenâ€, â€œparentsâ€.                         None   \n",
       "22                               None                         None   \n",
       "23                               None                         None   \n",
       "24                               None                         None   \n",
       "25                               None                         None   \n",
       "26                               None                         None   \n",
       "27                               None                         None   \n",
       "28                               None                         None   \n",
       "29                               None                         None   \n",
       "30                               None                         None   \n",
       "31                               None                         None   \n",
       "32                               None                         None   \n",
       "33                               None                         None   \n",
       "34                               None                         None   \n",
       "35                               None                         None   \n",
       "36                               None                         None   \n",
       "37            brain worked real-world  worked real-world scenario,   \n",
       "38                               None                         None   \n",
       "39                               None                         None   \n",
       "40                               None                         None   \n",
       "41                               None                         None   \n",
       "42                               None                         None   \n",
       "43                     de force scope        force scope methodsâ€.   \n",
       "44                               None                         None   \n",
       "45                               None                         None   \n",
       "46                               None                         None   \n",
       "47                               None                         None   \n",
       "48                               None                         None   \n",
       "\n",
       "                            16                            17  \\\n",
       "0                         None                          None   \n",
       "1                         None                          None   \n",
       "2           said Jack Gallant,  Jack Gallant, neuroscientist   \n",
       "3                         None                          None   \n",
       "4                         None                          None   \n",
       "5                         None                          None   \n",
       "6                         None                          None   \n",
       "7                         None                          None   \n",
       "8                         None                          None   \n",
       "9                         None                          None   \n",
       "10          Huth, first author           first author study.   \n",
       "11                        None                          None   \n",
       "12                        None                          None   \n",
       "13    pea-sized spots cerebral        spots cerebral cortex.   \n",
       "14                        None                          None   \n",
       "15                        None                          None   \n",
       "16                        None                          None   \n",
       "17                        None                          None   \n",
       "18                        None                          None   \n",
       "19                        None                          None   \n",
       "20                        None                          None   \n",
       "21                        None                          None   \n",
       "22                        None                          None   \n",
       "23                        None                          None   \n",
       "24                        None                          None   \n",
       "25                        None                          None   \n",
       "26                        None                          None   \n",
       "27                        None                          None   \n",
       "28                        None                          None   \n",
       "29                        None                          None   \n",
       "30                        None                          None   \n",
       "31                        None                          None   \n",
       "32                        None                          None   \n",
       "33                        None                          None   \n",
       "34                        None                          None   \n",
       "35                        None                          None   \n",
       "36                        None                          None   \n",
       "37  real-world scenario, said.                          None   \n",
       "38                        None                          None   \n",
       "39                        None                          None   \n",
       "40                        None                          None   \n",
       "41                        None                          None   \n",
       "42                        None                          None   \n",
       "43                        None                          None   \n",
       "44                        None                          None   \n",
       "45                        None                          None   \n",
       "46                        None                          None   \n",
       "47                        None                          None   \n",
       "48                        None                          None   \n",
       "\n",
       "                                    18                                     19  \\\n",
       "0                                 None                                   None   \n",
       "1                                 None                                   None   \n",
       "2   Gallant, neuroscientist University  neuroscientist University California,   \n",
       "3                                 None                                   None   \n",
       "4                                 None                                   None   \n",
       "5                                 None                                   None   \n",
       "6                                 None                                   None   \n",
       "7                                 None                                   None   \n",
       "8                                 None                                   None   \n",
       "9                                 None                                   None   \n",
       "10                                None                                   None   \n",
       "11                                None                                   None   \n",
       "12                                None                                   None   \n",
       "13                                None                                   None   \n",
       "14                                None                                   None   \n",
       "15                                None                                   None   \n",
       "16                                None                                   None   \n",
       "17                                None                                   None   \n",
       "18                                None                                   None   \n",
       "19                                None                                   None   \n",
       "20                                None                                   None   \n",
       "21                                None                                   None   \n",
       "22                                None                                   None   \n",
       "23                                None                                   None   \n",
       "24                                None                                   None   \n",
       "25                                None                                   None   \n",
       "26                                None                                   None   \n",
       "27                                None                                   None   \n",
       "28                                None                                   None   \n",
       "29                                None                                   None   \n",
       "30                                None                                   None   \n",
       "31                                None                                   None   \n",
       "32                                None                                   None   \n",
       "33                                None                                   None   \n",
       "34                                None                                   None   \n",
       "35                                None                                   None   \n",
       "36                                None                                   None   \n",
       "37                                None                                   None   \n",
       "38                                None                                   None   \n",
       "39                                None                                   None   \n",
       "40                                None                                   None   \n",
       "41                                None                                   None   \n",
       "42                                None                                   None   \n",
       "43                                None                                   None   \n",
       "44                                None                                   None   \n",
       "45                                None                                   None   \n",
       "46                                None                                   None   \n",
       "47                                None                                   None   \n",
       "48                                None                                   None   \n",
       "\n",
       "                                  20  \n",
       "0                               None  \n",
       "1                               None  \n",
       "2   University California, Berkeley.  \n",
       "3                               None  \n",
       "4                               None  \n",
       "5                               None  \n",
       "6                               None  \n",
       "7                               None  \n",
       "8                               None  \n",
       "9                               None  \n",
       "10                              None  \n",
       "11                              None  \n",
       "12                              None  \n",
       "13                              None  \n",
       "14                              None  \n",
       "15                              None  \n",
       "16                              None  \n",
       "17                              None  \n",
       "18                              None  \n",
       "19                              None  \n",
       "20                              None  \n",
       "21                              None  \n",
       "22                              None  \n",
       "23                              None  \n",
       "24                              None  \n",
       "25                              None  \n",
       "26                              None  \n",
       "27                              None  \n",
       "28                              None  \n",
       "29                              None  \n",
       "30                              None  \n",
       "31                              None  \n",
       "32                              None  \n",
       "33                              None  \n",
       "34                              None  \n",
       "35                              None  \n",
       "36                              None  \n",
       "37                              None  \n",
       "38                              None  \n",
       "39                              None  \n",
       "40                              None  \n",
       "41                              None  \n",
       "42                              None  \n",
       "43                              None  \n",
       "44                              None  \n",
       "45                              None  \n",
       "46                              None  \n",
       "47                              None  \n",
       "48                              None  \n",
       "\n",
       "[49 rows x 21 columns]"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"sciencearticle_line.txt\", \"r\") as testset:\n",
    "    filtered_words = []\n",
    "    testmeta = []\n",
    "    testmeta1 = []\n",
    "    word_list = ''\n",
    "    for line in testset:\n",
    "        word_list = line.split()\n",
    "        filtered_words = [word for word in word_list if word not in stopwords.words('english')]\n",
    "        testmeta1.append(' '.join(x) for x in trigrams(filtered_words))\n",
    "df_test = pd.DataFrame(testmeta1)\n",
    "# df_test = df_test.ix[:, 0:6]\n",
    "# df_test.columns = ['sample0', 'sample1', 'sample2', 'sample3', 'sample4', 'sample5', 'sample6']\n",
    "# df_test = df_test.fillna(\"\")\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample0</th>\n",
       "      <th>sample1</th>\n",
       "      <th>sample2</th>\n",
       "      <th>sample3</th>\n",
       "      <th>sample4</th>\n",
       "      <th>sample5</th>\n",
       "      <th>sample6</th>\n",
       "      <th>metaphor0</th>\n",
       "      <th>metaphor1</th>\n",
       "      <th>metaphor2</th>\n",
       "      <th>metaphor3</th>\n",
       "      <th>metaphor4</th>\n",
       "      <th>metaphor5</th>\n",
       "      <th>metaphor6</th>\n",
       "      <th>sum_metaphor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Scientists created â€œatlas</td>\n",
       "      <td>created â€œatlas brainâ€</td>\n",
       "      <td>â€œatlas brainâ€ reveals</td>\n",
       "      <td>brainâ€ reveals meanings</td>\n",
       "      <td>reveals meanings words</td>\n",
       "      <td>meanings words arranged</td>\n",
       "      <td>words arranged across</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Like colourful quilt</td>\n",
       "      <td>colourful quilt laid</td>\n",
       "      <td>quilt laid cortex,</td>\n",
       "      <td>laid cortex, atlas</td>\n",
       "      <td>cortex, atlas displays</td>\n",
       "      <td>atlas displays rainbow</td>\n",
       "      <td>displays rainbow hues</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>â€œOur goal build</td>\n",
       "      <td>goal build giant</td>\n",
       "      <td>build giant atlas</td>\n",
       "      <td>giant atlas shows</td>\n",
       "      <td>atlas shows one</td>\n",
       "      <td>shows one specific</td>\n",
       "      <td>one specific aspect</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No single brain</td>\n",
       "      <td>single brain region</td>\n",
       "      <td>brain region holds</td>\n",
       "      <td>region holds one</td>\n",
       "      <td>holds one word</td>\n",
       "      <td>one word concept.</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A single brain</td>\n",
       "      <td>single brain spot</td>\n",
       "      <td>brain spot associated</td>\n",
       "      <td>spot associated number</td>\n",
       "      <td>associated number related</td>\n",
       "      <td>number related words.</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>And single word</td>\n",
       "      <td>single word lights</td>\n",
       "      <td>word lights many</td>\n",
       "      <td>lights many different</td>\n",
       "      <td>many different brain</td>\n",
       "      <td>different brain spots.</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Together make networks</td>\n",
       "      <td>make networks represent</td>\n",
       "      <td>networks represent meanings</td>\n",
       "      <td>represent meanings word</td>\n",
       "      <td>meanings word use:</td>\n",
       "      <td>word use: life</td>\n",
       "      <td>use: life love;</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>All light networks.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Described â€œtour de</td>\n",
       "      <td>â€œtour de forceâ€</td>\n",
       "      <td>de forceâ€ one</td>\n",
       "      <td>forceâ€ one researcher</td>\n",
       "      <td>one researcher involved</td>\n",
       "      <td>researcher involved study,</td>\n",
       "      <td>involved study, atlas</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>With advances, technology</td>\n",
       "      <td>advances, technology could</td>\n",
       "      <td>technology could profound</td>\n",
       "      <td>could profound impact</td>\n",
       "      <td>profound impact medicine</td>\n",
       "      <td>impact medicine fields.</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>â€œIt possible approach</td>\n",
       "      <td>possible approach could</td>\n",
       "      <td>approach could used</td>\n",
       "      <td>could used decode</td>\n",
       "      <td>used decode information</td>\n",
       "      <td>decode information words</td>\n",
       "      <td>information words person</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>One potential use</td>\n",
       "      <td>potential use would</td>\n",
       "      <td>use would language</td>\n",
       "      <td>would language decoder</td>\n",
       "      <td>language decoder could</td>\n",
       "      <td>decoder could allow</td>\n",
       "      <td>could allow people</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>To create atlas,</td>\n",
       "      <td>create atlas, scientists</td>\n",
       "      <td>atlas, scientists recorded</td>\n",
       "      <td>scientists recorded peopleâ€™s</td>\n",
       "      <td>recorded peopleâ€™s brain</td>\n",
       "      <td>peopleâ€™s brain activity</td>\n",
       "      <td>brain activity listened</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>They matched transcripts</td>\n",
       "      <td>matched transcripts stories</td>\n",
       "      <td>transcripts stories brain</td>\n",
       "      <td>stories brain activity</td>\n",
       "      <td>brain activity data</td>\n",
       "      <td>activity data show</td>\n",
       "      <td>data show groups</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Huth used stories</td>\n",
       "      <td>used stories The</td>\n",
       "      <td>stories The Moth</td>\n",
       "      <td>The Moth Radio</td>\n",
       "      <td>Moth Radio Hour</td>\n",
       "      <td>Radio Hour short</td>\n",
       "      <td>Hour short compelling.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>The enthralling stories,</td>\n",
       "      <td>enthralling stories, confident</td>\n",
       "      <td>stories, confident scientists</td>\n",
       "      <td>confident scientists could</td>\n",
       "      <td>scientists could people</td>\n",
       "      <td>could people scanned</td>\n",
       "      <td>people scanned focusing</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Seven people listened</td>\n",
       "      <td>people listened two</td>\n",
       "      <td>listened two hours</td>\n",
       "      <td>two hours stories</td>\n",
       "      <td>hours stories each.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Per person, amounted</td>\n",
       "      <td>person, amounted hearing</td>\n",
       "      <td>amounted hearing roughly</td>\n",
       "      <td>hearing roughly 25,000</td>\n",
       "      <td>roughly 25,000 words-</td>\n",
       "      <td>25,000 words- 3,000</td>\n",
       "      <td>words- 3,000 different</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>The atlas shows</td>\n",
       "      <td>atlas shows words</td>\n",
       "      <td>shows words related</td>\n",
       "      <td>words related terms</td>\n",
       "      <td>related terms exercise</td>\n",
       "      <td>terms exercise regions</td>\n",
       "      <td>exercise regions brain.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>For example, left-hand</td>\n",
       "      <td>example, left-hand side</td>\n",
       "      <td>left-hand side brain,</td>\n",
       "      <td>side brain, ear,</td>\n",
       "      <td>brain, ear, one</td>\n",
       "      <td>ear, one tiny</td>\n",
       "      <td>one tiny regions</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>The region responds</td>\n",
       "      <td>region responds â€œkilledâ€,</td>\n",
       "      <td>responds â€œkilledâ€, â€œconvictedâ€,</td>\n",
       "      <td>â€œkilledâ€, â€œconvictedâ€, â€œmurderedâ€</td>\n",
       "      <td>â€œconvictedâ€, â€œmurderedâ€ â€œconfessedâ€.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>On brainâ€™s right-hand</td>\n",
       "      <td>brainâ€™s right-hand side,</td>\n",
       "      <td>right-hand side, near</td>\n",
       "      <td>side, near top</td>\n",
       "      <td>near top head,</td>\n",
       "      <td>top head, one</td>\n",
       "      <td>head, one brain</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Each word represented</td>\n",
       "      <td>word represented one</td>\n",
       "      <td>represented one spot</td>\n",
       "      <td>one spot words</td>\n",
       "      <td>spot words tend</td>\n",
       "      <td>words tend several</td>\n",
       "      <td>tend several meanings.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>One part brain,</td>\n",
       "      <td>part brain, example,</td>\n",
       "      <td>brain, example, reliably</td>\n",
       "      <td>example, reliably responds</td>\n",
       "      <td>reliably responds word</td>\n",
       "      <td>responds word â€œtopâ€,</td>\n",
       "      <td>word â€œtopâ€, along</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>But word â€œtopâ€</td>\n",
       "      <td>word â€œtopâ€ activates</td>\n",
       "      <td>â€œtopâ€ activates many</td>\n",
       "      <td>activates many regions.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>One responds numbers</td>\n",
       "      <td>responds numbers measurements,</td>\n",
       "      <td>numbers measurements, another</td>\n",
       "      <td>measurements, another buildings</td>\n",
       "      <td>another buildings places.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>The scientists created</td>\n",
       "      <td>scientists created interactive</td>\n",
       "      <td>created interactive website</td>\n",
       "      <td>interactive website public</td>\n",
       "      <td>website public explore</td>\n",
       "      <td>public explore brain</td>\n",
       "      <td>explore brain atlas.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Strikingly, brain atlases</td>\n",
       "      <td>brain atlases similar</td>\n",
       "      <td>atlases similar participants,</td>\n",
       "      <td>similar participants, suggesting</td>\n",
       "      <td>participants, suggesting brains</td>\n",
       "      <td>suggesting brains organised</td>\n",
       "      <td>brains organised meanings</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>The scientists scanned</td>\n",
       "      <td>scientists scanned five</td>\n",
       "      <td>scanned five men</td>\n",
       "      <td>five men two</td>\n",
       "      <td>men two women,</td>\n",
       "      <td>two women, however.</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>All native English</td>\n",
       "      <td>native English speakers,</td>\n",
       "      <td>English speakers, two</td>\n",
       "      <td>speakers, two authors</td>\n",
       "      <td>two authors study</td>\n",
       "      <td>authors study published</td>\n",
       "      <td>study published Nature.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>It highly possible</td>\n",
       "      <td>highly possible people</td>\n",
       "      <td>possible people different</td>\n",
       "      <td>people different backgrounds</td>\n",
       "      <td>different backgrounds cultures</td>\n",
       "      <td>backgrounds cultures different</td>\n",
       "      <td>cultures different semantic</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Armed atlas, researchers</td>\n",
       "      <td>atlas, researchers piece</td>\n",
       "      <td>researchers piece together</td>\n",
       "      <td>piece together brain</td>\n",
       "      <td>together brain networks</td>\n",
       "      <td>brain networks represent</td>\n",
       "      <td>networks represent wildly</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>â€œThe idea murder</td>\n",
       "      <td>idea murder represented</td>\n",
       "      <td>murder represented lot</td>\n",
       "      <td>represented lot brain,â€</td>\n",
       "      <td>lot brain,â€ Gallant</td>\n",
       "      <td>brain,â€ Gallant said.</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Using haul data,</td>\n",
       "      <td>haul data, group</td>\n",
       "      <td>data, group begun</td>\n",
       "      <td>group begun work</td>\n",
       "      <td>begun work new</td>\n",
       "      <td>work new atlases</td>\n",
       "      <td>new atlases show</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>A brain atlas</td>\n",
       "      <td>brain atlas narrative</td>\n",
       "      <td>atlas narrative structure</td>\n",
       "      <td>narrative structure far</td>\n",
       "      <td>structure far proved</td>\n",
       "      <td>far proved elusive,</td>\n",
       "      <td>proved elusive, however.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>â€œEvery time come</td>\n",
       "      <td>time come set</td>\n",
       "      <td>come set narrative</td>\n",
       "      <td>set narrative features,</td>\n",
       "      <td>narrative features, get</td>\n",
       "      <td>features, get told</td>\n",
       "      <td>get told arenâ€™t</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Uri Hasson, neuroscientist</td>\n",
       "      <td>Hasson, neuroscientist Princeton</td>\n",
       "      <td>neuroscientist Princeton University,</td>\n",
       "      <td>Princeton University, praised</td>\n",
       "      <td>University, praised work.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Unlike many studies</td>\n",
       "      <td>many studies looked</td>\n",
       "      <td>studies looked brain</td>\n",
       "      <td>looked brain activity</td>\n",
       "      <td>brain activity isolated</td>\n",
       "      <td>activity isolated word</td>\n",
       "      <td>isolated word sentence</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>The next step,</td>\n",
       "      <td>next step, said,</td>\n",
       "      <td>step, said, create</td>\n",
       "      <td>said, create comprehensive</td>\n",
       "      <td>create comprehensive precise</td>\n",
       "      <td>comprehensive precise semantic</td>\n",
       "      <td>precise semantic brain</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Ultimately, Hasson believes</td>\n",
       "      <td>Hasson believes possible</td>\n",
       "      <td>believes possible reconstruct</td>\n",
       "      <td>possible reconstruct words</td>\n",
       "      <td>reconstruct words person</td>\n",
       "      <td>words person thinking</td>\n",
       "      <td>person thinking brain</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>The ethical implications</td>\n",
       "      <td>ethical implications enormous.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>One benign use</td>\n",
       "      <td>benign use would</td>\n",
       "      <td>use would see</td>\n",
       "      <td>would see brain</td>\n",
       "      <td>see brain activity</td>\n",
       "      <td>brain activity used</td>\n",
       "      <td>activity used assess</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>â€œThere many implications,</td>\n",
       "      <td>many implications, barely</td>\n",
       "      <td>implications, barely touching</td>\n",
       "      <td>barely touching surface,â€</td>\n",
       "      <td>touching surface,â€ said.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Lorraine Tyler, cognitive</td>\n",
       "      <td>Tyler, cognitive neuroscientist</td>\n",
       "      <td>cognitive neuroscientist head</td>\n",
       "      <td>neuroscientist head Centre</td>\n",
       "      <td>head Centre Speech,</td>\n",
       "      <td>Centre Speech, Language</td>\n",
       "      <td>Speech, Language Brain</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>But brain atlas</td>\n",
       "      <td>brain atlas current</td>\n",
       "      <td>atlas current form</td>\n",
       "      <td>current form capture</td>\n",
       "      <td>form capture fine</td>\n",
       "      <td>capture fine differences</td>\n",
       "      <td>fine differences word</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>It member many</td>\n",
       "      <td>member many different</td>\n",
       "      <td>many different groups,</td>\n",
       "      <td>different groups, says</td>\n",
       "      <td>groups, says Tyler.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>â€œIt something eat</td>\n",
       "      <td>something eat off,</td>\n",
       "      <td>eat off, things</td>\n",
       "      <td>off, things made</td>\n",
       "      <td>things made wood,</td>\n",
       "      <td>made wood, things</td>\n",
       "      <td>wood, things heavy,</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>This kind detailed</td>\n",
       "      <td>kind detailed semantic</td>\n",
       "      <td>detailed semantic information</td>\n",
       "      <td>semantic information enables</td>\n",
       "      <td>information enables words</td>\n",
       "      <td>enables words used</td>\n",
       "      <td>words used flexibly</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>â€œWhile research path-breaking</td>\n",
       "      <td>research path-breaking scope,</td>\n",
       "      <td>path-breaking scope, still</td>\n",
       "      <td>scope, still lot</td>\n",
       "      <td>still lot learn</td>\n",
       "      <td>lot learn semantics</td>\n",
       "      <td>learn semantics represented</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          sample0                           sample1  \\\n",
       "0       Scientists created â€œatlas             created â€œatlas brainâ€   \n",
       "1            Like colourful quilt              colourful quilt laid   \n",
       "2                 â€œOur goal build                  goal build giant   \n",
       "3                 No single brain               single brain region   \n",
       "4                  A single brain                 single brain spot   \n",
       "5                 And single word                single word lights   \n",
       "6          Together make networks           make networks represent   \n",
       "7             All light networks.                                     \n",
       "8              Described â€œtour de                   â€œtour de forceâ€   \n",
       "9       With advances, technology        advances, technology could   \n",
       "10          â€œIt possible approach           possible approach could   \n",
       "11              One potential use               potential use would   \n",
       "12               To create atlas,          create atlas, scientists   \n",
       "13       They matched transcripts       matched transcripts stories   \n",
       "14              Huth used stories                  used stories The   \n",
       "15       The enthralling stories,    enthralling stories, confident   \n",
       "16          Seven people listened               people listened two   \n",
       "17           Per person, amounted          person, amounted hearing   \n",
       "18                The atlas shows                 atlas shows words   \n",
       "19         For example, left-hand           example, left-hand side   \n",
       "20            The region responds         region responds â€œkilledâ€,   \n",
       "21          On brainâ€™s right-hand          brainâ€™s right-hand side,   \n",
       "22          Each word represented              word represented one   \n",
       "23                One part brain,              part brain, example,   \n",
       "24                 But word â€œtopâ€              word â€œtopâ€ activates   \n",
       "25           One responds numbers    responds numbers measurements,   \n",
       "26         The scientists created    scientists created interactive   \n",
       "27      Strikingly, brain atlases             brain atlases similar   \n",
       "28         The scientists scanned           scientists scanned five   \n",
       "29             All native English          native English speakers,   \n",
       "30             It highly possible            highly possible people   \n",
       "31       Armed atlas, researchers          atlas, researchers piece   \n",
       "32               â€œThe idea murder           idea murder represented   \n",
       "33               Using haul data,                  haul data, group   \n",
       "34                  A brain atlas             brain atlas narrative   \n",
       "35               â€œEvery time come                     time come set   \n",
       "36     Uri Hasson, neuroscientist  Hasson, neuroscientist Princeton   \n",
       "37            Unlike many studies               many studies looked   \n",
       "38                 The next step,                  next step, said,   \n",
       "39    Ultimately, Hasson believes          Hasson believes possible   \n",
       "40       The ethical implications    ethical implications enormous.   \n",
       "41                 One benign use                  benign use would   \n",
       "42      â€œThere many implications,         many implications, barely   \n",
       "43      Lorraine Tyler, cognitive   Tyler, cognitive neuroscientist   \n",
       "44                But brain atlas               brain atlas current   \n",
       "45                 It member many             member many different   \n",
       "46              â€œIt something eat                something eat off,   \n",
       "47             This kind detailed            kind detailed semantic   \n",
       "48  â€œWhile research path-breaking     research path-breaking scope,   \n",
       "\n",
       "                                 sample2                            sample3  \\\n",
       "0                  â€œatlas brainâ€ reveals            brainâ€ reveals meanings   \n",
       "1                     quilt laid cortex,                 laid cortex, atlas   \n",
       "2                      build giant atlas                  giant atlas shows   \n",
       "3                     brain region holds                   region holds one   \n",
       "4                  brain spot associated             spot associated number   \n",
       "5                       word lights many              lights many different   \n",
       "6            networks represent meanings            represent meanings word   \n",
       "7                                                                             \n",
       "8                          de forceâ€ one              forceâ€ one researcher   \n",
       "9              technology could profound              could profound impact   \n",
       "10                   approach could used                  could used decode   \n",
       "11                    use would language             would language decoder   \n",
       "12            atlas, scientists recorded       scientists recorded peopleâ€™s   \n",
       "13             transcripts stories brain             stories brain activity   \n",
       "14                      stories The Moth                     The Moth Radio   \n",
       "15         stories, confident scientists         confident scientists could   \n",
       "16                    listened two hours                  two hours stories   \n",
       "17              amounted hearing roughly             hearing roughly 25,000   \n",
       "18                   shows words related                words related terms   \n",
       "19                 left-hand side brain,                   side brain, ear,   \n",
       "20       responds â€œkilledâ€, â€œconvictedâ€,  â€œkilledâ€, â€œconvictedâ€, â€œmurderedâ€   \n",
       "21                 right-hand side, near                     side, near top   \n",
       "22                  represented one spot                     one spot words   \n",
       "23              brain, example, reliably         example, reliably responds   \n",
       "24                  â€œtopâ€ activates many            activates many regions.   \n",
       "25         numbers measurements, another    measurements, another buildings   \n",
       "26           created interactive website         interactive website public   \n",
       "27         atlases similar participants,   similar participants, suggesting   \n",
       "28                      scanned five men                       five men two   \n",
       "29                 English speakers, two              speakers, two authors   \n",
       "30             possible people different       people different backgrounds   \n",
       "31            researchers piece together               piece together brain   \n",
       "32                murder represented lot            represented lot brain,â€   \n",
       "33                     data, group begun                   group begun work   \n",
       "34             atlas narrative structure            narrative structure far   \n",
       "35                    come set narrative            set narrative features,   \n",
       "36  neuroscientist Princeton University,      Princeton University, praised   \n",
       "37                  studies looked brain              looked brain activity   \n",
       "38                    step, said, create         said, create comprehensive   \n",
       "39         believes possible reconstruct         possible reconstruct words   \n",
       "40                                                                            \n",
       "41                         use would see                    would see brain   \n",
       "42         implications, barely touching          barely touching surface,â€   \n",
       "43         cognitive neuroscientist head         neuroscientist head Centre   \n",
       "44                    atlas current form               current form capture   \n",
       "45                many different groups,             different groups, says   \n",
       "46                       eat off, things                   off, things made   \n",
       "47         detailed semantic information       semantic information enables   \n",
       "48            path-breaking scope, still                   scope, still lot   \n",
       "\n",
       "                                 sample4                         sample5  \\\n",
       "0                 reveals meanings words         meanings words arranged   \n",
       "1                 cortex, atlas displays          atlas displays rainbow   \n",
       "2                        atlas shows one              shows one specific   \n",
       "3                         holds one word               one word concept.   \n",
       "4              associated number related           number related words.   \n",
       "5                   many different brain          different brain spots.   \n",
       "6                     meanings word use:                  word use: life   \n",
       "7                                                                          \n",
       "8                one researcher involved      researcher involved study,   \n",
       "9               profound impact medicine         impact medicine fields.   \n",
       "10               used decode information        decode information words   \n",
       "11                language decoder could             decoder could allow   \n",
       "12               recorded peopleâ€™s brain         peopleâ€™s brain activity   \n",
       "13                   brain activity data              activity data show   \n",
       "14                       Moth Radio Hour                Radio Hour short   \n",
       "15               scientists could people            could people scanned   \n",
       "16                   hours stories each.                                   \n",
       "17                 roughly 25,000 words-             25,000 words- 3,000   \n",
       "18                related terms exercise          terms exercise regions   \n",
       "19                       brain, ear, one                   ear, one tiny   \n",
       "20  â€œconvictedâ€, â€œmurderedâ€ â€œconfessedâ€.                                   \n",
       "21                        near top head,                   top head, one   \n",
       "22                       spot words tend              words tend several   \n",
       "23                reliably responds word            responds word â€œtopâ€,   \n",
       "24                                                                         \n",
       "25             another buildings places.                                   \n",
       "26                website public explore            public explore brain   \n",
       "27       participants, suggesting brains     suggesting brains organised   \n",
       "28                        men two women,             two women, however.   \n",
       "29                     two authors study         authors study published   \n",
       "30        different backgrounds cultures  backgrounds cultures different   \n",
       "31               together brain networks        brain networks represent   \n",
       "32                   lot brain,â€ Gallant           brain,â€ Gallant said.   \n",
       "33                        begun work new                work new atlases   \n",
       "34                  structure far proved             far proved elusive,   \n",
       "35               narrative features, get              features, get told   \n",
       "36             University, praised work.                                   \n",
       "37               brain activity isolated          activity isolated word   \n",
       "38          create comprehensive precise  comprehensive precise semantic   \n",
       "39              reconstruct words person           words person thinking   \n",
       "40                                                                         \n",
       "41                    see brain activity             brain activity used   \n",
       "42              touching surface,â€ said.                                   \n",
       "43                   head Centre Speech,         Centre Speech, Language   \n",
       "44                     form capture fine        capture fine differences   \n",
       "45                   groups, says Tyler.                                   \n",
       "46                     things made wood,               made wood, things   \n",
       "47             information enables words              enables words used   \n",
       "48                       still lot learn             lot learn semantics   \n",
       "\n",
       "                        sample6  metaphor0  metaphor1  metaphor2  metaphor3  \\\n",
       "0         words arranged across          1          1          1          1   \n",
       "1         displays rainbow hues          1          1          1          1   \n",
       "2           one specific aspect          1          1          1          1   \n",
       "3                                        1          1          1          1   \n",
       "4                                        1          1          1          1   \n",
       "5                                        1          1          1          1   \n",
       "6               use: life love;          1          1          1          1   \n",
       "7                                        1          1          1          1   \n",
       "8         involved study, atlas          1          1          1          1   \n",
       "9                                        1          1          1          1   \n",
       "10     information words person          1          1          1          1   \n",
       "11           could allow people          1          1          1          1   \n",
       "12      brain activity listened          1          1          1          1   \n",
       "13             data show groups          1          1          1          1   \n",
       "14       Hour short compelling.          1          1          1          1   \n",
       "15      people scanned focusing          1          1          1          1   \n",
       "16                                       1          1          1          1   \n",
       "17       words- 3,000 different          1          1          1          1   \n",
       "18      exercise regions brain.          1          1          1          1   \n",
       "19             one tiny regions          1          1          1          1   \n",
       "20                                       1          1          1          1   \n",
       "21              head, one brain          1          1          1          1   \n",
       "22       tend several meanings.          1          1          1          1   \n",
       "23            word â€œtopâ€, along          1          1          1          1   \n",
       "24                                       1          1          1          1   \n",
       "25                                       1          1          1          1   \n",
       "26         explore brain atlas.          1          1          1          1   \n",
       "27    brains organised meanings          1          1          1          1   \n",
       "28                                       1          1          1          1   \n",
       "29      study published Nature.          1          1          1          1   \n",
       "30  cultures different semantic          1          1          1          1   \n",
       "31    networks represent wildly          1          1          1          1   \n",
       "32                                       1          1          1          1   \n",
       "33             new atlases show          1          1          1          1   \n",
       "34     proved elusive, however.          1          1          1          1   \n",
       "35              get told arenâ€™t          1          1          1          1   \n",
       "36                                       1          1          1          1   \n",
       "37       isolated word sentence          1          1          1          1   \n",
       "38       precise semantic brain          1          1          1          1   \n",
       "39        person thinking brain          1          1          1          1   \n",
       "40                                       1          1          1          1   \n",
       "41         activity used assess          1          1          1          1   \n",
       "42                                       1          1          1          1   \n",
       "43       Speech, Language Brain          1          1          1          1   \n",
       "44        fine differences word          1          1          1          1   \n",
       "45                                       1          1          1          1   \n",
       "46          wood, things heavy,          1          1          1          1   \n",
       "47          words used flexibly          1          1          1          1   \n",
       "48  learn semantics represented          1          1          1          1   \n",
       "\n",
       "    metaphor4  metaphor5  metaphor6  sum_metaphor  \n",
       "0           1          1          1             1  \n",
       "1           1          1          1             1  \n",
       "2           1          1          1             1  \n",
       "3           1          1          1             1  \n",
       "4           1          1          1             1  \n",
       "5           1          1          1             1  \n",
       "6           1          1          1             1  \n",
       "7           1          1          1             1  \n",
       "8           1          1          1             1  \n",
       "9           1          1          1             1  \n",
       "10          1          1          1             1  \n",
       "11          1          1          1             1  \n",
       "12          1          1          1             1  \n",
       "13          1          1          1             1  \n",
       "14          1          1          1             1  \n",
       "15          1          1          1             1  \n",
       "16          1          1          1             1  \n",
       "17          1          1          1             1  \n",
       "18          1          1          1             1  \n",
       "19          1          1          1             1  \n",
       "20          1          1          1             1  \n",
       "21          1          1          1             1  \n",
       "22          1          1          1             1  \n",
       "23          1          1          1             1  \n",
       "24          1          1          1             1  \n",
       "25          1          1          1             1  \n",
       "26          1          1          1             1  \n",
       "27          1          1          1             1  \n",
       "28          1          1          1             1  \n",
       "29          1          1          1             1  \n",
       "30          1          1          1             1  \n",
       "31          1          1          1             1  \n",
       "32          1          1          1             1  \n",
       "33          1          1          1             1  \n",
       "34          1          1          1             1  \n",
       "35          1          1          1             1  \n",
       "36          1          1          1             1  \n",
       "37          1          1          1             1  \n",
       "38          1          1          1             1  \n",
       "39          1          1          1             1  \n",
       "40          1          1          1             1  \n",
       "41          1          1          1             1  \n",
       "42          1          1          1             1  \n",
       "43          1          1          1             1  \n",
       "44          1          1          1             1  \n",
       "45          1          1          1             1  \n",
       "46          1          1          1             1  \n",
       "47          1          1          1             1  \n",
       "48          1          1          1             1  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_test_feature_sparse = vec.transform(df_test['sample0'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "df_test['metaphor0'] = logreg_predictions\n",
    "\n",
    "arr_test_feature_sparse = vec.transform(df_test['sample1'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "df_test['metaphor1'] = logreg_predictions\n",
    "\n",
    "arr_test_feature_sparse = vec.transform(df_test['sample2'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "df_test['metaphor2'] = logreg_predictions\n",
    "\n",
    "arr_test_feature_sparse = vec.transform(df_test['sample3'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "df_test['metaphor3'] = logreg_predictions\n",
    "\n",
    "arr_test_feature_sparse = vec.transform(df_test['sample4'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "df_test['metaphor4'] = logreg_predictions\n",
    "\n",
    "arr_test_feature_sparse = vec.transform(df_test['sample5'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "df_test['metaphor5'] = logreg_predictions\n",
    "\n",
    "arr_test_feature_sparse = vec.transform(df_test['sample6'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "df_test['metaphor6'] = logreg_predictions\n",
    "\n",
    "df_test['sum_metaphor'] = df_test['metaphor1'] + df_test['metaphor2'] + df_test['metaphor3'] +df_test['metaphor4'] +df_test['metaphor5'] \n",
    "\n",
    "if sum(df_test['sum_metaphor']) >= 1:\n",
    "    df_test['sum_metaphor'] = 1\n",
    "sum(df_test['sum_metaphor'])\n",
    "#len(df_test['sum_metaphor'])\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"sciencearticle_line.txt\", \"r\") as testset:\n",
    "    filtered_words = []\n",
    "    testmeta = []\n",
    "    testmeta1 = []\n",
    "    word_list = ''\n",
    "    for line in testset:\n",
    "        word_list = line.split()\n",
    "        filtered_words = ' '.join([word.lower() for word in word_list if word not in stopwords.words('english')])\n",
    "        testmeta1.append(filtered_words)\n",
    "df_test = pd.DataFrame(testmeta1)\n",
    "df_test\n",
    "# df_test = df_test.ix[:, 0:6]\n",
    "df_test.columns = ['sample0']\n",
    "df_test = df_test.fillna(\"\")\n",
    "# df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample0</th>\n",
       "      <th>metaphor0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>scientists created â€œatlas brainâ€ reveals meani...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>like colourful quilt laid cortex, atlas displa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>â€œour goal build giant atlas shows one specific...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no single brain region holds one word concept.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a single brain spot associated number related ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>and single word lights many different brain sp...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>together make networks represent meanings word...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>all light networks.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>described â€œtour de forceâ€ one researcher invol...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>with advances, technology could profound impac...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>â€œit possible approach could used decode inform...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>one potential use would language decoder could...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>to create atlas, scientists recorded peopleâ€™s ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>they matched transcripts stories brain activit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>huth used stories the moth radio hour short co...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>the enthralling stories, confident scientists ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>seven people listened two hours stories each.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>per person, amounted hearing roughly 25,000 wo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>the atlas shows words related terms exercise r...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>for example, left-hand side brain, ear, one ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>the region responds â€œkilledâ€, â€œconvictedâ€, â€œmu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>on brainâ€™s right-hand side, near top head, one...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>each word represented one spot words tend seve...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>one part brain, example, reliably responds wor...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>but word â€œtopâ€ activates many regions.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>one responds numbers measurements, another bui...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>the scientists created interactive website pub...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>strikingly, brain atlases similar participants...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>the scientists scanned five men two women, how...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>all native english speakers, two authors study...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>it highly possible people different background...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>armed atlas, researchers piece together brain ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>â€œthe idea murder represented lot brain,â€ galla...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>using haul data, group begun work new atlases ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>a brain atlas narrative structure far proved e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>â€œevery time come set narrative features, get t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>uri hasson, neuroscientist princeton universit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>unlike many studies looked brain activity isol...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>the next step, said, create comprehensive prec...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>ultimately, hasson believes possible reconstru...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>the ethical implications enormous.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>one benign use would see brain activity used a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>â€œthere many implications, barely touching surf...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>lorraine tyler, cognitive neuroscientist head ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>but brain atlas current form capture fine diff...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>it member many different groups, says tyler.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>â€œit something eat off, things made wood, thing...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>this kind detailed semantic information enable...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>â€œwhile research path-breaking scope, still lot...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sample0  metaphor0\n",
       "0   scientists created â€œatlas brainâ€ reveals meani...          1\n",
       "1   like colourful quilt laid cortex, atlas displa...          1\n",
       "2   â€œour goal build giant atlas shows one specific...          1\n",
       "3      no single brain region holds one word concept.          1\n",
       "4   a single brain spot associated number related ...          1\n",
       "5   and single word lights many different brain sp...          1\n",
       "6   together make networks represent meanings word...          1\n",
       "7                                 all light networks.          1\n",
       "8   described â€œtour de forceâ€ one researcher invol...          1\n",
       "9   with advances, technology could profound impac...          1\n",
       "10  â€œit possible approach could used decode inform...          1\n",
       "11  one potential use would language decoder could...          1\n",
       "12  to create atlas, scientists recorded peopleâ€™s ...          1\n",
       "13  they matched transcripts stories brain activit...          1\n",
       "14  huth used stories the moth radio hour short co...          1\n",
       "15  the enthralling stories, confident scientists ...          1\n",
       "16      seven people listened two hours stories each.          1\n",
       "17  per person, amounted hearing roughly 25,000 wo...          1\n",
       "18  the atlas shows words related terms exercise r...          1\n",
       "19  for example, left-hand side brain, ear, one ti...          1\n",
       "20  the region responds â€œkilledâ€, â€œconvictedâ€, â€œmu...          1\n",
       "21  on brainâ€™s right-hand side, near top head, one...          1\n",
       "22  each word represented one spot words tend seve...          1\n",
       "23  one part brain, example, reliably responds wor...          1\n",
       "24             but word â€œtopâ€ activates many regions.          1\n",
       "25  one responds numbers measurements, another bui...          1\n",
       "26  the scientists created interactive website pub...          1\n",
       "27  strikingly, brain atlases similar participants...          1\n",
       "28  the scientists scanned five men two women, how...          1\n",
       "29  all native english speakers, two authors study...          1\n",
       "30  it highly possible people different background...          1\n",
       "31  armed atlas, researchers piece together brain ...          1\n",
       "32  â€œthe idea murder represented lot brain,â€ galla...          1\n",
       "33  using haul data, group begun work new atlases ...          1\n",
       "34  a brain atlas narrative structure far proved e...          1\n",
       "35  â€œevery time come set narrative features, get t...          1\n",
       "36  uri hasson, neuroscientist princeton universit...          1\n",
       "37  unlike many studies looked brain activity isol...          1\n",
       "38  the next step, said, create comprehensive prec...          1\n",
       "39  ultimately, hasson believes possible reconstru...          1\n",
       "40                 the ethical implications enormous.          1\n",
       "41  one benign use would see brain activity used a...          1\n",
       "42  â€œthere many implications, barely touching surf...          1\n",
       "43  lorraine tyler, cognitive neuroscientist head ...          1\n",
       "44  but brain atlas current form capture fine diff...          1\n",
       "45       it member many different groups, says tyler.          1\n",
       "46  â€œit something eat off, things made wood, thing...          1\n",
       "47  this kind detailed semantic information enable...          1\n",
       "48  â€œwhile research path-breaking scope, still lot...          1"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_test_feature_sparse = vec.transform(df_test['sample0'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "df_test['metaphor0'] = logreg_predictions\n",
    "\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample0</th>\n",
       "      <th>sample1</th>\n",
       "      <th>sample2</th>\n",
       "      <th>sample3</th>\n",
       "      <th>sample4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Scientists created â€œatlas</td>\n",
       "      <td>created â€œatlas brainâ€</td>\n",
       "      <td>â€œatlas brainâ€ reveals</td>\n",
       "      <td>brainâ€ reveals meanings</td>\n",
       "      <td>reveals meanings words</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Like colourful quilt</td>\n",
       "      <td>colourful quilt laid</td>\n",
       "      <td>quilt laid cortex,</td>\n",
       "      <td>laid cortex, atlas</td>\n",
       "      <td>cortex, atlas displays</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>â€œOur goal build</td>\n",
       "      <td>goal build giant</td>\n",
       "      <td>build giant atlas</td>\n",
       "      <td>giant atlas shows</td>\n",
       "      <td>atlas shows one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No single brain</td>\n",
       "      <td>single brain region</td>\n",
       "      <td>brain region holds</td>\n",
       "      <td>region holds one</td>\n",
       "      <td>holds one word</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A single brain</td>\n",
       "      <td>single brain spot</td>\n",
       "      <td>brain spot associated</td>\n",
       "      <td>spot associated number</td>\n",
       "      <td>associated number related</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     sample0                sample1                sample2  \\\n",
       "0  Scientists created â€œatlas  created â€œatlas brainâ€  â€œatlas brainâ€ reveals   \n",
       "1       Like colourful quilt   colourful quilt laid     quilt laid cortex,   \n",
       "2            â€œOur goal build       goal build giant      build giant atlas   \n",
       "3            No single brain    single brain region     brain region holds   \n",
       "4             A single brain      single brain spot  brain spot associated   \n",
       "\n",
       "                   sample3                    sample4  \n",
       "0  brainâ€ reveals meanings     reveals meanings words  \n",
       "1       laid cortex, atlas     cortex, atlas displays  \n",
       "2        giant atlas shows            atlas shows one  \n",
       "3         region holds one             holds one word  \n",
       "4   spot associated number  associated number related  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.ix[:, 0:5].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_test.columns)\n",
    "#df_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"sciencearticle_line.txt\", \"r\") as testset:\n",
    "    filtered_words = []\n",
    "    testmeta = []\n",
    "    testmeta1 = []\n",
    "    word_list = ''\n",
    "    for line in testset:\n",
    "        word_list = line.split()\n",
    "        filtered_words = ' '.join([word.lower() for word in word_list if word not in stopwords.words('english')])\n",
    "        testmeta1.append(filtered_words)\n",
    "df_test = pd.DataFrame(testmeta1)\n",
    "df_test\n",
    "# df_test = df_test.ix[:, 0:6]\n",
    "df_test.columns = ['sample0']\n",
    "df_test = df_test.fillna(\"\")\n",
    "# df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk, re\n",
    "from nltk.corpus import brown\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_data_sets(sentences):\n",
    "    size = int(len(sentences) * 0.9)\n",
    "    train_sents = sentences[:size]\n",
    "    test_sents = sentences[size:]\n",
    "    return train_sents, test_sents\n",
    "\n",
    "def build_backoff_tagger (train_sents):\n",
    "    t0 = nltk.DefaultTagger('NN')\n",
    "    t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
    "    t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
    "    return t2\n",
    "\n",
    "\n",
    "def train_tagger(already_tagged_sents):\n",
    "    train_sents, test_sents = create_data_sets(already_tagged_sents)\n",
    "    ngram_tagger = build_backoff_tagger(train_sents)\n",
    "    print (\"%0.3f pos accuracy on test set\" % ngram_tagger.evaluate(test_sents))\n",
    "    return ngram_tagger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_tagger_on_brown():\n",
    "    brown_tagged_sents = brown.tagged_sents(categories=['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies',\n",
    "    'humor', 'learned', 'lore', 'mystery', 'religion', 'reviews', 'romance','science_fiction'])\n",
    "    return train_tagger(brown_tagged_sents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_text(corpus):\n",
    "    sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sents = sent_tokenizer.tokenize(corpus) # Split text into sentences    \n",
    "    return [nltk.word_tokenize(word) for word in raw_sents]\n",
    "\n",
    "def create_corpus(f):\n",
    "    with open(f, 'r') as text_file:\n",
    "        new_corpus = text_file.read()\n",
    "    return new_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.911 pos accuracy on test set\n"
     ]
    }
   ],
   "source": [
    "brown_tagger = train_tagger_on_brown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('scientists', 'NNS'),\n",
       "  ('have', 'HV'),\n",
       "  ('created', 'VBN'),\n",
       "  ('an', 'AT'),\n",
       "  ('atlas', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('brain', 'NN'),\n",
       "  ('that', 'CS'),\n",
       "  ('reveals', 'VBZ'),\n",
       "  ('how', 'WRB'),\n",
       "  ('the', 'AT'),\n",
       "  ('meanings', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('words', 'NNS'),\n",
       "  ('are', 'BER'),\n",
       "  ('arranged', 'VBN'),\n",
       "  ('across', 'IN'),\n",
       "  ('different', 'JJ'),\n",
       "  ('regions', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('organ', 'NN')],\n",
       " [('like', 'CS'),\n",
       "  ('a', 'AT'),\n",
       "  ('colourful', 'NN'),\n",
       "  ('quilt', 'NN'),\n",
       "  ('laid', 'VBN'),\n",
       "  ('over', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('cortex', 'NN'),\n",
       "  ('the', 'AT'),\n",
       "  ('atlas', 'NN'),\n",
       "  ('displays', 'VBZ'),\n",
       "  ('in', 'IN'),\n",
       "  ('rainbow', 'NN'),\n",
       "  ('hues', 'NNS'),\n",
       "  ('how', 'WRB'),\n",
       "  ('individual', 'JJ'),\n",
       "  ('words', 'NNS'),\n",
       "  ('and', 'CC'),\n",
       "  ('the', 'AT'),\n",
       "  ('concepts', 'NNS'),\n",
       "  ('they', 'PPSS'),\n",
       "  ('convey', 'VB'),\n",
       "  ('can', 'MD'),\n",
       "  ('be', 'BE'),\n",
       "  ('grouped', 'VBN'),\n",
       "  ('together', 'RB'),\n",
       "  ('in', 'IN'),\n",
       "  ('clumps', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('white', 'JJ'),\n",
       "  ('matter', 'NN')],\n",
       " [('our', 'PP$'),\n",
       "  ('goal', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('to', 'TO'),\n",
       "  ('build', 'VB'),\n",
       "  ('a', 'AT'),\n",
       "  ('giant', 'JJ'),\n",
       "  ('atlas', 'NN'),\n",
       "  ('that', 'CS'),\n",
       "  ('shows', 'NNS'),\n",
       "  ('how', 'WRB'),\n",
       "  ('one', 'PN'),\n",
       "  ('specific', 'JJ'),\n",
       "  ('aspect', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('language', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('represented', 'VBN'),\n",
       "  ('in', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('brain', 'NN'),\n",
       "  ('in', 'IN'),\n",
       "  ('this', 'DT'),\n",
       "  ('case', 'NN'),\n",
       "  ('semantics', 'NN'),\n",
       "  ('or', 'CC'),\n",
       "  ('the', 'AT'),\n",
       "  ('meanings', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('words', 'NNS'),\n",
       "  ('said', 'VBD'),\n",
       "  ('jack', 'NN'),\n",
       "  ('gallant', 'JJ'),\n",
       "  ('a', 'AT'),\n",
       "  ('neuroscientist', 'NN'),\n",
       "  ('at', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('university', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('california', 'NN'),\n",
       "  ('berkeley', 'NN')],\n",
       " [('no', 'AT'),\n",
       "  ('single', 'AP'),\n",
       "  ('brain', 'NN'),\n",
       "  ('region', 'NN'),\n",
       "  ('holds', 'VBZ'),\n",
       "  ('one', 'CD'),\n",
       "  ('word', 'NN'),\n",
       "  ('or', 'CC'),\n",
       "  ('concept', 'NN')],\n",
       " [('a', 'AT'),\n",
       "  ('single', 'AP'),\n",
       "  ('brain', 'NN'),\n",
       "  ('spot', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('associated', 'VBN'),\n",
       "  ('with', 'IN'),\n",
       "  ('a', 'AT'),\n",
       "  ('number', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('related', 'VBN'),\n",
       "  ('words', 'NNS')],\n",
       " [('and', 'CC'),\n",
       "  ('each', 'DT'),\n",
       "  ('single', 'AP'),\n",
       "  ('word', 'NN'),\n",
       "  ('lights', 'NNS'),\n",
       "  ('up', 'RP'),\n",
       "  ('many', 'AP'),\n",
       "  ('different', 'JJ'),\n",
       "  ('brain', 'NN'),\n",
       "  ('spots', 'NNS')],\n",
       " [('together', 'RB'),\n",
       "  ('they', 'PPSS'),\n",
       "  ('make', 'VB'),\n",
       "  ('up', 'RP'),\n",
       "  ('networks', 'NNS'),\n",
       "  ('that', 'WPS'),\n",
       "  ('represent', 'VB'),\n",
       "  ('the', 'AT'),\n",
       "  ('meanings', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('each', 'DT'),\n",
       "  ('word', 'NN'),\n",
       "  ('we', 'PPSS'),\n",
       "  ('use', 'VB'),\n",
       "  ('life', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('love', 'NN'),\n",
       "  ('death', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('taxes', 'NNS'),\n",
       "  ('clouds', 'NNS'),\n",
       "  ('florida', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('bra', 'NN')],\n",
       " [('all', 'ABN'),\n",
       "  ('light', 'NN'),\n",
       "  ('up', 'RP'),\n",
       "  ('their', 'PP$'),\n",
       "  ('own', 'JJ'),\n",
       "  ('networks', 'NNS')],\n",
       " [('described', 'VBN'),\n",
       "  ('as', 'CS'),\n",
       "  ('a', 'AT'),\n",
       "  ('tour', 'NN'),\n",
       "  ('de', 'FW-IN'),\n",
       "  ('force', 'FW-NN'),\n",
       "  ('by', 'IN'),\n",
       "  ('one', 'CD'),\n",
       "  ('researcher', 'NN'),\n",
       "  ('who', 'WPS'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('not', '*'),\n",
       "  ('involved', 'VBN'),\n",
       "  ('in', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('study', 'NN'),\n",
       "  ('the', 'AT'),\n",
       "  ('atlas', 'NN'),\n",
       "  ('demonstrates', 'VBZ'),\n",
       "  ('how', 'WRB'),\n",
       "  ('modern', 'JJ'),\n",
       "  ('imaging', 'NN'),\n",
       "  ('can', 'MD'),\n",
       "  ('transform', 'VB'),\n",
       "  ('our', 'PP$'),\n",
       "  ('knowledge', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('how', 'WRB'),\n",
       "  ('the', 'AT'),\n",
       "  ('brain', 'NN'),\n",
       "  ('performs', 'VBZ'),\n",
       "  ('some', 'DTI'),\n",
       "  ('of', 'IN'),\n",
       "  ('its', 'PP$'),\n",
       "  ('most', 'QL'),\n",
       "  ('important', 'JJ'),\n",
       "  ('tasks', 'NNS')],\n",
       " [('with', 'IN'),\n",
       "  ('further', 'JJR'),\n",
       "  ('advances', 'NNS'),\n",
       "  ('the', 'AT'),\n",
       "  ('technology', 'NN'),\n",
       "  ('could', 'MD'),\n",
       "  ('have', 'HV'),\n",
       "  ('a', 'AT'),\n",
       "  ('profound', 'JJ'),\n",
       "  ('impact', 'NN'),\n",
       "  ('on', 'IN'),\n",
       "  ('medicine', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('other', 'AP'),\n",
       "  ('fields', 'NNS')],\n",
       " [('it', 'PPS'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('possible', 'JJ'),\n",
       "  ('that', 'CS'),\n",
       "  ('this', 'DT'),\n",
       "  ('approach', 'NN'),\n",
       "  ('could', 'MD'),\n",
       "  ('be', 'BE'),\n",
       "  ('used', 'VBN'),\n",
       "  ('to', 'TO'),\n",
       "  ('decode', 'NN'),\n",
       "  ('information', 'NN'),\n",
       "  ('about', 'IN'),\n",
       "  ('what', 'WDT'),\n",
       "  ('words', 'NNS'),\n",
       "  ('a', 'AT'),\n",
       "  ('person', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('hearing', 'NN'),\n",
       "  ('reading', 'NN'),\n",
       "  ('or', 'CC'),\n",
       "  ('possibly', 'RB'),\n",
       "  ('even', 'RB'),\n",
       "  ('thinking', 'VBG'),\n",
       "  ('said', 'VBD'),\n",
       "  ('alexander', 'NN'),\n",
       "  ('huth', 'NN'),\n",
       "  ('the', 'AT'),\n",
       "  ('first', 'OD'),\n",
       "  ('author', 'NN'),\n",
       "  ('on', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('study', 'NN')],\n",
       " [('one', 'CD'),\n",
       "  ('potential', 'JJ'),\n",
       "  ('use', 'NN'),\n",
       "  ('would', 'MD'),\n",
       "  ('be', 'BE'),\n",
       "  ('a', 'AT'),\n",
       "  ('language', 'NN'),\n",
       "  ('decoder', 'NN'),\n",
       "  ('that', 'CS'),\n",
       "  ('could', 'MD'),\n",
       "  ('allow', 'VB'),\n",
       "  ('people', 'NNS'),\n",
       "  ('silenced', 'VBN'),\n",
       "  ('by', 'IN'),\n",
       "  ('motor', 'NN'),\n",
       "  ('neurone', 'NN'),\n",
       "  ('disease', 'NN'),\n",
       "  ('or', 'CC'),\n",
       "  ('locked', 'VBN'),\n",
       "  ('in', 'IN'),\n",
       "  ('syndrome', 'NN'),\n",
       "  ('to', 'IN'),\n",
       "  ('speak', 'VB'),\n",
       "  ('through', 'IN'),\n",
       "  ('a', 'AT'),\n",
       "  ('computer', 'NN')],\n",
       " [('to', 'IN-HL'),\n",
       "  ('create', 'VB'),\n",
       "  ('the', 'AT'),\n",
       "  ('atlas', 'NN'),\n",
       "  ('the', 'AT'),\n",
       "  ('scientists', 'NNS'),\n",
       "  ('recorded', 'VBN'),\n",
       "  ('people', 'NNS'),\n",
       "  ('s', 'NN'),\n",
       "  ('brain', 'NN'),\n",
       "  ('activity', 'NN'),\n",
       "  ('while', 'CS'),\n",
       "  ('they', 'PPSS'),\n",
       "  ('listened', 'VBD'),\n",
       "  ('to', 'TO'),\n",
       "  ('stories', 'NNS'),\n",
       "  ('read', 'VB'),\n",
       "  ('out', 'RP'),\n",
       "  ('on', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('moth', 'NN'),\n",
       "  ('radio', 'NN'),\n",
       "  ('hour', 'NN'),\n",
       "  ('a', 'AT'),\n",
       "  ('us', 'PPO'),\n",
       "  ('radio', 'NN'),\n",
       "  ('show', 'NN')],\n",
       " [('they', 'PPSS'),\n",
       "  ('then', 'RB'),\n",
       "  ('matched', 'VBN'),\n",
       "  ('the', 'AT'),\n",
       "  ('transcripts', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('stories', 'NNS'),\n",
       "  ('with', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('brain', 'NN'),\n",
       "  ('activity', 'NN'),\n",
       "  ('data', 'NN'),\n",
       "  ('to', 'IN'),\n",
       "  ('show', 'NN'),\n",
       "  ('how', 'WRB'),\n",
       "  ('groups', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('related', 'VBN'),\n",
       "  ('words', 'NNS'),\n",
       "  ('triggered', 'VBN'),\n",
       "  ('neural', 'JJ'),\n",
       "  ('responses', 'NNS'),\n",
       "  ('in', 'IN'),\n",
       "  ('50', 'CD'),\n",
       "  ('000', 'NN'),\n",
       "  ('to', 'IN'),\n",
       "  ('80', 'CD'),\n",
       "  ('000', 'NN'),\n",
       "  ('pea', 'NN'),\n",
       "  ('sized', 'VBD'),\n",
       "  ('spots', 'NNS'),\n",
       "  ('all', 'ABN'),\n",
       "  ('over', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('cerebral', 'JJ'),\n",
       "  ('cortex', 'NN')],\n",
       " [('huth', 'NN'),\n",
       "  ('used', 'VBN'),\n",
       "  ('stories', 'NNS'),\n",
       "  ('from', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('moth', 'NN'),\n",
       "  ('radio', 'NN'),\n",
       "  ('hour', 'NN'),\n",
       "  ('because', 'CS'),\n",
       "  ('they', 'PPSS'),\n",
       "  ('are', 'BER'),\n",
       "  ('short', 'JJ'),\n",
       "  ('and', 'CC'),\n",
       "  ('compelling', 'JJ')],\n",
       " [('the', 'AT'),\n",
       "  ('more', 'QL'),\n",
       "  ('enthralling', 'JJ'),\n",
       "  ('the', 'AT'),\n",
       "  ('stories', 'NNS'),\n",
       "  ('the', 'AT'),\n",
       "  ('more', 'QL'),\n",
       "  ('confident', 'JJ'),\n",
       "  ('the', 'AT'),\n",
       "  ('scientists', 'NNS'),\n",
       "  ('could', 'MD'),\n",
       "  ('be', 'BE'),\n",
       "  ('that', 'CS'),\n",
       "  ('the', 'AT'),\n",
       "  ('people', 'NNS'),\n",
       "  ('being', 'BEG'),\n",
       "  ('scanned', 'VBD'),\n",
       "  ('were', 'BED'),\n",
       "  ('focusing', 'VBG'),\n",
       "  ('on', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('words', 'NNS'),\n",
       "  ('and', 'CC'),\n",
       "  ('not', '*'),\n",
       "  ('drifting', 'VBG'),\n",
       "  ('off', 'RP')],\n",
       " [('seven', 'CD'),\n",
       "  ('people', 'NNS'),\n",
       "  ('listened', 'VBD'),\n",
       "  ('to', 'TO'),\n",
       "  ('two', 'CD'),\n",
       "  ('hours', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('stories', 'NNS'),\n",
       "  ('each', 'DT')],\n",
       " [('per', 'IN'),\n",
       "  ('person', 'NN'),\n",
       "  ('that', 'CS'),\n",
       "  ('amounted', 'VBN'),\n",
       "  ('to', 'TO'),\n",
       "  ('hearing', 'NN'),\n",
       "  ('roughly', 'QL'),\n",
       "  ('25', 'CD'),\n",
       "  ('000', 'NN'),\n",
       "  ('words', 'NNS'),\n",
       "  ('and', 'CC'),\n",
       "  ('more', 'AP'),\n",
       "  ('than', 'IN'),\n",
       "  ('3', 'CD'),\n",
       "  ('000', 'NN'),\n",
       "  ('different', 'JJ'),\n",
       "  ('words', 'NNS'),\n",
       "  ('as', 'CS'),\n",
       "  ('they', 'PPSS'),\n",
       "  ('lay', 'VBD'),\n",
       "  ('in', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('scanner', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('atlas', 'NN'),\n",
       "  ('shows', 'VBZ'),\n",
       "  ('how', 'WRB'),\n",
       "  ('words', 'NNS'),\n",
       "  ('and', 'CC'),\n",
       "  ('related', 'VBN'),\n",
       "  ('terms', 'NNS'),\n",
       "  ('exercise', 'VB'),\n",
       "  ('the', 'AT'),\n",
       "  ('same', 'AP'),\n",
       "  ('regions', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('brain', 'NN')],\n",
       " [('for', 'CS'),\n",
       "  ('example', 'NN'),\n",
       "  ('on', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('left', 'NR'),\n",
       "  ('hand', 'NN'),\n",
       "  ('side', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('brain', 'NN'),\n",
       "  ('above', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('ear', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('one', 'CD'),\n",
       "  ('of', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('tiny', 'JJ'),\n",
       "  ('regions', 'NNS'),\n",
       "  ('that', 'WPS'),\n",
       "  ('represents', 'VBZ'),\n",
       "  ('the', 'AT'),\n",
       "  ('word', 'NN'),\n",
       "  ('victim', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('same', 'AP'),\n",
       "  ('region', 'NN'),\n",
       "  ('responds', 'VBZ'),\n",
       "  ('to', 'TO'),\n",
       "  ('killed', 'VBN'),\n",
       "  ('convicted', 'VBN'),\n",
       "  ('murdered', 'VBN'),\n",
       "  ('and', 'CC'),\n",
       "  ('confessed', 'VBD')],\n",
       " [('on', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('brain', 'NN'),\n",
       "  ('s', 'NN'),\n",
       "  ('right', 'QL'),\n",
       "  ('hand', 'NN'),\n",
       "  ('side', 'NN'),\n",
       "  ('near', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('top', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('head', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('one', 'CD'),\n",
       "  ('of', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('brain', 'NN'),\n",
       "  ('spots', 'NNS'),\n",
       "  ('activated', 'VBN'),\n",
       "  ('by', 'IN'),\n",
       "  ('family', 'NN'),\n",
       "  ('terms', 'NNS'),\n",
       "  ('wife', 'NN'),\n",
       "  ('husband', 'NN'),\n",
       "  ('children', 'NNS'),\n",
       "  ('parents', 'NNS')],\n",
       " [('each', 'DT'),\n",
       "  ('word', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('represented', 'VBN'),\n",
       "  ('by', 'IN'),\n",
       "  ('more', 'AP'),\n",
       "  ('than', 'IN'),\n",
       "  ('one', 'CD'),\n",
       "  ('spot', 'NN'),\n",
       "  ('because', 'CS'),\n",
       "  ('words', 'NNS'),\n",
       "  ('tend', 'VB'),\n",
       "  ('to', 'TO'),\n",
       "  ('have', 'HV'),\n",
       "  ('several', 'AP'),\n",
       "  ('meanings', 'NNS')],\n",
       " [('one', 'CD'),\n",
       "  ('part', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('brain', 'NN'),\n",
       "  ('for', 'IN'),\n",
       "  ('example', 'NN'),\n",
       "  ('reliably', 'NN'),\n",
       "  ('responds', 'VBZ'),\n",
       "  ('to', 'TO'),\n",
       "  ('the', 'AT'),\n",
       "  ('word', 'NN'),\n",
       "  ('top', 'NN'),\n",
       "  ('along', 'IN'),\n",
       "  ('with', 'IN'),\n",
       "  ('other', 'AP'),\n",
       "  ('words', 'NNS'),\n",
       "  ('that', 'WPS'),\n",
       "  ('describe', 'VB'),\n",
       "  ('clothing', 'NN')],\n",
       " [('but', 'CC'),\n",
       "  ('the', 'AT'),\n",
       "  ('word', 'NN'),\n",
       "  ('top', 'NN'),\n",
       "  ('activates', 'NN'),\n",
       "  ('many', 'AP'),\n",
       "  ('other', 'AP'),\n",
       "  ('regions', 'NNS')],\n",
       " [('one', 'CD'),\n",
       "  ('of', 'IN'),\n",
       "  ('them', 'PPO'),\n",
       "  ('responds', 'VBZ'),\n",
       "  ('to', 'TO'),\n",
       "  ('numbers', 'NNS'),\n",
       "  ('and', 'CC'),\n",
       "  ('measurements', 'NNS'),\n",
       "  ('another', 'DT'),\n",
       "  ('to', 'IN'),\n",
       "  ('buildings', 'NNS'),\n",
       "  ('and', 'CC'),\n",
       "  ('places', 'NNS')],\n",
       " [('the', 'AT'),\n",
       "  ('scientists', 'NNS'),\n",
       "  ('have', 'HV'),\n",
       "  ('created', 'VBN'),\n",
       "  ('an', 'AT'),\n",
       "  ('interactive', 'NN'),\n",
       "  ('website', 'NN'),\n",
       "  ('where', 'WRB'),\n",
       "  ('the', 'AT'),\n",
       "  ('public', 'JJ'),\n",
       "  ('can', 'MD'),\n",
       "  ('explore', 'VB'),\n",
       "  ('the', 'AT'),\n",
       "  ('brain', 'NN'),\n",
       "  ('atlas', 'NN')],\n",
       " [('strikingly', 'RB'),\n",
       "  ('the', 'AT'),\n",
       "  ('brain', 'NN'),\n",
       "  ('atlases', 'NN'),\n",
       "  ('were', 'BED'),\n",
       "  ('similar', 'JJ'),\n",
       "  ('for', 'IN'),\n",
       "  ('all', 'ABN'),\n",
       "  ('the', 'AT'),\n",
       "  ('participants', 'NNS'),\n",
       "  ('suggesting', 'VBG'),\n",
       "  ('that', 'CS'),\n",
       "  ('their', 'PP$'),\n",
       "  ('brains', 'NNS'),\n",
       "  ('organised', 'VBD'),\n",
       "  ('the', 'AT'),\n",
       "  ('meanings', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('words', 'NNS'),\n",
       "  ('in', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('same', 'AP'),\n",
       "  ('way', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('scientists', 'NNS'),\n",
       "  ('only', 'RB'),\n",
       "  ('scanned', 'VBD'),\n",
       "  ('five', 'CD'),\n",
       "  ('men', 'NNS'),\n",
       "  ('and', 'CC'),\n",
       "  ('two', 'CD'),\n",
       "  ('women', 'NNS'),\n",
       "  ('however', 'RB')],\n",
       " [('all', 'ABN'),\n",
       "  ('are', 'BER'),\n",
       "  ('native', 'JJ'),\n",
       "  ('english', 'NN'),\n",
       "  ('speakers', 'NNS'),\n",
       "  ('and', 'CC'),\n",
       "  ('two', 'CD'),\n",
       "  ('are', 'BER'),\n",
       "  ('authors', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('study', 'NN'),\n",
       "  ('published', 'VBN'),\n",
       "  ('in', 'IN'),\n",
       "  ('nature', 'NN')],\n",
       " [('it', 'PPS'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('highly', 'QL'),\n",
       "  ('possible', 'JJ'),\n",
       "  ('that', 'CS'),\n",
       "  ('people', 'NNS'),\n",
       "  ('from', 'IN'),\n",
       "  ('different', 'JJ'),\n",
       "  ('backgrounds', 'NNS'),\n",
       "  ('and', 'CC'),\n",
       "  ('cultures', 'NNS'),\n",
       "  ('will', 'MD'),\n",
       "  ('have', 'HV'),\n",
       "  ('different', 'JJ'),\n",
       "  ('semantic', 'JJ'),\n",
       "  ('brain', 'NN'),\n",
       "  ('atlases', 'NN')],\n",
       " [('armed', 'VBN'),\n",
       "  ('with', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('atlas', 'NN'),\n",
       "  ('researchers', 'NNS'),\n",
       "  ('can', 'MD'),\n",
       "  ('now', 'RB'),\n",
       "  ('piece', 'NN'),\n",
       "  ('together', 'RB'),\n",
       "  ('the', 'AT'),\n",
       "  ('brain', 'NN'),\n",
       "  ('networks', 'NNS'),\n",
       "  ('that', 'WPS'),\n",
       "  ('represent', 'VB'),\n",
       "  ('wildly', 'RB'),\n",
       "  ('different', 'JJ'),\n",
       "  ('concepts', 'NNS'),\n",
       "  ('from', 'IN'),\n",
       "  ('numbers', 'NNS'),\n",
       "  ('to', 'TO'),\n",
       "  ('murder', 'VB'),\n",
       "  ('and', 'CC'),\n",
       "  ('religion', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('idea', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('murder', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('represented', 'VBN'),\n",
       "  ('a', 'AT'),\n",
       "  ('lot', 'NN'),\n",
       "  ('in', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('brain', 'NN'),\n",
       "  ('gallant', 'JJ'),\n",
       "  ('said', 'VBD')],\n",
       " [('using', 'VBG'),\n",
       "  ('the', 'AT'),\n",
       "  ('same', 'AP'),\n",
       "  ('haul', 'VB'),\n",
       "  ('of', 'IN'),\n",
       "  ('data', 'NN'),\n",
       "  ('the', 'AT'),\n",
       "  ('group', 'NN'),\n",
       "  ('has', 'HVZ'),\n",
       "  ('begun', 'VBN'),\n",
       "  ('work', 'NN'),\n",
       "  ('on', 'IN'),\n",
       "  ('new', 'JJ'),\n",
       "  ('atlases', 'NN'),\n",
       "  ('that', 'CS'),\n",
       "  ('show', 'VB'),\n",
       "  ('how', 'WRB'),\n",
       "  ('the', 'AT'),\n",
       "  ('brain', 'NN'),\n",
       "  ('holds', 'VBZ'),\n",
       "  ('information', 'NN'),\n",
       "  ('on', 'IN'),\n",
       "  ('other', 'AP'),\n",
       "  ('aspects', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('language', 'NN'),\n",
       "  ('from', 'IN'),\n",
       "  ('phonemes', 'NNS'),\n",
       "  ('to', 'TO'),\n",
       "  ('syntax', 'NN')],\n",
       " [('a', 'AT'),\n",
       "  ('brain', 'NN'),\n",
       "  ('atlas', 'NN'),\n",
       "  ('for', 'IN'),\n",
       "  ('narrative', 'NN'),\n",
       "  ('structure', 'NN'),\n",
       "  ('has', 'HVZ'),\n",
       "  ('so', 'QL'),\n",
       "  ('far', 'RB'),\n",
       "  ('proved', 'VBD'),\n",
       "  ('elusive', 'JJ'),\n",
       "  ('however', 'WRB')],\n",
       " [('every', 'AT'),\n",
       "  ('time', 'NN'),\n",
       "  ('we', 'PPSS'),\n",
       "  ('come', 'VB'),\n",
       "  ('up', 'RP'),\n",
       "  ('with', 'IN'),\n",
       "  ('a', 'AT'),\n",
       "  ('set', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('narrative', 'NN'),\n",
       "  ('features', 'NNS'),\n",
       "  ('we', 'PPSS'),\n",
       "  ('get', 'VB'),\n",
       "  ('told', 'VBD'),\n",
       "  ('they', 'PPSS'),\n",
       "  ('aren', 'NN'),\n",
       "  ('t', 'NN'),\n",
       "  ('the', 'AT'),\n",
       "  ('right', 'NN'),\n",
       "  ('set', 'VBN'),\n",
       "  ('of', 'IN'),\n",
       "  ('narrative', 'NN'),\n",
       "  ('features', 'NNS'),\n",
       "  ('said', 'VBD'),\n",
       "  ('gallant', 'JJ')],\n",
       " [('uri', 'NN'),\n",
       "  ('hasson', 'NN'),\n",
       "  ('a', 'AT'),\n",
       "  ('neuroscientist', 'NN'),\n",
       "  ('at', 'IN'),\n",
       "  ('princeton', 'NN'),\n",
       "  ('university', 'NN'),\n",
       "  ('praised', 'VBD'),\n",
       "  ('the', 'AT'),\n",
       "  ('work', 'NN')],\n",
       " [('unlike', 'IN'),\n",
       "  ('many', 'AP'),\n",
       "  ('studies', 'NNS'),\n",
       "  ('that', 'WPS'),\n",
       "  ('looked', 'VBD'),\n",
       "  ('at', 'IN'),\n",
       "  ('brain', 'NN'),\n",
       "  ('activity', 'NN'),\n",
       "  ('when', 'WRB'),\n",
       "  ('an', 'AT'),\n",
       "  ('isolated', 'VBN'),\n",
       "  ('word', 'NN'),\n",
       "  ('or', 'CC'),\n",
       "  ('sentence', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('spoken', 'VBN'),\n",
       "  ('gallant', 'JJ'),\n",
       "  ('s', 'NN'),\n",
       "  ('team', 'NN'),\n",
       "  ('had', 'HVD'),\n",
       "  ('shed', 'NN'),\n",
       "  ('light', 'NN'),\n",
       "  ('on', 'IN'),\n",
       "  ('how', 'WRB'),\n",
       "  ('the', 'AT'),\n",
       "  ('brain', 'NN'),\n",
       "  ('worked', 'VBD'),\n",
       "  ('in', 'IN'),\n",
       "  ('a', 'AT'),\n",
       "  ('real', 'JJ'),\n",
       "  ('world', 'NN'),\n",
       "  ('scenario', 'NN'),\n",
       "  ('he', 'PPS'),\n",
       "  ('said', 'VBD')],\n",
       " [('the', 'AT'),\n",
       "  ('next', 'AP'),\n",
       "  ('step', 'NN'),\n",
       "  ('he', 'PPS'),\n",
       "  ('said', 'VBD'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('to', 'TO'),\n",
       "  ('create', 'VB'),\n",
       "  ('a', 'AT'),\n",
       "  ('more', 'QL'),\n",
       "  ('comprehensive', 'JJ'),\n",
       "  ('and', 'CC'),\n",
       "  ('precise', 'JJ'),\n",
       "  ('semantic', 'JJ'),\n",
       "  ('brain', 'NN'),\n",
       "  ('atlas', 'NN')],\n",
       " [('ultimately', 'RB'),\n",
       "  ('hasson', 'NN'),\n",
       "  ('believes', 'VBZ'),\n",
       "  ('it', 'PPO'),\n",
       "  ('will', 'MD'),\n",
       "  ('be', 'BE'),\n",
       "  ('possible', 'JJ'),\n",
       "  ('to', 'TO'),\n",
       "  ('reconstruct', 'VB'),\n",
       "  ('the', 'AT'),\n",
       "  ('words', 'NNS'),\n",
       "  ('a', 'AT'),\n",
       "  ('person', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('thinking', 'VBG'),\n",
       "  ('from', 'IN'),\n",
       "  ('their', 'PP$'),\n",
       "  ('brain', 'NN'),\n",
       "  ('activity', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('ethical', 'JJ'),\n",
       "  ('implications', 'NNS'),\n",
       "  ('are', 'BER'),\n",
       "  ('enormous', 'JJ')],\n",
       " [('one', 'CD'),\n",
       "  ('more', 'AP'),\n",
       "  ('benign', 'JJ'),\n",
       "  ('use', 'NN'),\n",
       "  ('would', 'MD'),\n",
       "  ('see', 'VB'),\n",
       "  ('brain', 'NN'),\n",
       "  ('activity', 'NN'),\n",
       "  ('used', 'VBN'),\n",
       "  ('to', 'TO'),\n",
       "  ('assess', 'VB'),\n",
       "  ('whether', 'CS'),\n",
       "  ('political', 'JJ'),\n",
       "  ('messages', 'NNS'),\n",
       "  ('have', 'HV'),\n",
       "  ('been', 'BEN'),\n",
       "  ('effectively', 'RB'),\n",
       "  ('communicated', 'VBN'),\n",
       "  ('to', 'TO'),\n",
       "  ('the', 'AT'),\n",
       "  ('public', 'JJ')],\n",
       " [('there', 'EX'),\n",
       "  ('are', 'BER'),\n",
       "  ('so', 'QL'),\n",
       "  ('many', 'AP'),\n",
       "  ('implications', 'NNS'),\n",
       "  ('and', 'CC'),\n",
       "  ('we', 'PPSS'),\n",
       "  ('are', 'BER'),\n",
       "  ('barely', 'RB'),\n",
       "  ('touching', 'JJ'),\n",
       "  ('the', 'AT'),\n",
       "  ('surface', 'NN'),\n",
       "  ('he', 'PPS'),\n",
       "  ('said', 'VBD')],\n",
       " [('lorraine', 'NN'),\n",
       "  ('tyler', 'NN'),\n",
       "  ('a', 'AT'),\n",
       "  ('cognitive', 'JJ'),\n",
       "  ('neuroscientist', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('head', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('centre', 'NN'),\n",
       "  ('for', 'IN'),\n",
       "  ('speech', 'NN'),\n",
       "  ('language', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('the', 'AT'),\n",
       "  ('brain', 'NN'),\n",
       "  ('at', 'IN'),\n",
       "  ('cambridge', 'NN'),\n",
       "  ('university', 'NN'),\n",
       "  ('said', 'VBD'),\n",
       "  ('the', 'AT'),\n",
       "  ('research', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('tour', 'NN'),\n",
       "  ('de', 'FW-IN'),\n",
       "  ('force', 'FW-NN'),\n",
       "  ('in', 'IN'),\n",
       "  ('its', 'PP$'),\n",
       "  ('scope', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('methods', 'NNS')],\n",
       " [('but', 'CC'),\n",
       "  ('the', 'AT'),\n",
       "  ('brain', 'NN'),\n",
       "  ('atlas', 'NN'),\n",
       "  ('in', 'IN'),\n",
       "  ('its', 'PP$'),\n",
       "  ('current', 'JJ'),\n",
       "  ('form', 'NN'),\n",
       "  ('does', 'DOZ'),\n",
       "  ('not', '*'),\n",
       "  ('capture', 'VB'),\n",
       "  ('fine', 'JJ'),\n",
       "  ('differences', 'NNS'),\n",
       "  ('in', 'IN'),\n",
       "  ('word', 'NN'),\n",
       "  ('meanings', 'NNS'),\n",
       "  ('take', 'VB'),\n",
       "  ('the', 'AT'),\n",
       "  ('word', 'NN'),\n",
       "  ('table', 'NN')],\n",
       " [('it', 'PPS'),\n",
       "  ('can', 'MD'),\n",
       "  ('be', 'BE'),\n",
       "  ('a', 'AT'),\n",
       "  ('member', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('many', 'AP'),\n",
       "  ('different', 'JJ'),\n",
       "  ('groups', 'NNS'),\n",
       "  ('says', 'VBZ'),\n",
       "  ('tyler', 'NN')],\n",
       " [('it', 'PPS'),\n",
       "  ('can', 'MD'),\n",
       "  ('be', 'BE'),\n",
       "  ('something', 'PN'),\n",
       "  ('to', 'TO'),\n",
       "  ('eat', 'VB'),\n",
       "  ('off', 'RP'),\n",
       "  ('things', 'NNS'),\n",
       "  ('made', 'VBN'),\n",
       "  ('of', 'IN'),\n",
       "  ('wood', 'NN'),\n",
       "  ('things', 'NNS'),\n",
       "  ('that', 'WPS'),\n",
       "  ('are', 'BER'),\n",
       "  ('heavy', 'JJ'),\n",
       "  ('things', 'NNS'),\n",
       "  ('having', 'HVG'),\n",
       "  ('four', 'CD'),\n",
       "  ('legs', 'NNS'),\n",
       "  ('non', 'FW-*'),\n",
       "  ('animate', 'JJ'),\n",
       "  ('objects', 'NNS'),\n",
       "  ('and', 'CC'),\n",
       "  ('so', 'RB'),\n",
       "  ('on', 'IN')],\n",
       " [('this', 'DT'),\n",
       "  ('kind', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('detailed', 'VBN'),\n",
       "  ('semantic', 'JJ'),\n",
       "  ('information', 'NN'),\n",
       "  ('that', 'CS'),\n",
       "  ('enables', 'VBZ'),\n",
       "  ('words', 'NNS'),\n",
       "  ('to', 'TO'),\n",
       "  ('be', 'BE'),\n",
       "  ('used', 'VBN'),\n",
       "  ('flexibly', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('lost', 'VBN'),\n",
       "  ('in', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('analysis', 'NN'),\n",
       "  ('she', 'PPS'),\n",
       "  ('said', 'VBD')],\n",
       " [('while', 'CS'),\n",
       "  ('this', 'DT'),\n",
       "  ('research', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('path', 'NN'),\n",
       "  ('breaking', 'VBG'),\n",
       "  ('in', 'IN'),\n",
       "  ('its', 'PP$'),\n",
       "  ('scope', 'NN'),\n",
       "  ('there', 'EX'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('still', 'RB'),\n",
       "  ('a', 'AT'),\n",
       "  ('lot', 'NN'),\n",
       "  ('to', 'IN'),\n",
       "  ('learn', 'VB'),\n",
       "  ('about', 'IN'),\n",
       "  ('how', 'WRB'),\n",
       "  ('semantics', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('represented', 'VBN'),\n",
       "  ('in', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('brain', 'NN')]]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"sciencearticle_line.txt\", \"r\") as testset:\n",
    "    filtered_words = []\n",
    "    testmeta = []\n",
    "    testmeta1 = []\n",
    "    word_list = ''\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    for line in testset:\n",
    "        word_list = line.split()\n",
    "#         filtered_words = ' '.join([word.lower() for word in word_list if word not in stopwords.words('english')])\n",
    "        filtered_words = ' '.join([word.lower() for word in word_list])\n",
    "        testmeta1.append(tokenizer.tokenize(filtered_words))\n",
    "testmeta1[0]\n",
    "\n",
    "def get_pos(sents, tagger):        \n",
    "    return [tagger.tag(sent) for sent in sents]\n",
    "\n",
    "pos = get_pos(testmeta1, brown_tagger)\n",
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.911 pos accuracy on test set\n"
     ]
    }
   ],
   "source": [
    "def train_tagger_on_brown_augmented_with_additional_sents():\n",
    "\n",
    "    additional_sents = [[('colorful', 'JJ'), ('quilt', 'NN')],\n",
    "                        [('regions', 'NN'), ('represent', 'VB'), ('world', 'NN')],\n",
    "                        [('public', 'NN'), ('explore', 'VB'), ('brain', 'NN')]]\n",
    "\n",
    "\n",
    "    brown_tagged_sents = brown.tagged_sents(categories=['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies',\n",
    "    'humor', 'learned', 'lore', 'mystery', 'religion', 'reviews', 'romance', 'science_fiction'])\n",
    "    \n",
    "    #append hand-tagged cooking sentences to the front of the training data\n",
    "    all_tagged_sents = additional_sents + brown_tagged_sents\n",
    "    return train_tagger(all_tagged_sents)\n",
    "\n",
    "brown_and_additional_tagger = train_tagger_on_brown_augmented_with_cooking_sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "better_sentences = get_pos(testmeta1, brown_and_additional_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('the', 'AT'),\n",
       "  ('snow', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('white', 'JJ'),\n",
       "  ('blanket', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('hospital', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('refrigerator', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('classroom', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('zoo', 'NN')],\n",
       " [('america', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('melting', 'VBG'),\n",
       "  ('pot', 'NN')],\n",
       " [('her', 'PP$'),\n",
       "  ('lovely', 'JJ'),\n",
       "  ('voice', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('music', 'NN'),\n",
       "  ('to', 'IN'),\n",
       "  ('his', 'PP$'),\n",
       "  ('ears', 'NNS')],\n",
       " [('life', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('roller', 'NN'),\n",
       "  ('coaster', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('alligator', 'NN'),\n",
       "  ('s', 'NN'),\n",
       "  ('teeth', 'NNS'),\n",
       "  ('are', 'BER'),\n",
       "  ('white', 'JJ'),\n",
       "  ('daggers', 'NN')],\n",
       " [('their', 'PP$'),\n",
       "  ('home', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('prison', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('slide', 'NN'),\n",
       "  ('on', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('playground', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('hot', 'JJ'),\n",
       "  ('stove', 'NN')],\n",
       " [('his', 'PP$'),\n",
       "  ('heart', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('cold', 'JJ'),\n",
       "  ('iron', 'NN')],\n",
       " [('she', 'PPS'), ('is', 'BEZ'), ('a', 'AT'), ('peacock', 'NN')],\n",
       " [('he', 'PPS'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('shining', 'VBG'),\n",
       "  ('star', 'NN')],\n",
       " [('time', 'NN'), ('is', 'BEZ'), ('money', 'NN')],\n",
       " [('my', 'PP$'),\n",
       "  ('teacher', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('dragon', 'NN')],\n",
       " [('tom', 'NN'), ('s', 'NN'), ('eyes', 'NNS'), ('were', 'BED'), ('ice', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('detective', 'NN'),\n",
       "  ('s', 'NN'),\n",
       "  ('face', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('wood', 'NN'),\n",
       "  ('as', 'CS'),\n",
       "  ('he', 'PPS'),\n",
       "  ('listened', 'VBD'),\n",
       "  ('to', 'TO'),\n",
       "  ('her', 'PPO'),\n",
       "  ('story', 'NN')],\n",
       " [('she', 'PPS'),\n",
       "  ('feels', 'VBZ'),\n",
       "  ('that', 'CS'),\n",
       "  ('life', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('fashion', 'NN'),\n",
       "  ('show', 'NN')],\n",
       " [('the', 'AT'), ('world', 'NN'), ('is', 'BEZ'), ('a', 'AT'), ('stage', 'NN')],\n",
       " [('my', 'PP$'),\n",
       "  ('kid', 'NN'),\n",
       "  ('s', 'NN'),\n",
       "  ('room', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('disaster', 'NN'),\n",
       "  ('area', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('children', 'NNS'),\n",
       "  ('were', 'BED'),\n",
       "  ('flowers', 'NNS'),\n",
       "  ('grown', 'VBN'),\n",
       "  ('in', 'IN'),\n",
       "  ('concrete', 'NN'),\n",
       "  ('gardens', 'NNS')],\n",
       " [('kisses', 'NNS'),\n",
       "  ('are', 'BER'),\n",
       "  ('the', 'AT'),\n",
       "  ('flowers', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('affection', 'NN')],\n",
       " [('his', 'PP$'),\n",
       "  ('words', 'NNS'),\n",
       "  ('were', 'BED'),\n",
       "  ('cotton', 'NN'),\n",
       "  ('candy', 'NN')],\n",
       " [('mary', 'NN'),\n",
       "  ('s', 'NN'),\n",
       "  ('eyes', 'NNS'),\n",
       "  ('were', 'BED'),\n",
       "  ('fireflies', 'NN')],\n",
       " [('john', 'NN'),\n",
       "  ('s', 'NN'),\n",
       "  ('suggestion', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('just', 'RB'),\n",
       "  ('a', 'AT'),\n",
       "  ('band', 'NN'),\n",
       "  ('aid', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('cast', 'NN'),\n",
       "  ('on', 'IN'),\n",
       "  ('his', 'PP$'),\n",
       "  ('broken', 'VBN'),\n",
       "  ('leg', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('plaster', 'NN'),\n",
       "  ('shackle', 'NN')],\n",
       " [('jane', 'NN'),\n",
       "  ('s', 'NN'),\n",
       "  ('ambitions', 'NNS'),\n",
       "  ('are', 'BER'),\n",
       "  ('a', 'AT'),\n",
       "  ('house', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('cards', 'NNS')],\n",
       " [('her', 'PP$'),\n",
       "  ('long', 'JJ'),\n",
       "  ('hair', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('flowing', 'VBG'),\n",
       "  ('golden', 'JJ'),\n",
       "  ('river', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('computers', 'NNS'),\n",
       "  ('at', 'IN'),\n",
       "  ('school', 'NN'),\n",
       "  ('are', 'BER'),\n",
       "  ('old', 'JJ'),\n",
       "  ('dinosaurs', 'NNS')],\n",
       " [('laughter', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('the', 'AT'),\n",
       "  ('music', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('soul', 'NN')],\n",
       " [('he', 'PPS'), ('is', 'BEZ'), ('a', 'AT'), ('night', 'NN'), ('owl', 'NN')],\n",
       " [('maria', 'FW-NNS'), ('is', 'BEZ'), ('a', 'AT'), ('chicken', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('falling', 'VBG'),\n",
       "  ('snowflakes', 'NNS'),\n",
       "  ('are', 'BER'),\n",
       "  ('dancers', 'NNS')],\n",
       " [('with', 'IN'),\n",
       "  ('his', 'PP$'),\n",
       "  ('new', 'JJ'),\n",
       "  ('haircut', 'NN'),\n",
       "  ('he', 'PPS'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('sheepdog', 'NN')],\n",
       " [('at', 'IN'),\n",
       "  ('five', 'CD'),\n",
       "  ('o', 'NN'),\n",
       "  ('clock', 'NN'),\n",
       "  ('the', 'AT'),\n",
       "  ('interstate', 'JJ'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('parking', 'VBG'),\n",
       "  ('lot', 'NN')],\n",
       " [('books', 'NNS'),\n",
       "  ('are', 'BER'),\n",
       "  ('keys', 'NNS'),\n",
       "  ('to', 'TO'),\n",
       "  ('your', 'PP$'),\n",
       "  ('imagination', 'NN')],\n",
       " [('her', 'PP$'),\n",
       "  ('teddy', 'NN'),\n",
       "  ('bear', 'VB'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('her', 'PP$'),\n",
       "  ('best', 'JJT'),\n",
       "  ('friend', 'NN'),\n",
       "  ('never', 'RB'),\n",
       "  ('telling', 'VBG'),\n",
       "  ('her', 'PP$'),\n",
       "  ('secrets', 'NNS')],\n",
       " [('the', 'AT'),\n",
       "  ('peaceful', 'JJ'),\n",
       "  ('lake', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('mirror', 'NN')],\n",
       " [('terry', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('blue', 'JJ'),\n",
       "  ('when', 'WRB'),\n",
       "  ('his', 'PP$'),\n",
       "  ('goldfish', 'NN'),\n",
       "  ('died', 'VBD')],\n",
       " [('the', 'AT'),\n",
       "  ('wind', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('an', 'AT'),\n",
       "  ('angry', 'JJ'),\n",
       "  ('witch', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('ballerina', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('swan', 'NN'),\n",
       "  ('gliding', 'NN'),\n",
       "  ('across', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('stage', 'NN')],\n",
       " [('her', 'PP$'),\n",
       "  ('angry', 'JJ'),\n",
       "  ('words', 'NNS'),\n",
       "  ('were', 'BED'),\n",
       "  ('bullets', 'NNS'),\n",
       "  ('to', 'TO'),\n",
       "  ('him', 'PPO')],\n",
       " [('your', 'PP$'),\n",
       "  ('brain', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('computer', 'NN')],\n",
       " [('jamal', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('pig', 'NN'),\n",
       "  ('at', 'IN'),\n",
       "  ('dinner', 'NN')],\n",
       " [('you', 'PPSS'), ('are', 'BER'), ('my', 'PP$'), ('sunshine', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('car', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('furnace', 'NN'),\n",
       "  ('in', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('sun', 'NN')],\n",
       " [('thank', 'VB'),\n",
       "  ('you', 'PPO'),\n",
       "  ('so', 'QL'),\n",
       "  ('much', 'AP'),\n",
       "  ('you', 'PPSS'),\n",
       "  ('are', 'BER'),\n",
       "  ('an', 'AT'),\n",
       "  ('angel', 'NN')],\n",
       " [('that', 'DT'),\n",
       "  ('coach', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('an', 'AT'),\n",
       "  ('ogre', 'NN')],\n",
       " [('ben', 'NN'),\n",
       "  ('s', 'NN'),\n",
       "  ('temper', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('volcano', 'NN'),\n",
       "  ('ready', 'JJ'),\n",
       "  ('to', 'TO'),\n",
       "  ('explode', 'VB')],\n",
       " [('the', 'AT'),\n",
       "  ('kids', 'NNS'),\n",
       "  ('were', 'BED'),\n",
       "  ('monkeys', 'NNS'),\n",
       "  ('on', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('jungle', 'NN'),\n",
       "  ('gym', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('sun', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('golden', 'JJ'),\n",
       "  ('ball', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('clouds', 'NNS'),\n",
       "  ('are', 'BER'),\n",
       "  ('balls', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('cotton', 'NN')],\n",
       " [('sue', 'VB'),\n",
       "  ('s', 'NN'),\n",
       "  ('room', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('zoo', 'NN'),\n",
       "  ('with', 'IN'),\n",
       "  ('fish', 'NN'),\n",
       "  ('a', 'AT'),\n",
       "  ('gerbil', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('a', 'AT'),\n",
       "  ('parakeet', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('park', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('lake', 'NN'),\n",
       "  ('after', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('rain', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('lightning', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('fireworks', 'NNS'),\n",
       "  ('in', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('sky', 'NN')],\n",
       " [('gary', 'NN'), ('is', 'BEZ'), ('a', 'AT'), ('mule', 'NN')],\n",
       " [('that', 'DT'),\n",
       "  ('lawn', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('green', 'JJ'),\n",
       "  ('carpet', 'NN')],\n",
       " [('my', 'PP$'),\n",
       "  ('dad', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('road', 'NN'),\n",
       "  ('hog', 'NNS')],\n",
       " [('the', 'AT'),\n",
       "  ('stars', 'NNS'),\n",
       "  ('are', 'BER'),\n",
       "  ('sparkling', 'VBG'),\n",
       "  ('diamonds', 'NNS')],\n",
       " [('those', 'DTS'),\n",
       "  ('two', 'CD'),\n",
       "  ('best', 'JJT'),\n",
       "  ('friends', 'NNS'),\n",
       "  ('are', 'BER'),\n",
       "  ('two', 'CD'),\n",
       "  ('peas', 'NNS'),\n",
       "  ('in', 'IN'),\n",
       "  ('a', 'AT'),\n",
       "  ('pod', 'NN')],\n",
       " [('he', 'PPS'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('walking', 'VBG'),\n",
       "  ('dictionary', 'NN')],\n",
       " [('donations', 'NN'),\n",
       "  ('for', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('popular', 'JJ'),\n",
       "  ('charity', 'NN'),\n",
       "  ('were', 'BED'),\n",
       "  ('a', 'AT'),\n",
       "  ('tsunami', 'NN')],\n",
       " [('necessity', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('the', 'AT'),\n",
       "  ('mother', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('invention', 'NN')],\n",
       " [('my', 'PP$'),\n",
       "  ('big', 'JJ'),\n",
       "  ('brother', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('couch', 'NN'),\n",
       "  ('potato', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('road', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('ribbon', 'NN'),\n",
       "  ('stretching', 'VBG'),\n",
       "  ('across', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('desert', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('teenager', 'NN'),\n",
       "  ('s', 'NN'),\n",
       "  ('stomach', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('bottomless', 'JJ'),\n",
       "  ('pit', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('thunder', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('mighty', 'JJ'),\n",
       "  ('lion', 'NN')],\n",
       " [('i', 'NN'),\n",
       "  ('am', 'BEM'),\n",
       "  ('so', 'QL'),\n",
       "  ('excited', 'VBN'),\n",
       "  ('my', 'PP$'),\n",
       "  ('pulse', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('race', 'NN'),\n",
       "  ('car', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('moon', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('white', 'JJ'),\n",
       "  ('balloon', 'NN')],\n",
       " [('toddlers', 'NNS'), ('are', 'BER'), ('rug', 'NN'), ('rats', 'NNS')],\n",
       " [('the', 'AT'),\n",
       "  ('stormy', 'JJ'),\n",
       "  ('ocean', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('raging', 'VBG'),\n",
       "  ('bull', 'NN')],\n",
       " [('her', 'PP$'),\n",
       "  ('tears', 'NNS'),\n",
       "  ('were', 'BED'),\n",
       "  ('a', 'AT'),\n",
       "  ('river', 'NN'),\n",
       "  ('flowing', 'VBG'),\n",
       "  ('down', 'RP'),\n",
       "  ('her', 'PP$'),\n",
       "  ('cheeks', 'NNS')]]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"test_meta_sent.txt\", \"r\") as testset:\n",
    "    filtered_words = []\n",
    "    testmeta = []\n",
    "    testmeta1 = []\n",
    "    word_list = ''\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    for line in testset:\n",
    "        word_list = line.split()\n",
    "#         filtered_words = ' '.join([word.lower() for word in word_list if word not in stopwords.words('english')])\n",
    "        filtered_words = ' '.join([word.lower() for word in word_list])\n",
    "        testmeta1.append(tokenizer.tokenize(filtered_words))\n",
    "testmeta1[0]\n",
    "\n",
    "def get_pos(sents, tagger):        \n",
    "    return [tagger.tag(sent) for sent in sents]\n",
    "\n",
    "pos = get_pos(testmeta1, brown_tagger)\n",
    "pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Switch to Classification Features with NLTK Classification Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import nltk\n",
    "from nltk.corpus import names\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('an_mets.txt', 'r')\n",
    "x = f.readlines()\n",
    "an_mets = [t.rstrip() for t in x[1::]]\n",
    "f = open('an_nonmets.txt', 'r')\n",
    "x = f.readlines()\n",
    "an_nonmets = [t.rstrip() for t in x[1::]]\n",
    "f = open('svo_mets.txt', 'r')\n",
    "x = f.readlines()\n",
    "svo_mets = [t.rstrip() for t in x[1::]]\n",
    "f = open('svo_nonmets.txt', 'r')\n",
    "x = f.readlines()\n",
    "svo_nonmets = [t.rstrip() for t in x[1::]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'POS': 'm'}\n"
     ]
    }
   ],
   "source": [
    "# def double_letter(word):\n",
    "#     letter_list = []\n",
    "#     for letter in word:\n",
    "#         if word.count(letter) > 1:\n",
    "#             letter_list.append(True)\n",
    "#         else:\n",
    "#             letter_list.append(False)\n",
    "#     if True in letter_list:\n",
    "#         return True\n",
    "#     else:\n",
    "#         return False\n",
    "    \n",
    "def metaphor_features(word):\n",
    "    features = {}\n",
    "    word = word.lower()\n",
    "    features['POS'] = word[0]\n",
    "#     features['last'] = word[-1]\n",
    "#     features['last 2'] = word[-2]\n",
    "#     features['first 3'] = word[:3]\n",
    "#     features['first'] = word[:1]\n",
    "#     features['length'] = len(word)\n",
    "#     features['starts with K'] = word.startswith('k')\n",
    "#     features['ends with i'] = word.endswith('i')\n",
    "#     features['ends with a'] = word.endswith('a')\n",
    "#     features['double letter'] = double_letter(word)\n",
    "    return features\n",
    "print(str(metaphor_features('Michelle')))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_an_data():\n",
    "    an_mets_tag = [(re.sub(r'[^\\w\\s]','',line.lower()), 'metaphor') for line in an_mets]\n",
    "    an_nonmets_tag = [(re.sub(r'[^\\w\\s]','',line.lower()), 'NOT metaphor') for line in an_nonmets]\n",
    "    svo_mets_tag = [(re.sub(r'[^\\w\\s]','',line.lower()), 'metaphor') for line in svo_mets]\n",
    "    svo_nonmets_tag = [(re.sub(r'[^\\w\\s]','',line.lower()), 'NOT metaphor') for line in svo_nonmets]\n",
    "    all_an = an_mets_tag + an_nonmets_tag + svo_mets_tag + svo_nonmets_tag\n",
    "    \n",
    "    # Randomize the order of male and female names, and de-alphabatize\n",
    "    random.shuffle(all_an)\n",
    "    return all_an\n",
    "\n",
    "all_an_data = create_an_data()\n",
    "# print(all_an_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function allows experimentation with different feature definitions\n",
    "# items is a list of (key, value) pairs from which features are extracted and training sets are made\n",
    "# Feature sets returned are dictionaries of features\n",
    "\n",
    "# This function also optionally returns the names of the training, development, \n",
    "# and test data for the purposes of error checking\n",
    "\n",
    "def create_training_sets (feature_function, items, return_items=False):\n",
    "    # Create the features sets.  Call the function that was passed in.\n",
    "    # For names data, key is the name, and value is the gender\n",
    "    featuresets = [(feature_function(key), value) for (key, value) in items]\n",
    "    \n",
    "    # Divided training and testing in thirds.  Could divide in other proportions instead.\n",
    "    third = int(float(len(featuresets)) / 3.0)\n",
    "    \n",
    "    train_set, dev_set, test_set = featuresets[0:third], featuresets[third:third*2], featuresets[third*2:]\n",
    "    train_items, dev_items, test_items = items[0:third], items[third:third*2], items[third*2:]\n",
    "    if return_items == True:\n",
    "        return train_set, dev_set, test_set, train_items, dev_items, test_items\n",
    "    else:\n",
    "        return train_set, dev_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# note that this passes in a function name as an argument (gender_features)\n",
    "\n",
    "train_set, dev_set, test_set = create_training_sets(metaphor_features, all_an_data)\n",
    "cl = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'white blanket': NOT metaphor\n",
      "'snow white': metaphor\n",
      "'white as snow': NOT metaphor\n",
      "'happy customer': NOT metaphor\n",
      "'boiling rage': NOT metaphor\n"
     ]
    }
   ],
   "source": [
    "print (\"'white blanket': \" + cl.classify(metaphor_features('white blanket')))\n",
    "print (\"'snow white': \" + cl.classify(metaphor_features('snow white')))\n",
    "print (\"'white as snow': \" + cl.classify(metaphor_features('white as snow')))\n",
    "print (\"'happy customer': \" + cl.classify(metaphor_features('happy customer')))\n",
    "print (\"'boiling rage': \" + cl.classify(metaphor_features('boiling rage')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one specific aspect: NOT metaphor\n"
     ]
    }
   ],
   "source": [
    "testsample = 'one specific aspect'\n",
    "print (testsample + \": \" + cl.classify(metaphor_features(testsample)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.464\n",
      "Most Informative Features\n",
      "                     POS = 'n'            NOT me : metaph =      3.3 : 1.0\n",
      "                     POS = 'a'            metaph : NOT me =      3.3 : 1.0\n",
      "                     POS = 'f'            metaph : NOT me =      2.9 : 1.0\n",
      "                     POS = 'c'            metaph : NOT me =      2.1 : 1.0\n",
      "                     POS = 'p'            NOT me : metaph =      1.9 : 1.0\n",
      "                     POS = 'h'            NOT me : metaph =      1.3 : 1.0\n",
      "                     POS = 'm'            NOT me : metaph =      1.3 : 1.0\n",
      "                     POS = 's'            metaph : NOT me =      1.2 : 1.0\n",
      "                     POS = 'w'            NOT me : metaph =      1.2 : 1.0\n",
      "                     POS = 't'            NOT me : metaph =      1.1 : 1.0\n",
      "                     POS = 'i'            metaph : NOT me =      1.1 : 1.0\n",
      "                     POS = 'r'            metaph : NOT me =      1.1 : 1.0\n",
      "                     POS = 'b'            metaph : NOT me =      1.1 : 1.0\n",
      "                     POS = 'g'            metaph : NOT me =      1.1 : 1.0\n"
     ]
    }
   ],
   "source": [
    "print (\"%.3f\" % nltk.classify.accuracy(cl, dev_set))\n",
    "cl.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['scientists created atlas', 'created atlas brain', 'atlas brain reveals', 'brain reveals meanings', 'reveals meanings words', 'meanings words arranged', 'words arranged across', 'arranged across different', 'across different regions', 'different regions organ'], ['like colourful quilt', 'colourful quilt laid', 'quilt laid cortex', 'laid cortex atlas', 'cortex atlas displays', 'atlas displays rainbow', 'displays rainbow hues', 'rainbow hues individual', 'hues individual words', 'individual words concepts', 'words concepts convey', 'concepts convey grouped', 'convey grouped together', 'grouped together clumps', 'together clumps white', 'clumps white matter'], ['our goal build', 'goal build giant', 'build giant atlas', 'giant atlas shows', 'atlas shows one', 'shows one specific', 'one specific aspect', 'specific aspect language', 'aspect language represented', 'language represented brain', 'represented brain case', 'brain case semantics', 'case semantics meanings', 'semantics meanings words', 'meanings words said', 'words said jack', 'said jack gallant', 'jack gallant neuroscientist', 'gallant neuroscientist university', 'neuroscientist university california', 'university california berkeley'], ['single brain region', 'brain region holds', 'region holds one', 'holds one word', 'one word concept'], ['single brain spot', 'brain spot associated', 'spot associated number', 'associated number related', 'number related words'], ['single word lights', 'word lights many', 'lights many different', 'many different brain', 'different brain spots'], ['together make networks', 'make networks represent', 'networks represent meanings', 'represent meanings word', 'meanings word use', 'word use life', 'use life love', 'life love death', 'love death taxes', 'death taxes clouds', 'taxes clouds florida', 'clouds florida bra'], [], ['described tour de', 'tour de force', 'de force one', 'force one researcher', 'one researcher involved', 'researcher involved study', 'involved study atlas', 'study atlas demonstrates', 'atlas demonstrates modern', 'demonstrates modern imaging', 'modern imaging transform', 'imaging transform knowledge', 'transform knowledge brain', 'knowledge brain performs', 'brain performs important', 'performs important tasks'], ['advances technology could', 'technology could profound', 'could profound impact', 'profound impact medicine', 'impact medicine fields'], ['it possible approach', 'possible approach could', 'approach could used', 'could used decode', 'used decode information', 'decode information words', 'information words person', 'words person hearing', 'person hearing reading', 'hearing reading possibly', 'reading possibly even', 'possibly even thinking', 'even thinking said', 'thinking said alexander', 'said alexander huth', 'alexander huth first', 'huth first author', 'first author study'], ['one potential use', 'potential use would', 'use would language', 'would language decoder', 'language decoder could', 'decoder could allow', 'could allow people', 'allow people silenced', 'people silenced motor', 'silenced motor neurone', 'motor neurone disease', 'neurone disease lockedin', 'disease lockedin syndrome', 'lockedin syndrome speak', 'syndrome speak computer'], ['create atlas scientists', 'atlas scientists recorded', 'scientists recorded peoples', 'recorded peoples brain', 'peoples brain activity', 'brain activity listened', 'activity listened stories', 'listened stories read', 'stories read moth', 'read moth radio', 'moth radio hour', 'radio hour us', 'hour us radio', 'us radio show'], ['matched transcripts stories', 'transcripts stories brain', 'stories brain activity', 'brain activity data', 'activity data show', 'data show groups', 'show groups related', 'groups related words', 'related words triggered', 'words triggered neural', 'triggered neural responses', 'neural responses 50000', 'responses 50000 80000', '50000 80000 peasized', '80000 peasized spots', 'peasized spots cerebral', 'spots cerebral cortex'], ['huth used stories', 'used stories moth', 'stories moth radio', 'moth radio hour', 'radio hour short', 'hour short compelling'], ['enthralling stories confident', 'stories confident scientists', 'confident scientists could', 'scientists could people', 'could people scanned', 'people scanned focusing', 'scanned focusing words', 'focusing words drifting', 'words drifting off'], ['seven people listened', 'people listened two', 'listened two hours', 'two hours stories', 'hours stories each'], ['per person amounted', 'person amounted hearing', 'amounted hearing roughly', 'hearing roughly 25000', 'roughly 25000 words', '25000 words 3000', 'words 3000 different', '3000 different words', 'different words ', 'words  lay', ' lay scanner'], ['atlas shows words', 'shows words related', 'words related terms', 'related terms exercise', 'terms exercise regions', 'exercise regions brain'], ['example lefthand side', 'lefthand side brain', 'side brain ear', 'brain ear one', 'ear one tiny', 'one tiny regions', 'tiny regions represents', 'regions represents word', 'represents word victim'], ['region responds killed', 'responds killed convicted', 'killed convicted murdered', 'convicted murdered confessed'], ['brains righthand side', 'righthand side near', 'side near top', 'near top head', 'top head one', 'head one brain', 'one brain spots', 'brain spots activated', 'spots activated family', 'activated family terms', 'family terms wife', 'terms wife husband', 'wife husband children', 'husband children parents'], ['word represented one', 'represented one spot', 'one spot words', 'spot words tend', 'words tend several', 'tend several meanings'], ['one part brain', 'part brain example', 'brain example reliably', 'example reliably responds', 'reliably responds word', 'responds word top', 'word top along', 'top along words', 'along words describe', 'words describe clothing'], ['word top activates', 'top activates many', 'activates many regions'], ['one responds numbers', 'responds numbers measurements', 'numbers measurements another', 'measurements another buildings', 'another buildings places'], ['scientists created interactive', 'created interactive website', 'interactive website public', 'website public explore', 'public explore brain', 'explore brain atlas'], ['strikingly brain atlases', 'brain atlases similar', 'atlases similar participants', 'similar participants suggesting', 'participants suggesting brains', 'suggesting brains organised', 'brains organised meanings', 'organised meanings words', 'meanings words way'], ['scientists scanned five', 'scanned five men', 'five men two', 'men two women', 'two women however'], ['native english speakers', 'english speakers two', 'speakers two authors', 'two authors study', 'authors study published', 'study published nature'], ['highly possible people', 'possible people different', 'people different backgrounds', 'different backgrounds cultures', 'backgrounds cultures different', 'cultures different semantic', 'different semantic brain', 'semantic brain atlases'], ['armed atlas researchers', 'atlas researchers piece', 'researchers piece together', 'piece together brain', 'together brain networks', 'brain networks represent', 'networks represent wildly', 'represent wildly different', 'wildly different concepts', 'different concepts numbers', 'concepts numbers murder', 'numbers murder religion'], ['the idea murder', 'idea murder represented', 'murder represented lot', 'represented lot brain', 'lot brain gallant', 'brain gallant said'], ['using haul data', 'haul data group', 'data group begun', 'group begun work', 'begun work new', 'work new atlases', 'new atlases show', 'atlases show brain', 'show brain holds', 'brain holds information', 'holds information aspects', 'information aspects language', 'aspects language phonemes', 'language phonemes syntax'], ['brain atlas narrative', 'atlas narrative structure', 'narrative structure far', 'structure far proved', 'far proved elusive', 'proved elusive however'], ['every time come', 'time come set', 'come set narrative', 'set narrative features', 'narrative features get', 'features get told', 'get told arent', 'told arent right', 'arent right set', 'right set narrative', 'set narrative features', 'narrative features said', 'features said gallant'], ['uri hasson neuroscientist', 'hasson neuroscientist princeton', 'neuroscientist princeton university', 'princeton university praised', 'university praised work'], ['unlike many studies', 'many studies looked', 'studies looked brain', 'looked brain activity', 'brain activity isolated', 'activity isolated word', 'isolated word sentence', 'word sentence spoken', 'sentence spoken gallants', 'spoken gallants team', 'gallants team shed', 'team shed light', 'shed light brain', 'light brain worked', 'brain worked realworld', 'worked realworld scenario', 'realworld scenario said'], ['next step said', 'step said create', 'said create comprehensive', 'create comprehensive precise', 'comprehensive precise semantic', 'precise semantic brain', 'semantic brain atlas'], ['ultimately hasson believes', 'hasson believes possible', 'believes possible reconstruct', 'possible reconstruct words', 'reconstruct words person', 'words person thinking', 'person thinking brain', 'thinking brain activity'], ['ethical implications enormous'], ['one benign use', 'benign use would', 'use would see', 'would see brain', 'see brain activity', 'brain activity used', 'activity used assess', 'used assess whether', 'assess whether political', 'whether political messages', 'political messages effectively', 'messages effectively communicated', 'effectively communicated public'], ['there many implications', 'many implications barely', 'implications barely touching', 'barely touching surface', 'touching surface said'], ['lorraine tyler cognitive', 'tyler cognitive neuroscientist', 'cognitive neuroscientist head', 'neuroscientist head centre', 'head centre speech', 'centre speech language', 'speech language brain', 'language brain cambridge', 'brain cambridge university', 'cambridge university said', 'university said research', 'said research tour', 'research tour de', 'tour de force', 'de force scope', 'force scope methods'], ['brain atlas current', 'atlas current form', 'current form capture', 'form capture fine', 'capture fine differences', 'fine differences word', 'differences word meanings', 'word meanings take', 'meanings take word', 'take word table'], ['member many different', 'many different groups', 'different groups says', 'groups says tyler'], ['it something eat', 'something eat off', 'eat off things', 'off things made', 'things made wood', 'made wood things', 'wood things heavy', 'things heavy things', 'heavy things four', 'things four legs', 'four legs nonanimate', 'legs nonanimate objects', 'nonanimate objects on'], ['kind detailed semantic', 'detailed semantic information', 'semantic information enables', 'information enables words', 'enables words used', 'words used flexibly', 'used flexibly lost', 'flexibly lost analysis', 'lost analysis said'], ['while research pathbreaking', 'research pathbreaking scope', 'pathbreaking scope still', 'scope still lot', 'still lot learn', 'lot learn semantics', 'learn semantics represented', 'semantics represented brain']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(\"sciencearticle_line.txt\", \"r\") as testset:\n",
    "    filtered_words = []\n",
    "    testmeta = []\n",
    "    word_list = ''\n",
    "    for line in testset:\n",
    "        word_list = line.split()\n",
    "        filtered_words = [re.sub(r'[^\\w\\s]','',word.lower()) for word in word_list if word.lower() not in stopwords.words('english')]\n",
    "        testmeta.append([' '.join(x) for x in trigrams(filtered_words)])\n",
    "print(testmeta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['snow white blanket'], [], [], ['america melting pot'], ['lovely voice music', 'voice music ears'], ['life roller coaster'], ['alligators teeth white', 'teeth white daggers'], [], ['slide playground hot', 'playground hot stove'], ['heart cold iron'], [], [], [], [], ['toms eyes ice'], ['detectives face wood', 'face wood listened', 'wood listened story'], ['feels life fashion', 'life fashion show'], [], ['kids room disaster', 'room disaster area'], ['children flowers grown', 'flowers grown concrete', 'grown concrete gardens'], ['kisses flowers affection'], ['words cotton candy'], ['marys eyes fireflies'], ['johns suggestion bandaid'], ['cast broken leg', 'broken leg plaster', 'leg plaster shackle'], ['janes ambitions house', 'ambitions house cards'], ['long hair flowing', 'hair flowing golden', 'flowing golden river'], ['computers school old', 'school old dinosaurs'], ['laughter music soul'], [], [], ['falling snowflakes dancers'], ['new haircut sheepdog'], ['five oclock interstate', 'oclock interstate parking', 'interstate parking lot'], ['books keys imagination'], ['teddy bear best', 'bear best friend', 'best friend never', 'friend never telling', 'never telling secrets'], ['peaceful lake mirror'], ['terry blue goldfish', 'blue goldfish died'], ['wind angry witch'], ['ballerina swan gliding', 'swan gliding across', 'gliding across stage'], ['angry words bullets', 'words bullets him'], [], ['jamal pig dinner'], [], ['car furnace sun'], ['thank much angel'], [], ['bens temper volcano', 'temper volcano ready', 'volcano ready explode'], ['kids monkeys jungle', 'monkeys jungle gym'], ['sun golden ball'], ['clouds balls cotton'], ['sues room zoo', 'room zoo fish', 'zoo fish gerbil', 'fish gerbil parakeet'], ['park lake rain'], ['lightning fireworks sky'], [], ['lawn green carpet'], ['dad road hog'], ['stars sparkling diamonds'], ['two best friends', 'best friends two', 'friends two peas', 'two peas pod'], [], ['donations popular charity', 'popular charity tsunami'], ['necessity mother invention'], ['big brother couch', 'brother couch potato'], ['road ribbon stretching', 'ribbon stretching across', 'stretching across desert'], ['teenagers stomach bottomless', 'stomach bottomless pit'], ['thunder mighty lion'], ['excited pulse race', 'pulse race car'], ['moon white balloon'], ['toddlers rug rats'], ['stormy ocean raging', 'ocean raging bull'], ['tears river flowing', 'river flowing cheeks']]\n"
     ]
    }
   ],
   "source": [
    "with open(\"test_meta_sent.txt\", \"r\") as testset:\n",
    "    filtered_words = []\n",
    "    testmeta = []\n",
    "    word_list = ''\n",
    "    for line in testset:\n",
    "        word_list = line.split()\n",
    "        filtered_words = [re.sub(r'[^\\w\\s]','',word.lower()) for word in word_list if word.lower() not in stopwords.words('english')]\n",
    "        testmeta.append([' '.join(x) for x in trigrams(filtered_words)])\n",
    "print(testmeta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scientists created atlas: metaphor\n",
      "created atlas brain: metaphor\n",
      "atlas brain reveals: metaphor\n",
      "brain reveals meanings: NOT metaphor\n",
      "reveals meanings words: NOT metaphor\n",
      "meanings words arranged: NOT metaphor\n",
      "words arranged across: NOT metaphor\n",
      "arranged across different: metaphor\n",
      "across different regions: metaphor\n",
      "different regions organ: NOT metaphor\n",
      "like colourful quilt: NOT metaphor\n",
      "colourful quilt laid: metaphor\n",
      "quilt laid cortex: NOT metaphor\n",
      "laid cortex atlas: NOT metaphor\n",
      "cortex atlas displays: metaphor\n",
      "atlas displays rainbow: metaphor\n",
      "displays rainbow hues: NOT metaphor\n",
      "rainbow hues individual: NOT metaphor\n",
      "hues individual words: NOT metaphor\n",
      "individual words concepts: NOT metaphor\n",
      "words concepts convey: NOT metaphor\n",
      "concepts convey grouped: metaphor\n",
      "convey grouped together: metaphor\n",
      "grouped together clumps: NOT metaphor\n",
      "together clumps white: NOT metaphor\n",
      "clumps white matter: metaphor\n",
      "our goal build: NOT metaphor\n",
      "goal build giant: NOT metaphor\n",
      "build giant atlas: NOT metaphor\n",
      "giant atlas shows: NOT metaphor\n",
      "atlas shows one: metaphor\n",
      "shows one specific: metaphor\n",
      "one specific aspect: NOT metaphor\n",
      "specific aspect language: metaphor\n",
      "aspect language represented: metaphor\n",
      "language represented brain: NOT metaphor\n",
      "represented brain case: NOT metaphor\n",
      "brain case semantics: NOT metaphor\n",
      "case semantics meanings: metaphor\n",
      "semantics meanings words: metaphor\n",
      "meanings words said: NOT metaphor\n",
      "words said jack: NOT metaphor\n",
      "said jack gallant: metaphor\n",
      "jack gallant neuroscientist: NOT metaphor\n",
      "gallant neuroscientist university: NOT metaphor\n",
      "neuroscientist university california: NOT metaphor\n",
      "university california berkeley: metaphor\n",
      "single brain region: metaphor\n",
      "brain region holds: NOT metaphor\n",
      "region holds one: NOT metaphor\n",
      "holds one word: NOT metaphor\n",
      "one word concept: NOT metaphor\n",
      "single brain spot: metaphor\n",
      "brain spot associated: NOT metaphor\n",
      "spot associated number: metaphor\n",
      "associated number related: metaphor\n",
      "number related words: NOT metaphor\n",
      "single word lights: metaphor\n",
      "word lights many: NOT metaphor\n",
      "lights many different: NOT metaphor\n",
      "many different brain: NOT metaphor\n",
      "different brain spots: NOT metaphor\n",
      "together make networks: NOT metaphor\n",
      "make networks represent: NOT metaphor\n",
      "networks represent meanings: NOT metaphor\n",
      "represent meanings word: NOT metaphor\n",
      "meanings word use: NOT metaphor\n",
      "word use life: NOT metaphor\n",
      "use life love: metaphor\n",
      "life love death: NOT metaphor\n",
      "love death taxes: NOT metaphor\n",
      "death taxes clouds: NOT metaphor\n",
      "taxes clouds florida: NOT metaphor\n",
      "clouds florida bra: metaphor\n",
      "described tour de: NOT metaphor\n",
      "tour de force: NOT metaphor\n",
      "de force one: NOT metaphor\n",
      "force one researcher: metaphor\n",
      "one researcher involved: NOT metaphor\n",
      "researcher involved study: NOT metaphor\n",
      "involved study atlas: NOT metaphor\n",
      "study atlas demonstrates: metaphor\n",
      "atlas demonstrates modern: metaphor\n",
      "demonstrates modern imaging: NOT metaphor\n",
      "modern imaging transform: NOT metaphor\n",
      "imaging transform knowledge: NOT metaphor\n",
      "transform knowledge brain: NOT metaphor\n",
      "knowledge brain performs: NOT metaphor\n",
      "brain performs important: NOT metaphor\n",
      "performs important tasks: NOT metaphor\n",
      "advances technology could: metaphor\n",
      "technology could profound: NOT metaphor\n",
      "could profound impact: metaphor\n",
      "profound impact medicine: NOT metaphor\n",
      "impact medicine fields: NOT metaphor\n",
      "it possible approach: NOT metaphor\n",
      "possible approach could: NOT metaphor\n",
      "approach could used: metaphor\n",
      "could used decode: metaphor\n",
      "used decode information: metaphor\n",
      "decode information words: NOT metaphor\n",
      "information words person: NOT metaphor\n",
      "words person hearing: NOT metaphor\n",
      "person hearing reading: NOT metaphor\n",
      "hearing reading possibly: NOT metaphor\n",
      "reading possibly even: NOT metaphor\n",
      "possibly even thinking: NOT metaphor\n",
      "even thinking said: metaphor\n",
      "thinking said alexander: NOT metaphor\n",
      "said alexander huth: metaphor\n",
      "alexander huth first: metaphor\n",
      "huth first author: NOT metaphor\n",
      "first author study: metaphor\n",
      "one potential use: NOT metaphor\n",
      "potential use would: NOT metaphor\n",
      "use would language: metaphor\n",
      "would language decoder: NOT metaphor\n",
      "language decoder could: NOT metaphor\n",
      "decoder could allow: NOT metaphor\n",
      "could allow people: metaphor\n",
      "allow people silenced: metaphor\n",
      "people silenced motor: NOT metaphor\n",
      "silenced motor neurone: metaphor\n",
      "motor neurone disease: NOT metaphor\n",
      "neurone disease lockedin: NOT metaphor\n",
      "disease lockedin syndrome: NOT metaphor\n",
      "lockedin syndrome speak: NOT metaphor\n",
      "syndrome speak computer: metaphor\n",
      "create atlas scientists: metaphor\n",
      "atlas scientists recorded: metaphor\n",
      "scientists recorded peoples: metaphor\n",
      "recorded peoples brain: NOT metaphor\n",
      "peoples brain activity: NOT metaphor\n",
      "brain activity listened: NOT metaphor\n",
      "activity listened stories: metaphor\n",
      "listened stories read: NOT metaphor\n",
      "stories read moth: metaphor\n",
      "read moth radio: NOT metaphor\n",
      "moth radio hour: NOT metaphor\n",
      "radio hour us: NOT metaphor\n",
      "hour us radio: NOT metaphor\n",
      "us radio show: metaphor\n",
      "matched transcripts stories: NOT metaphor\n",
      "transcripts stories brain: NOT metaphor\n",
      "stories brain activity: metaphor\n",
      "brain activity data: NOT metaphor\n",
      "activity data show: metaphor\n",
      "data show groups: NOT metaphor\n",
      "show groups related: metaphor\n",
      "groups related words: NOT metaphor\n",
      "related words triggered: NOT metaphor\n",
      "words triggered neural: NOT metaphor\n",
      "triggered neural responses: NOT metaphor\n",
      "neural responses 50000: NOT metaphor\n",
      "responses 50000 80000: NOT metaphor\n",
      "50000 80000 peasized: NOT metaphor\n",
      "80000 peasized spots: NOT metaphor\n",
      "peasized spots cerebral: NOT metaphor\n",
      "spots cerebral cortex: metaphor\n",
      "huth used stories: NOT metaphor\n",
      "used stories moth: metaphor\n",
      "stories moth radio: metaphor\n",
      "moth radio hour: NOT metaphor\n",
      "radio hour short: NOT metaphor\n",
      "hour short compelling: NOT metaphor\n",
      "enthralling stories confident: metaphor\n",
      "stories confident scientists: metaphor\n",
      "confident scientists could: metaphor\n",
      "scientists could people: metaphor\n",
      "could people scanned: metaphor\n",
      "people scanned focusing: NOT metaphor\n",
      "scanned focusing words: metaphor\n",
      "focusing words drifting: metaphor\n",
      "words drifting off: NOT metaphor\n",
      "seven people listened: metaphor\n",
      "people listened two: NOT metaphor\n",
      "listened two hours: NOT metaphor\n",
      "two hours stories: NOT metaphor\n",
      "hours stories each: NOT metaphor\n",
      "per person amounted: NOT metaphor\n",
      "person amounted hearing: NOT metaphor\n",
      "amounted hearing roughly: metaphor\n",
      "hearing roughly 25000: NOT metaphor\n",
      "roughly 25000 words: NOT metaphor\n",
      "25000 words 3000: NOT metaphor\n",
      "words 3000 different: NOT metaphor\n",
      "3000 different words: NOT metaphor\n",
      "different words : NOT metaphor\n",
      "words  lay: NOT metaphor\n",
      " lay scanner: NOT metaphor\n",
      "atlas shows words: metaphor\n",
      "shows words related: metaphor\n",
      "words related terms: NOT metaphor\n",
      "related terms exercise: NOT metaphor\n",
      "terms exercise regions: NOT metaphor\n",
      "exercise regions brain: metaphor\n",
      "example lefthand side: metaphor\n",
      "lefthand side brain: NOT metaphor\n",
      "side brain ear: metaphor\n",
      "brain ear one: NOT metaphor\n",
      "ear one tiny: metaphor\n",
      "one tiny regions: NOT metaphor\n",
      "tiny regions represents: NOT metaphor\n",
      "regions represents word: NOT metaphor\n",
      "represents word victim: NOT metaphor\n",
      "region responds killed: NOT metaphor\n",
      "responds killed convicted: NOT metaphor\n",
      "killed convicted murdered: NOT metaphor\n",
      "convicted murdered confessed: metaphor\n",
      "brains righthand side: NOT metaphor\n",
      "righthand side near: NOT metaphor\n",
      "side near top: metaphor\n",
      "near top head: NOT metaphor\n",
      "top head one: NOT metaphor\n",
      "head one brain: NOT metaphor\n",
      "one brain spots: NOT metaphor\n",
      "brain spots activated: NOT metaphor\n",
      "spots activated family: metaphor\n",
      "activated family terms: metaphor\n",
      "family terms wife: metaphor\n",
      "terms wife husband: NOT metaphor\n",
      "wife husband children: NOT metaphor\n",
      "husband children parents: NOT metaphor\n",
      "word represented one: NOT metaphor\n",
      "represented one spot: NOT metaphor\n",
      "one spot words: NOT metaphor\n",
      "spot words tend: metaphor\n",
      "words tend several: NOT metaphor\n",
      "tend several meanings: NOT metaphor\n",
      "one part brain: NOT metaphor\n",
      "part brain example: NOT metaphor\n",
      "brain example reliably: NOT metaphor\n",
      "example reliably responds: metaphor\n",
      "reliably responds word: NOT metaphor\n",
      "responds word top: NOT metaphor\n",
      "word top along: NOT metaphor\n",
      "top along words: NOT metaphor\n",
      "along words describe: metaphor\n",
      "words describe clothing: NOT metaphor\n",
      "word top activates: NOT metaphor\n",
      "top activates many: NOT metaphor\n",
      "activates many regions: metaphor\n",
      "one responds numbers: NOT metaphor\n",
      "responds numbers measurements: NOT metaphor\n",
      "numbers measurements another: NOT metaphor\n",
      "measurements another buildings: NOT metaphor\n",
      "another buildings places: metaphor\n",
      "scientists created interactive: metaphor\n",
      "created interactive website: metaphor\n",
      "interactive website public: NOT metaphor\n",
      "website public explore: NOT metaphor\n",
      "public explore brain: NOT metaphor\n",
      "explore brain atlas: metaphor\n",
      "strikingly brain atlases: metaphor\n",
      "brain atlases similar: NOT metaphor\n",
      "atlases similar participants: metaphor\n",
      "similar participants suggesting: metaphor\n",
      "participants suggesting brains: NOT metaphor\n",
      "suggesting brains organised: metaphor\n",
      "brains organised meanings: NOT metaphor\n",
      "organised meanings words: NOT metaphor\n",
      "meanings words way: NOT metaphor\n",
      "scientists scanned five: metaphor\n",
      "scanned five men: metaphor\n",
      "five men two: metaphor\n",
      "men two women: NOT metaphor\n",
      "two women however: NOT metaphor\n",
      "native english speakers: NOT metaphor\n",
      "english speakers two: metaphor\n",
      "speakers two authors: metaphor\n",
      "two authors study: NOT metaphor\n",
      "authors study published: metaphor\n",
      "study published nature: metaphor\n",
      "highly possible people: NOT metaphor\n",
      "possible people different: NOT metaphor\n",
      "people different backgrounds: NOT metaphor\n",
      "different backgrounds cultures: NOT metaphor\n",
      "backgrounds cultures different: NOT metaphor\n",
      "cultures different semantic: metaphor\n",
      "different semantic brain: NOT metaphor\n",
      "semantic brain atlases: metaphor\n",
      "armed atlas researchers: metaphor\n",
      "atlas researchers piece: metaphor\n",
      "researchers piece together: NOT metaphor\n",
      "piece together brain: NOT metaphor\n",
      "together brain networks: NOT metaphor\n",
      "brain networks represent: NOT metaphor\n",
      "networks represent wildly: NOT metaphor\n",
      "represent wildly different: NOT metaphor\n",
      "wildly different concepts: NOT metaphor\n",
      "different concepts numbers: NOT metaphor\n",
      "concepts numbers murder: metaphor\n",
      "numbers murder religion: NOT metaphor\n",
      "the idea murder: NOT metaphor\n",
      "idea murder represented: NOT metaphor\n",
      "murder represented lot: NOT metaphor\n",
      "represented lot brain: NOT metaphor\n",
      "lot brain gallant: NOT metaphor\n",
      "brain gallant said: NOT metaphor\n",
      "using haul data: metaphor\n",
      "haul data group: NOT metaphor\n",
      "data group begun: NOT metaphor\n",
      "group begun work: NOT metaphor\n",
      "begun work new: NOT metaphor\n",
      "work new atlases: NOT metaphor\n",
      "new atlases show: NOT metaphor\n",
      "atlases show brain: metaphor\n",
      "show brain holds: metaphor\n",
      "brain holds information: NOT metaphor\n",
      "holds information aspects: NOT metaphor\n",
      "information aspects language: NOT metaphor\n",
      "aspects language phonemes: metaphor\n",
      "language phonemes syntax: NOT metaphor\n",
      "brain atlas narrative: NOT metaphor\n",
      "atlas narrative structure: metaphor\n",
      "narrative structure far: NOT metaphor\n",
      "structure far proved: metaphor\n",
      "far proved elusive: metaphor\n",
      "proved elusive however: NOT metaphor\n",
      "every time come: metaphor\n",
      "time come set: NOT metaphor\n",
      "come set narrative: metaphor\n",
      "set narrative features: metaphor\n",
      "narrative features get: NOT metaphor\n",
      "features get told: metaphor\n",
      "get told arent: NOT metaphor\n",
      "told arent right: NOT metaphor\n",
      "arent right set: metaphor\n",
      "right set narrative: NOT metaphor\n",
      "set narrative features: metaphor\n",
      "narrative features said: NOT metaphor\n",
      "features said gallant: metaphor\n",
      "uri hasson neuroscientist: metaphor\n",
      "hasson neuroscientist princeton: NOT metaphor\n",
      "neuroscientist princeton university: NOT metaphor\n",
      "princeton university praised: NOT metaphor\n",
      "university praised work: metaphor\n",
      "unlike many studies: metaphor\n",
      "many studies looked: NOT metaphor\n",
      "studies looked brain: metaphor\n",
      "looked brain activity: NOT metaphor\n",
      "brain activity isolated: NOT metaphor\n",
      "activity isolated word: metaphor\n",
      "isolated word sentence: NOT metaphor\n",
      "word sentence spoken: NOT metaphor\n",
      "sentence spoken gallants: metaphor\n",
      "spoken gallants team: metaphor\n",
      "gallants team shed: NOT metaphor\n",
      "team shed light: NOT metaphor\n",
      "shed light brain: metaphor\n",
      "light brain worked: NOT metaphor\n",
      "brain worked realworld: NOT metaphor\n",
      "worked realworld scenario: NOT metaphor\n",
      "realworld scenario said: NOT metaphor\n",
      "next step said: NOT metaphor\n",
      "step said create: metaphor\n",
      "said create comprehensive: metaphor\n",
      "create comprehensive precise: metaphor\n",
      "comprehensive precise semantic: metaphor\n",
      "precise semantic brain: NOT metaphor\n",
      "semantic brain atlas: metaphor\n",
      "ultimately hasson believes: metaphor\n",
      "hasson believes possible: NOT metaphor\n",
      "believes possible reconstruct: NOT metaphor\n",
      "possible reconstruct words: NOT metaphor\n",
      "reconstruct words person: NOT metaphor\n",
      "words person thinking: NOT metaphor\n",
      "person thinking brain: NOT metaphor\n",
      "thinking brain activity: NOT metaphor\n",
      "ethical implications enormous: metaphor\n",
      "one benign use: NOT metaphor\n",
      "benign use would: NOT metaphor\n",
      "use would see: metaphor\n",
      "would see brain: NOT metaphor\n",
      "see brain activity: metaphor\n",
      "brain activity used: NOT metaphor\n",
      "activity used assess: metaphor\n",
      "used assess whether: metaphor\n",
      "assess whether political: metaphor\n",
      "whether political messages: NOT metaphor\n",
      "political messages effectively: NOT metaphor\n",
      "messages effectively communicated: NOT metaphor\n",
      "effectively communicated public: metaphor\n",
      "there many implications: NOT metaphor\n",
      "many implications barely: NOT metaphor\n",
      "implications barely touching: NOT metaphor\n",
      "barely touching surface: NOT metaphor\n",
      "touching surface said: NOT metaphor\n",
      "lorraine tyler cognitive: NOT metaphor\n",
      "tyler cognitive neuroscientist: NOT metaphor\n",
      "cognitive neuroscientist head: metaphor\n",
      "neuroscientist head centre: NOT metaphor\n",
      "head centre speech: NOT metaphor\n",
      "centre speech language: metaphor\n",
      "speech language brain: metaphor\n",
      "language brain cambridge: NOT metaphor\n",
      "brain cambridge university: NOT metaphor\n",
      "cambridge university said: metaphor\n",
      "university said research: metaphor\n",
      "said research tour: metaphor\n",
      "research tour de: NOT metaphor\n",
      "tour de force: NOT metaphor\n",
      "de force scope: NOT metaphor\n",
      "force scope methods: metaphor\n",
      "brain atlas current: NOT metaphor\n",
      "atlas current form: metaphor\n",
      "current form capture: metaphor\n",
      "form capture fine: metaphor\n",
      "capture fine differences: metaphor\n",
      "fine differences word: metaphor\n",
      "differences word meanings: NOT metaphor\n",
      "word meanings take: NOT metaphor\n",
      "meanings take word: NOT metaphor\n",
      "take word table: NOT metaphor\n",
      "member many different: NOT metaphor\n",
      "many different groups: NOT metaphor\n",
      "different groups says: NOT metaphor\n",
      "groups says tyler: NOT metaphor\n",
      "it something eat: NOT metaphor\n",
      "something eat off: metaphor\n",
      "eat off things: metaphor\n",
      "off things made: NOT metaphor\n",
      "things made wood: NOT metaphor\n",
      "made wood things: NOT metaphor\n",
      "wood things heavy: NOT metaphor\n",
      "things heavy things: NOT metaphor\n",
      "heavy things four: NOT metaphor\n",
      "things four legs: NOT metaphor\n",
      "four legs nonanimate: metaphor\n",
      "legs nonanimate objects: NOT metaphor\n",
      "nonanimate objects on: NOT metaphor\n",
      "kind detailed semantic: NOT metaphor\n",
      "detailed semantic information: NOT metaphor\n",
      "semantic information enables: metaphor\n",
      "information enables words: NOT metaphor\n",
      "enables words used: metaphor\n",
      "words used flexibly: NOT metaphor\n",
      "used flexibly lost: metaphor\n",
      "flexibly lost analysis: metaphor\n",
      "lost analysis said: NOT metaphor\n",
      "while research pathbreaking: NOT metaphor\n",
      "research pathbreaking scope: NOT metaphor\n",
      "pathbreaking scope still: NOT metaphor\n",
      "scope still lot: metaphor\n",
      "still lot learn: metaphor\n",
      "lot learn semantics: NOT metaphor\n",
      "learn semantics represented: NOT metaphor\n",
      "semantics represented brain: metaphor\n",
      "metaphor_count:156\n",
      "nonmetaphor_count:292\n"
     ]
    }
   ],
   "source": [
    "metaphor_count = 0\n",
    "nonmetaphor_count = 0\n",
    "for x in testmeta:\n",
    "    for y in x:\n",
    "        testsample = y\n",
    "        print (testsample + \": \" + cl.classify(metaphor_features(testsample)))\n",
    "        if cl.classify(metaphor_features(testsample)) == 'metaphor':\n",
    "            metaphor_count += 1\n",
    "        elif cl.classify(metaphor_features(testsample)) == 'NOT metaphor':\n",
    "            nonmetaphor_count += 1\n",
    "            \n",
    "print('metaphor_count:' + str(metaphor_count))\n",
    "print('nonmetaphor_count:' + str(nonmetaphor_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "scientists created atlas: metaphor\n",
      "created atlas brain: metaphor\n",
      "atlas brain reveals: metaphor\n",
      "brain reveals meanings: NOT metaphor\n",
      "reveals meanings words: NOT metaphor\n",
      "meanings words arranged: NOT metaphor\n",
      "words arranged across: NOT metaphor\n",
      "arranged across different: metaphor\n",
      "across different regions: metaphor\n",
      "different regions organ: NOT metaphor\n",
      "like colourful quilt: NOT metaphor\n",
      "colourful quilt laid: metaphor\n",
      "quilt laid cortex: NOT metaphor\n",
      "laid cortex atlas: NOT metaphor\n",
      "cortex atlas displays: metaphor\n",
      "atlas displays rainbow: metaphor\n",
      "displays rainbow hues: NOT metaphor\n",
      "rainbow hues individual: NOT metaphor\n",
      "hues individual words: NOT metaphor\n",
      "individual words concepts: NOT metaphor\n",
      "words concepts convey: NOT metaphor\n",
      "concepts convey grouped: metaphor\n",
      "convey grouped together: metaphor\n",
      "grouped together clumps: NOT metaphor\n",
      "together clumps white: NOT metaphor\n",
      "clumps white matter: metaphor\n",
      "our goal build: NOT metaphor\n",
      "goal build giant: NOT metaphor\n",
      "build giant atlas: NOT metaphor\n",
      "giant atlas shows: NOT metaphor\n",
      "atlas shows one: metaphor\n",
      "shows one specific: metaphor\n",
      "one specific aspect: NOT metaphor\n",
      "specific aspect language: metaphor\n",
      "aspect language represented: metaphor\n",
      "language represented brain: NOT metaphor\n",
      "represented brain case: NOT metaphor\n",
      "brain case semantics: NOT metaphor\n",
      "case semantics meanings: metaphor\n",
      "semantics meanings words: metaphor\n",
      "meanings words said: NOT metaphor\n",
      "words said jack: NOT metaphor\n",
      "said jack gallant: metaphor\n",
      "jack gallant neuroscientist: NOT metaphor\n",
      "gallant neuroscientist university: NOT metaphor\n",
      "neuroscientist university california: NOT metaphor\n",
      "university california berkeley: metaphor\n",
      "no single brain: NOT metaphor\n",
      "single brain region: metaphor\n",
      "brain region holds: NOT metaphor\n",
      "region holds one: NOT metaphor\n",
      "holds one word: NOT metaphor\n",
      "one word concept: NOT metaphor\n",
      "word concept a: NOT metaphor\n",
      "concept a single: metaphor\n",
      "a single brain: metaphor\n",
      "single brain spot: metaphor\n",
      "brain spot associated: NOT metaphor\n",
      "spot associated number: metaphor\n",
      "associated number related: metaphor\n",
      "number related words: NOT metaphor\n",
      "related words and: NOT metaphor\n",
      "words and single: NOT metaphor\n",
      "and single word: metaphor\n",
      "single word lights: metaphor\n",
      "word lights many: NOT metaphor\n",
      "lights many different: NOT metaphor\n",
      "many different brain: NOT metaphor\n",
      "different brain spots: NOT metaphor\n",
      "brain spots together: NOT metaphor\n",
      "spots together make: metaphor\n",
      "together make networks: NOT metaphor\n",
      "make networks represent: NOT metaphor\n",
      "networks represent meanings: NOT metaphor\n",
      "represent meanings word: NOT metaphor\n",
      "meanings word use: NOT metaphor\n",
      "word use life: NOT metaphor\n",
      "use life love: metaphor\n",
      "life love death: NOT metaphor\n",
      "love death taxes: NOT metaphor\n",
      "death taxes clouds: NOT metaphor\n",
      "taxes clouds florida: NOT metaphor\n",
      "clouds florida bra: metaphor\n",
      "florida bra all: metaphor\n",
      "bra all light: NOT metaphor\n",
      "all light networks: metaphor\n",
      "described tour de: NOT metaphor\n",
      "tour de force: NOT metaphor\n",
      "de force one: NOT metaphor\n",
      "force one researcher: metaphor\n",
      "one researcher involved: NOT metaphor\n",
      "researcher involved study: NOT metaphor\n",
      "involved study atlas: NOT metaphor\n",
      "study atlas demonstrates: metaphor\n",
      "atlas demonstrates modern: metaphor\n",
      "demonstrates modern imaging: NOT metaphor\n",
      "modern imaging transform: NOT metaphor\n",
      "imaging transform knowledge: NOT metaphor\n",
      "transform knowledge brain: NOT metaphor\n",
      "knowledge brain performs: NOT metaphor\n",
      "brain performs important: NOT metaphor\n",
      "performs important tasks: NOT metaphor\n",
      "important tasks with: NOT metaphor\n",
      "tasks with advances: NOT metaphor\n",
      "with advances technology: NOT metaphor\n",
      "advances technology could: metaphor\n",
      "technology could profound: NOT metaphor\n",
      "could profound impact: metaphor\n",
      "profound impact medicine: NOT metaphor\n",
      "impact medicine fields: NOT metaphor\n",
      "it possible approach: NOT metaphor\n",
      "possible approach could: NOT metaphor\n",
      "approach could used: metaphor\n",
      "could used decode: metaphor\n",
      "used decode information: metaphor\n",
      "decode information words: NOT metaphor\n",
      "information words person: NOT metaphor\n",
      "words person hearing: NOT metaphor\n",
      "person hearing reading: NOT metaphor\n",
      "hearing reading possibly: NOT metaphor\n",
      "reading possibly even: NOT metaphor\n",
      "possibly even thinking: NOT metaphor\n",
      "even thinking said: metaphor\n",
      "thinking said alexander: NOT metaphor\n",
      "said alexander huth: metaphor\n",
      "alexander huth first: metaphor\n",
      "huth first author: NOT metaphor\n",
      "first author study: metaphor\n",
      "author study one: metaphor\n",
      "study one potential: metaphor\n",
      "one potential use: NOT metaphor\n",
      "potential use would: NOT metaphor\n",
      "use would language: metaphor\n",
      "would language decoder: NOT metaphor\n",
      "language decoder could: NOT metaphor\n",
      "decoder could allow: NOT metaphor\n",
      "could allow people: metaphor\n",
      "allow people silenced: metaphor\n",
      "people silenced motor: NOT metaphor\n",
      "silenced motor neurone: metaphor\n",
      "motor neurone disease: NOT metaphor\n",
      "neurone disease lockedin: NOT metaphor\n",
      "disease lockedin syndrome: NOT metaphor\n",
      "lockedin syndrome speak: NOT metaphor\n",
      "syndrome speak computer: metaphor\n",
      "to create atlas: NOT metaphor\n",
      "create atlas scientists: metaphor\n",
      "atlas scientists recorded: metaphor\n",
      "scientists recorded peoples: metaphor\n",
      "recorded peoples brain: NOT metaphor\n",
      "peoples brain activity: NOT metaphor\n",
      "brain activity listened: NOT metaphor\n",
      "activity listened stories: metaphor\n",
      "listened stories read: NOT metaphor\n",
      "stories read the: metaphor\n",
      "read the moth: NOT metaphor\n",
      "the moth radio: NOT metaphor\n",
      "moth radio hour: NOT metaphor\n",
      "radio hour us: NOT metaphor\n",
      "hour us radio: NOT metaphor\n",
      "us radio show: metaphor\n",
      "radio show they: NOT metaphor\n",
      "show they matched: metaphor\n",
      "they matched transcripts: NOT metaphor\n",
      "matched transcripts stories: NOT metaphor\n",
      "transcripts stories brain: NOT metaphor\n",
      "stories brain activity: metaphor\n",
      "brain activity data: NOT metaphor\n",
      "activity data show: metaphor\n",
      "data show groups: NOT metaphor\n",
      "show groups related: metaphor\n",
      "groups related words: NOT metaphor\n",
      "related words triggered: NOT metaphor\n",
      "words triggered neural: NOT metaphor\n",
      "triggered neural responses: NOT metaphor\n",
      "neural responses 50000: NOT metaphor\n",
      "responses 50000 80000: NOT metaphor\n",
      "50000 80000 peasized: NOT metaphor\n",
      "80000 peasized spots: NOT metaphor\n",
      "peasized spots cerebral: NOT metaphor\n",
      "spots cerebral cortex: metaphor\n",
      "huth used stories: NOT metaphor\n",
      "used stories the: metaphor\n",
      "stories the moth: metaphor\n",
      "the moth radio: NOT metaphor\n",
      "moth radio hour: NOT metaphor\n",
      "radio hour short: NOT metaphor\n",
      "hour short compelling: NOT metaphor\n",
      "short compelling the: metaphor\n",
      "compelling the enthralling: metaphor\n",
      "the enthralling stories: NOT metaphor\n",
      "enthralling stories confident: metaphor\n",
      "stories confident scientists: metaphor\n",
      "confident scientists could: metaphor\n",
      "scientists could people: metaphor\n",
      "could people scanned: metaphor\n",
      "people scanned focusing: NOT metaphor\n",
      "scanned focusing words: metaphor\n",
      "focusing words drifting: metaphor\n",
      "words drifting off: NOT metaphor\n",
      "drifting off seven: NOT metaphor\n",
      "off seven people: NOT metaphor\n",
      "seven people listened: metaphor\n",
      "people listened two: NOT metaphor\n",
      "listened two hours: NOT metaphor\n",
      "two hours stories: NOT metaphor\n",
      "hours stories each: NOT metaphor\n",
      "stories each per: metaphor\n",
      "each per person: metaphor\n",
      "per person amounted: NOT metaphor\n",
      "person amounted hearing: NOT metaphor\n",
      "amounted hearing roughly: metaphor\n",
      "hearing roughly 25000: NOT metaphor\n",
      "roughly 25000 words: NOT metaphor\n",
      "25000 words 3000: NOT metaphor\n",
      "words 3000 different: NOT metaphor\n",
      "3000 different words: NOT metaphor\n",
      "different words : NOT metaphor\n",
      "words  lay: NOT metaphor\n",
      " lay scanner: NOT metaphor\n",
      "the atlas shows: NOT metaphor\n",
      "atlas shows words: metaphor\n",
      "shows words related: metaphor\n",
      "words related terms: NOT metaphor\n",
      "related terms exercise: NOT metaphor\n",
      "terms exercise regions: NOT metaphor\n",
      "exercise regions brain: metaphor\n",
      "regions brain for: NOT metaphor\n",
      "brain for example: NOT metaphor\n",
      "for example lefthand: metaphor\n",
      "example lefthand side: metaphor\n",
      "lefthand side brain: NOT metaphor\n",
      "side brain ear: metaphor\n",
      "brain ear one: NOT metaphor\n",
      "ear one tiny: metaphor\n",
      "one tiny regions: NOT metaphor\n",
      "tiny regions represents: NOT metaphor\n",
      "regions represents word: NOT metaphor\n",
      "represents word victim: NOT metaphor\n",
      "word victim the: NOT metaphor\n",
      "victim the region: NOT metaphor\n",
      "the region responds: NOT metaphor\n",
      "region responds killed: NOT metaphor\n",
      "responds killed convicted: NOT metaphor\n",
      "killed convicted murdered: NOT metaphor\n",
      "convicted murdered confessed: metaphor\n",
      "murdered confessed on: NOT metaphor\n",
      "confessed on brains: metaphor\n",
      "on brains righthand: NOT metaphor\n",
      "brains righthand side: NOT metaphor\n",
      "righthand side near: NOT metaphor\n",
      "side near top: metaphor\n",
      "near top head: NOT metaphor\n",
      "top head one: NOT metaphor\n",
      "head one brain: NOT metaphor\n",
      "one brain spots: NOT metaphor\n",
      "brain spots activated: NOT metaphor\n",
      "spots activated family: metaphor\n",
      "activated family terms: metaphor\n",
      "family terms wife: metaphor\n",
      "terms wife husband: NOT metaphor\n",
      "wife husband children: NOT metaphor\n",
      "husband children parents: NOT metaphor\n",
      "each word represented: metaphor\n",
      "word represented one: NOT metaphor\n",
      "represented one spot: NOT metaphor\n",
      "one spot words: NOT metaphor\n",
      "spot words tend: metaphor\n",
      "words tend several: NOT metaphor\n",
      "tend several meanings: NOT metaphor\n",
      "several meanings one: metaphor\n",
      "meanings one part: NOT metaphor\n",
      "one part brain: NOT metaphor\n",
      "part brain example: NOT metaphor\n",
      "brain example reliably: NOT metaphor\n",
      "example reliably responds: metaphor\n",
      "reliably responds word: NOT metaphor\n",
      "responds word top: NOT metaphor\n",
      "word top along: NOT metaphor\n",
      "top along words: NOT metaphor\n",
      "along words describe: metaphor\n",
      "words describe clothing: NOT metaphor\n",
      "describe clothing but: NOT metaphor\n",
      "clothing but word: metaphor\n",
      "but word top: NOT metaphor\n",
      "word top activates: NOT metaphor\n",
      "top activates many: NOT metaphor\n",
      "activates many regions: metaphor\n",
      "many regions one: NOT metaphor\n",
      "regions one responds: NOT metaphor\n",
      "one responds numbers: NOT metaphor\n",
      "responds numbers measurements: NOT metaphor\n",
      "numbers measurements another: NOT metaphor\n",
      "measurements another buildings: NOT metaphor\n",
      "another buildings places: metaphor\n",
      "buildings places the: NOT metaphor\n",
      "places the scientists: NOT metaphor\n",
      "the scientists created: NOT metaphor\n",
      "scientists created interactive: metaphor\n",
      "created interactive website: metaphor\n",
      "interactive website public: NOT metaphor\n",
      "website public explore: NOT metaphor\n",
      "public explore brain: NOT metaphor\n",
      "explore brain atlas: metaphor\n",
      "strikingly brain atlases: metaphor\n",
      "brain atlases similar: NOT metaphor\n",
      "atlases similar participants: metaphor\n",
      "similar participants suggesting: metaphor\n",
      "participants suggesting brains: NOT metaphor\n",
      "suggesting brains organised: metaphor\n",
      "brains organised meanings: NOT metaphor\n",
      "organised meanings words: NOT metaphor\n",
      "meanings words way: NOT metaphor\n",
      "words way the: NOT metaphor\n",
      "way the scientists: NOT metaphor\n",
      "the scientists scanned: NOT metaphor\n",
      "scientists scanned five: metaphor\n",
      "scanned five men: metaphor\n",
      "five men two: metaphor\n",
      "men two women: NOT metaphor\n",
      "two women however: NOT metaphor\n",
      "women however all: NOT metaphor\n",
      "however all native: NOT metaphor\n",
      "all native english: metaphor\n",
      "native english speakers: NOT metaphor\n",
      "english speakers two: metaphor\n",
      "speakers two authors: metaphor\n",
      "two authors study: NOT metaphor\n",
      "authors study published: metaphor\n",
      "study published nature: metaphor\n",
      "published nature it: NOT metaphor\n",
      "nature it highly: NOT metaphor\n",
      "it highly possible: NOT metaphor\n",
      "highly possible people: NOT metaphor\n",
      "possible people different: NOT metaphor\n",
      "people different backgrounds: NOT metaphor\n",
      "different backgrounds cultures: NOT metaphor\n",
      "backgrounds cultures different: NOT metaphor\n",
      "cultures different semantic: metaphor\n",
      "different semantic brain: NOT metaphor\n",
      "semantic brain atlases: metaphor\n",
      "armed atlas researchers: metaphor\n",
      "atlas researchers piece: metaphor\n",
      "researchers piece together: NOT metaphor\n",
      "piece together brain: NOT metaphor\n",
      "together brain networks: NOT metaphor\n",
      "brain networks represent: NOT metaphor\n",
      "networks represent wildly: NOT metaphor\n",
      "represent wildly different: NOT metaphor\n",
      "wildly different concepts: NOT metaphor\n",
      "different concepts numbers: NOT metaphor\n",
      "concepts numbers murder: metaphor\n",
      "numbers murder religion: NOT metaphor\n",
      "murder religion the: NOT metaphor\n",
      "religion the idea: NOT metaphor\n",
      "the idea murder: NOT metaphor\n",
      "idea murder represented: NOT metaphor\n",
      "murder represented lot: NOT metaphor\n",
      "represented lot brain: NOT metaphor\n",
      "lot brain gallant: NOT metaphor\n",
      "brain gallant said: NOT metaphor\n",
      "using haul data: metaphor\n",
      "haul data group: NOT metaphor\n",
      "data group begun: NOT metaphor\n",
      "group begun work: NOT metaphor\n",
      "begun work new: NOT metaphor\n",
      "work new atlases: NOT metaphor\n",
      "new atlases show: NOT metaphor\n",
      "atlases show brain: metaphor\n",
      "show brain holds: metaphor\n",
      "brain holds information: NOT metaphor\n",
      "holds information aspects: NOT metaphor\n",
      "information aspects language: NOT metaphor\n",
      "aspects language phonemes: metaphor\n",
      "language phonemes syntax: NOT metaphor\n",
      "phonemes syntax a: NOT metaphor\n",
      "syntax a brain: metaphor\n",
      "a brain atlas: metaphor\n",
      "brain atlas narrative: NOT metaphor\n",
      "atlas narrative structure: metaphor\n",
      "narrative structure far: NOT metaphor\n",
      "structure far proved: metaphor\n",
      "far proved elusive: metaphor\n",
      "proved elusive however: NOT metaphor\n",
      "elusive however every: metaphor\n",
      "however every time: NOT metaphor\n",
      "every time come: metaphor\n",
      "time come set: NOT metaphor\n",
      "come set narrative: metaphor\n",
      "set narrative features: metaphor\n",
      "narrative features get: NOT metaphor\n",
      "features get told: metaphor\n",
      "get told arent: NOT metaphor\n",
      "told arent right: NOT metaphor\n",
      "arent right set: metaphor\n",
      "right set narrative: NOT metaphor\n",
      "set narrative features: metaphor\n",
      "narrative features said: NOT metaphor\n",
      "features said gallant: metaphor\n",
      "uri hasson neuroscientist: metaphor\n",
      "hasson neuroscientist princeton: NOT metaphor\n",
      "neuroscientist princeton university: NOT metaphor\n",
      "princeton university praised: NOT metaphor\n",
      "university praised work: metaphor\n",
      "praised work unlike: NOT metaphor\n",
      "work unlike many: NOT metaphor\n",
      "unlike many studies: metaphor\n",
      "many studies looked: NOT metaphor\n",
      "studies looked brain: metaphor\n",
      "looked brain activity: NOT metaphor\n",
      "brain activity isolated: NOT metaphor\n",
      "activity isolated word: metaphor\n",
      "isolated word sentence: NOT metaphor\n",
      "word sentence spoken: NOT metaphor\n",
      "sentence spoken gallants: metaphor\n",
      "spoken gallants team: metaphor\n",
      "gallants team shed: NOT metaphor\n",
      "team shed light: NOT metaphor\n",
      "shed light brain: metaphor\n",
      "light brain worked: NOT metaphor\n",
      "brain worked realworld: NOT metaphor\n",
      "worked realworld scenario: NOT metaphor\n",
      "realworld scenario said: NOT metaphor\n",
      "scenario said the: metaphor\n",
      "said the next: metaphor\n",
      "the next step: NOT metaphor\n",
      "next step said: NOT metaphor\n",
      "step said create: metaphor\n",
      "said create comprehensive: metaphor\n",
      "create comprehensive precise: metaphor\n",
      "comprehensive precise semantic: metaphor\n",
      "precise semantic brain: NOT metaphor\n",
      "semantic brain atlas: metaphor\n",
      "brain atlas ultimately: NOT metaphor\n",
      "atlas ultimately hasson: metaphor\n",
      "ultimately hasson believes: metaphor\n",
      "hasson believes possible: NOT metaphor\n",
      "believes possible reconstruct: NOT metaphor\n",
      "possible reconstruct words: NOT metaphor\n",
      "reconstruct words person: NOT metaphor\n",
      "words person thinking: NOT metaphor\n",
      "person thinking brain: NOT metaphor\n",
      "thinking brain activity: NOT metaphor\n",
      "brain activity the: NOT metaphor\n",
      "activity the ethical: metaphor\n",
      "the ethical implications: NOT metaphor\n",
      "ethical implications enormous: metaphor\n",
      "implications enormous one: NOT metaphor\n",
      "enormous one benign: metaphor\n",
      "one benign use: NOT metaphor\n",
      "benign use would: NOT metaphor\n",
      "use would see: metaphor\n",
      "would see brain: NOT metaphor\n",
      "see brain activity: metaphor\n",
      "brain activity used: NOT metaphor\n",
      "activity used assess: metaphor\n",
      "used assess whether: metaphor\n",
      "assess whether political: metaphor\n",
      "whether political messages: NOT metaphor\n",
      "political messages effectively: NOT metaphor\n",
      "messages effectively communicated: NOT metaphor\n",
      "effectively communicated public: metaphor\n",
      "communicated public there: metaphor\n",
      "public there many: NOT metaphor\n",
      "there many implications: NOT metaphor\n",
      "many implications barely: NOT metaphor\n",
      "implications barely touching: NOT metaphor\n",
      "barely touching surface: NOT metaphor\n",
      "touching surface said: NOT metaphor\n",
      "lorraine tyler cognitive: NOT metaphor\n",
      "tyler cognitive neuroscientist: NOT metaphor\n",
      "cognitive neuroscientist head: metaphor\n",
      "neuroscientist head centre: NOT metaphor\n",
      "head centre speech: NOT metaphor\n",
      "centre speech language: metaphor\n",
      "speech language brain: metaphor\n",
      "language brain cambridge: NOT metaphor\n",
      "brain cambridge university: NOT metaphor\n",
      "cambridge university said: metaphor\n",
      "university said research: metaphor\n",
      "said research tour: metaphor\n",
      "research tour de: NOT metaphor\n",
      "tour de force: NOT metaphor\n",
      "de force scope: NOT metaphor\n",
      "force scope methods: metaphor\n",
      "scope methods but: metaphor\n",
      "methods but brain: NOT metaphor\n",
      "but brain atlas: NOT metaphor\n",
      "brain atlas current: NOT metaphor\n",
      "atlas current form: metaphor\n",
      "current form capture: metaphor\n",
      "form capture fine: metaphor\n",
      "capture fine differences: metaphor\n",
      "fine differences word: metaphor\n",
      "differences word meanings: NOT metaphor\n",
      "word meanings take: NOT metaphor\n",
      "meanings take word: NOT metaphor\n",
      "take word table: NOT metaphor\n",
      "word table it: NOT metaphor\n",
      "table it member: NOT metaphor\n",
      "it member many: NOT metaphor\n",
      "member many different: NOT metaphor\n",
      "many different groups: NOT metaphor\n",
      "different groups says: NOT metaphor\n",
      "groups says tyler: NOT metaphor\n",
      "says tyler it: metaphor\n",
      "tyler it something: NOT metaphor\n",
      "it something eat: NOT metaphor\n",
      "something eat off: metaphor\n",
      "eat off things: metaphor\n",
      "off things made: NOT metaphor\n",
      "things made wood: NOT metaphor\n",
      "made wood things: NOT metaphor\n",
      "wood things heavy: NOT metaphor\n",
      "things heavy things: NOT metaphor\n",
      "heavy things four: NOT metaphor\n",
      "things four legs: NOT metaphor\n",
      "four legs nonanimate: metaphor\n",
      "legs nonanimate objects: NOT metaphor\n",
      "nonanimate objects on: NOT metaphor\n",
      "objects on this: NOT metaphor\n",
      "on this kind: NOT metaphor\n",
      "this kind detailed: NOT metaphor\n",
      "kind detailed semantic: NOT metaphor\n",
      "detailed semantic information: NOT metaphor\n",
      "semantic information enables: metaphor\n",
      "information enables words: NOT metaphor\n",
      "enables words used: metaphor\n",
      "words used flexibly: NOT metaphor\n",
      "used flexibly lost: metaphor\n",
      "flexibly lost analysis: metaphor\n",
      "lost analysis said: NOT metaphor\n",
      "analysis said while: metaphor\n",
      "said while research: metaphor\n",
      "while research pathbreaking: NOT metaphor\n",
      "research pathbreaking scope: NOT metaphor\n",
      "pathbreaking scope still: NOT metaphor\n",
      "scope still lot: metaphor\n",
      "still lot learn: metaphor\n",
      "lot learn semantics: NOT metaphor\n",
      "learn semantics represented: NOT metaphor\n",
      "semantics represented brain: metaphor\n",
      "metaphor_count:188\n",
      "nonmetaphor_count:354\n",
      "['scientists created atlas', 'created atlas brain', 'atlas brain reveals', 'arranged across different', 'across different regions', 'colourful quilt laid', 'cortex atlas displays', 'atlas displays rainbow', 'concepts convey grouped', 'convey grouped together', 'clumps white matter', 'atlas shows one', 'shows one specific', 'specific aspect language', 'aspect language represented', 'case semantics meanings', 'semantics meanings words', 'said jack gallant', 'university california berkeley', 'single brain region', 'concept a single', 'a single brain', 'single brain spot', 'spot associated number', 'associated number related', 'and single word', 'single word lights', 'spots together make', 'use life love', 'clouds florida bra', 'florida bra all', 'all light networks', 'force one researcher', 'study atlas demonstrates', 'atlas demonstrates modern', 'advances technology could', 'could profound impact', 'approach could used', 'could used decode', 'used decode information', 'even thinking said', 'said alexander huth', 'alexander huth first', 'first author study', 'author study one', 'study one potential', 'use would language', 'could allow people', 'allow people silenced', 'silenced motor neurone', 'syndrome speak computer', 'create atlas scientists', 'atlas scientists recorded', 'scientists recorded peoples', 'activity listened stories', 'stories read the', 'us radio show', 'show they matched', 'stories brain activity', 'activity data show', 'show groups related', 'spots cerebral cortex', 'used stories the', 'stories the moth', 'short compelling the', 'compelling the enthralling', 'enthralling stories confident', 'stories confident scientists', 'confident scientists could', 'scientists could people', 'could people scanned', 'scanned focusing words', 'focusing words drifting', 'seven people listened', 'stories each per', 'each per person', 'amounted hearing roughly', 'atlas shows words', 'shows words related', 'exercise regions brain', 'for example lefthand', 'example lefthand side', 'side brain ear', 'ear one tiny', 'convicted murdered confessed', 'confessed on brains', 'side near top', 'spots activated family', 'activated family terms', 'family terms wife', 'each word represented', 'spot words tend', 'several meanings one', 'example reliably responds', 'along words describe', 'clothing but word', 'activates many regions', 'another buildings places', 'scientists created interactive', 'created interactive website', 'explore brain atlas', 'strikingly brain atlases', 'atlases similar participants', 'similar participants suggesting', 'suggesting brains organised', 'scientists scanned five', 'scanned five men', 'five men two', 'all native english', 'english speakers two', 'speakers two authors', 'authors study published', 'study published nature', 'cultures different semantic', 'semantic brain atlases', 'armed atlas researchers', 'atlas researchers piece', 'concepts numbers murder', 'using haul data', 'atlases show brain', 'show brain holds', 'aspects language phonemes', 'syntax a brain', 'a brain atlas', 'atlas narrative structure', 'structure far proved', 'far proved elusive', 'elusive however every', 'every time come', 'come set narrative', 'set narrative features', 'features get told', 'arent right set', 'set narrative features', 'features said gallant', 'uri hasson neuroscientist', 'university praised work', 'unlike many studies', 'studies looked brain', 'activity isolated word', 'sentence spoken gallants', 'spoken gallants team', 'shed light brain', 'scenario said the', 'said the next', 'step said create', 'said create comprehensive', 'create comprehensive precise', 'comprehensive precise semantic', 'semantic brain atlas', 'atlas ultimately hasson', 'ultimately hasson believes', 'activity the ethical', 'ethical implications enormous', 'enormous one benign', 'use would see', 'see brain activity', 'activity used assess', 'used assess whether', 'assess whether political', 'effectively communicated public', 'communicated public there', 'cognitive neuroscientist head', 'centre speech language', 'speech language brain', 'cambridge university said', 'university said research', 'said research tour', 'force scope methods', 'scope methods but', 'atlas current form', 'current form capture', 'form capture fine', 'capture fine differences', 'fine differences word', 'says tyler it', 'something eat off', 'eat off things', 'four legs nonanimate', 'semantic information enables', 'enables words used', 'used flexibly lost', 'flexibly lost analysis', 'analysis said while', 'said while research', 'scope still lot', 'still lot learn', 'semantics represented brain']\n"
     ]
    }
   ],
   "source": [
    "with open(\"sciencearticle.txt\", \"r\") as testset:\n",
    "    filtered_words = []\n",
    "    testmeta = []\n",
    "    word_list = ''\n",
    "    para_index = 0\n",
    "#     text = ''.join(testset.readlines())\n",
    "#     sentences = re.split(r' *[\\.\\?!][\\'\"\\)\\]]* *', text)\n",
    "#     sentences = [t.rstrip() for t in sentences]\n",
    "#     print(sentences)\n",
    "    for line in testset:\n",
    "        para_index += 1\n",
    "        word_list = line.split()\n",
    "        filtered_words = [re.sub(r'[^\\w\\s]','',word.lower()) for word in word_list if word not in stopwords.words('english')]\n",
    "        testmeta.append([' '.join(x) for x in trigrams(filtered_words)])\n",
    "        print(para_index)\n",
    "# print(testmeta)\n",
    "\n",
    "metaphor_count = 0\n",
    "nonmetaphor_count = 0\n",
    "metaphor_list = []\n",
    "for x in testmeta:\n",
    "    for y in x:\n",
    "        testsample = y\n",
    "        print (testsample + \": \" + cl.classify(metaphor_features(testsample)))\n",
    "        if cl.classify(metaphor_features(testsample)) == 'metaphor':\n",
    "            metaphor_count += 1\n",
    "            metaphor_list.append(testsample)\n",
    "        elif cl.classify(metaphor_features(testsample)) == 'NOT metaphor':\n",
    "            nonmetaphor_count += 1\n",
    "            \n",
    "print('metaphor_count:' + str(metaphor_count))\n",
    "print('nonmetaphor_count:' + str(nonmetaphor_count))\n",
    "print(metaphor_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scientists have created an â€œatlas of the brainâ€ that reveals how the meanings of words are arranged across different regions of the organ.\n",
      "\n",
      "\n",
      "\n",
      "Like a colourful quilt laid over the cortex, the atlas displays in rainbow hues how individual words and the concepts they convey can be grouped together in clumps of white matter.\n",
      "\n",
      "\n",
      "\n",
      "â€œOur goal was to build a giant atlas that shows how one specific aspect of language is represented in the brain, in this case semantics, or the meanings of words,â€ said Jack Gallant, a neuroscientist at the University of California, Berkeley.\n",
      "\n",
      "\n",
      "\n",
      "No single brain region holds one word or concept. A single brain spot is associated with a number of related words. And each single word lights up many different brain spots. Together they make up networks that represent the meanings of each word we use: life and love; death and taxes; clouds, Florida and bra. All light up their own networks.\n",
      "\n",
      "\n",
      "\n",
      "Described as a â€œtour de forceâ€ by one researcher who was not involved in the study, the atlas demonstrates how modern imaging can transform our knowledge of how the brain performs some of its most important tasks. With further advances, the technology could have a profound impact on medicine and other fields.\n",
      "\n",
      "\n",
      "\n",
      "â€œIt is possible that this approach could be used to decode information about what words a person is hearing, reading, or possibly even thinking,â€ said Alexander Huth, the first author on the study. One potential use would be a language decoder that could allow people silenced by motor neurone disease or locked-in syndrome to speak through a computer.\n",
      "\n",
      "\n",
      "\n",
      "To create the atlas, the scientists recorded peopleâ€™s brain activity while they listened to stories read out on The Moth Radio Hour, a US radio show. They then matched the transcripts of the stories with the brain activity data to show how groups of related words triggered neural responses in 50,000 to 80,000 pea-sized spots all over the cerebral cortex.\n",
      "\n",
      "\n",
      "\n",
      "Huth used stories from The Moth Radio Hour because they are short and compelling. The more enthralling the stories, the more confident the scientists could be that the people being scanned were focusing on the words and not drifting off. Seven people listened to two hours of stories each. Per person, that amounted to hearing roughly 25,000 words- and more than 3,000 different words - as they lay in the scanner.\n",
      "\n",
      "\n",
      "\n",
      "The atlas shows how words and related terms exercise the same regions of the brain. For example, on the left-hand side of the brain, above the ear, is one of the tiny regions that represents the word â€œvictimâ€. The same region responds to â€œkilledâ€, â€œconvictedâ€, â€œmurderedâ€ and â€œconfessedâ€. On the brainâ€™s right-hand side, near the top of the head, is one of the brain spots activated by family terms: â€œwifeâ€, â€œhusbandâ€, â€œchildrenâ€, â€œparentsâ€.\n",
      "\n",
      "\n",
      "\n",
      "Each word is represented by more than one spot because words tend to have several meanings. One part of the brain, for example, reliably responds to the word â€œtopâ€, along with other words that describe clothing. But the word â€œtopâ€ activates many other regions. One of them responds to numbers and measurements, another to buildings and places. The scientists have created an interactive website where the public can explore the brain atlas.\n",
      "\n",
      "\n",
      "\n",
      "Strikingly, the brain atlases were similar for all the participants, suggesting that their brains organised the meanings of words in the same way. The scientists only scanned five men and two women, however. All are native English speakers, and two are authors of the study published in Nature. It is highly possible that people from different backgrounds and cultures will have different semantic brain atlases.\n",
      "\n",
      "\n",
      "\n",
      "Armed with the atlas, researchers can now piece together the brain networks that represent wildly different concepts, from numbers to murder and religion. â€œThe idea of murder is represented a lot in the brain,â€ Gallant said.\n",
      "\n",
      "\n",
      "\n",
      "Using the same haul of data, the group has begun work on new atlases that show how the brain holds information on other aspects of language, from phonemes to syntax. A brain atlas for narrative structure has so far proved elusive, however. â€œEvery time we come up with a set of narrative features, we get told they arenâ€™t the right set of narrative features,â€ said Gallant.\n",
      "\n",
      "\n",
      "\n",
      "Uri Hasson, a neuroscientist at Princeton University, praised the work. Unlike many studies that looked at brain activity when an isolated word or sentence was spoken, Gallantâ€™s team had shed light on how the brain worked in a real-world scenario, he said. The next step, he said, was to create a more comprehensive and precise semantic brain atlas. Ultimately, Hasson believes it will be possible to reconstruct the words a person is thinking from their brain activity. The ethical implications are enormous. One more benign use would see brain activity used to assess whether political messages have been effectively communicated to the public. â€œThere are so many implications, and we are barely touching the surface,â€ he said.\n",
      "\n",
      "\n",
      "\n",
      "Lorraine Tyler, a cognitive neuroscientist and head of the Centre for Speech, Language and the Brain at Cambridge University said the research was a â€œtour de force in its scope and methodsâ€. But the brain atlas in its current form does not capture fine differences in word meanings. Take the word â€œtableâ€. It can be a member of many different groups, says Tyler. â€œIt can be something to eat off, things made of wood, things that are heavy, things having four legs, non-animate objects, and so on. This kind of detailed semantic information that enables words to be used flexibly is lost in the analysis,â€ she said. â€œWhile this research is path-breaking in its scope, there is still a lot to learn about how semantics is represented in the brain.â€\n",
      "['', 'created an â€œatlas o', 'atlas of the brainâ€', 'arranged across different', 'across different regions', '', '', 'atlas of the brainâ€ th', '', '', '', 'atlas of the br', '', '', '', '', '', '', '', '', '', 'ave created an', '', '', '', '', '', '', '', '', '', '', '', '', 'atlas of the brainâ€ that ', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'created an â€œatlas of th', 'atlas of the brainâ€ that ', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'atlas of the brai', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'created an â€œatlas of the br', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'atlas of the brainâ€ tha', '', '', '', '', '', '', 'ave created a', 'atlas of the brainâ€ that ', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'created an â€œatlas of the bra', '', '', 'atlas of the brainâ€ tha', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'atlas of the brain', '', '', '', '', '', '', 'eated an â€œatla', '', '', '', '', '', '', '', '', '', '', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '', '', 'atlas displays in r', '', '', 'colourful quilt laid', 'cortex, the atlas dis', 'atlas displays in rain', 'concepts they convey ca', 'convey can be grouped t', 'clumps of white mat', 'atlas displays ', '', '', '', '', '', '', '', '', 'concepts they co', 'a colourful qu', '', '', '', 'and the concept', '', '', '', '', '', '', '', '', 'atlas displays in rainbow', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'atlas displays in rainbow', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'atlas displays in', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'atlas displays in rainb', 'concepts they convey ca', '', '', '', '', '', 'a colourful q', 'atlas displays in rainbow', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'atlas displays in rainb', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'atlas displays in ', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '', '', 'atlas that shows ho', '', '', '', '', 'atlas that shows how o', '', '', '', 'atlas that show', 'shows how one spec', 'specific aspect of langu', 'aspect of language is repre', 'case semantics, or the ', 'semantics, or the meanin', 'said Jack Gallant', '', '', '', 'al was to buil', '', '', '', '', '', '', '', '', '', 'allant, a neurosci', '', '', 'atlas that shows how one ', '', '', '', '', '', '', 'said Jack Gallant, ', '', '', '', '', '', '', '', '', '', '', 'atlas that shows how one ', '', '', '', '', 'shows how one spe', '', '', 'shows how one speci', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'atlas that shows ', 'shows how one speci', '', 'fornia, Berkeley.\\n', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'allant, a neurosci', '', '', '', '', '', 'semantics, or the mean', '', 'atlas that shows how on', '', '', '', 'shows how one sp', '', '', 'al was to bui', 'atlas that shows how one ', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'said Jack Gal', '', 'said Jack Gallant, a neur', '', '', 'semantics, or the me', 'atlas that shows how on', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'said Jack Gallant,', '', '', 'atlas that shows h', '', '', '', '', '', '', '', '', 'semantics, or the meanings o', '', '', '', '', 'said Jack Gallant, ', '', '', 'semantics, or the meanings ', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'single brain region', 'concept. A singl', 'ain region hol', 'single brain regi', 'spot is associated wit', 'associated with a number ', 'and love; death', 'single brain regio', 'spots. Together the', 'use: life and', 'clouds, Florida an', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'use: life and love', '', '', '', '', '', '', '', '', '', 'use: life and', '', '', '', '', 'spots. Together they ', '', '', '', '', '', '', '', '', '', '', '', '', '', 'each single wor', '', '', '', '', '', '', '', '', '', '', '', 'spots. Together they m', '', '', 'each single word ligh', 'spot is associa', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'ain region ho', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'use: life and', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'eath and taxes', '', '', '', '', '', '', '', '', '', '', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '', '', 'atlas demonstrates ', '', '', '', '', 'atlas demonstrates how', '', '', '', 'atlas demonstra', '', '', '', '', '', '', '', '', '', 'as a â€œtour de ', '', '', '', 'and other field', '', '', '', '', '', '', 'forceâ€ by one resear', 'study, the atlas demonst', 'atlas demonstrates how mo', 'advances, the technology ', 'could have a profound', '', 'could have a prof', '', '', '', '', '', '', 'study, the atlas de', '', 'could have a profo', '', '', '', '', 'atlas demonstrates how mo', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'could have a profoun', '', '', '', '', '', '', 'atlas demonstrate', '', '', 'forceâ€ by one resear', '', '', 'earcher who ', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'study, the atlas demon', '', '', '', 'atlas demonstrates how ', '', '', '', '', '', '', 'as a â€œtour de', 'atlas demonstrates how mo', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'atlas demonstrates how ', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'forceâ€ by one resea', '', 'atlas demonstrates', '', 'form our knowledg', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'said Alexander Hu', '', '', '', 'at this approa', '', '', '', 'ander Huth, the', '', '', 'used to decod', '', '', 'allow people silen', '', 'study. One potential use', '', '', 'could be used to deco', 'approach could be u', 'could be used to ', 'used to decode informat', 'even thinking,â€ sa', 'said Alexander Huth', '', 'first author on th', 'author on the st', 'study. One potentia', 'used to decode inf', 'could be used to d', 'allow people silenced', 'silenced by motor neur', 'syndrome to speak throu', '', '', '', '', '', 'used to decod', '', '', '', '', '', 'used to decode i', '', '', '', '', '', '', '', 'could be used to dec', '', '', '', '', '', '', '', '', '', 'formation about what', '', '', 'earing, read', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'allow people silen', '', '', '', 'study. One potential u', '', '', '', '', '', '', '', '', '', '', 'at this appro', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'said Alexande', '', 'said Alexander Huth, the ', '', '', '', '', '', '', '', '', 'used to decod', '', '', 'used to decode info', '', '', '', '', '', '', '', '', 'said Alexander Hut', '', '', '', '', 'formation about w', '', '', '', '', '', '', '', '', 'used to decode inf', '', '', 'said Alexander Huth', '', '', '', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', 'scientists recorded peop', '', 'atlas, the scientis', '', '', '', 'cortex.\\n', 'atlas, the scientists ', '', '', '', 'atlas, the scie', '', '', '', '', '', '', '', '', '', 'ate the atlas,', '', 'spots all over the cer', '', '', '', 'spots all over the ', '', '', '', 'all over the cereb', '', '', 'atlas, the scientists rec', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'create the atlas, the s', 'atlas, the scientists rec', 'scientists recorded peopleâ€™', 'activity while they liste', 'stories read out', '', 'show. They then m', 'stories read out on Th', 'activity while the', 'show. They then mat', 'spots all over the ce', '', 'stories read out', '', '', '', 'stories read out on The Moth', '', 'scientists recorded peo', '', '', '', '', 'stories read out', '', '', 'atlas, the scient', '', '', '', '', '', '', '', '', '', 'spots all over the cer', '', '', '', 'spots all over ', '', '', '', '', '', '', 'scientists recorded peopleâ€™s b', '', '', '', '', '', '', 'scientists recorded peo', '', '', 'all over the cereb', '', '', '', '', '', '', '', 'atlas, the scientists r', '', '', '', 'show. They then ', '', '', 'ate the atlas', 'atlas, the scientists rec', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'activity while they li', '', '', '', '', '', '', '', 'create the atlas, the scient', '', '', 'atlas, the scientists r', '', 'activity while they ', '', '', '', '', 'activity while they ', '', '', '', '', '', '', '', '', '', '', '', '', 'atlas, the scienti', '', '', '', '', '', '', 'eate the atlas', '', '', '', '', '', '', '', '', '', '', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', 'scientists could be that', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'adio Hour beca', '', '', '', 'and compelling.', '', '', 'used stories ', '', '', 'alling the stories', '', '', '', '', 'could be that the peo', '', 'could be that the', 'used stories from The M', 'even people listen', '', '', '', '', '', 'used stories from ', 'could be that the ', '', '', '', '', '', 'scientists could be that th', '', 'stories from The', 'used stories ', '', 'stories from The Moth ', '', '', '', 'used stories fro', 'stories from The', 'short and compelling', 'compelling. The more enthr', 'enthralling the stories, the ', 'stories from The Moth Radio ', 'confident the scientists c', 'scientists could be tha', 'could be that the pe', 'scanned were focusing ', 'focusing on the words a', '', 'stories from The', 'each. Per perso', 'amounted to hearing roug', '', '', '', '', '', '', 'earing rough', '', '', '', '', '', '', 'each. Per person, tha', '', '', '', '', '', '', '', 'scientists could be that the p', '', '', '', '', '', '', 'scientists could be tha', 'scanned were foc', '', 'alling the stories', '', '', '', '', '', '', '', '', '', 'using on the wo', '', '', '', '', 'adio Hour bec', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'used stories ', '', '', 'used stories from T', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'used stories from ', '', '', '', '', '', '', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '', '', 'atlas shows how wor', '', '', '', '', 'atlas shows how words ', '', '', '', 'atlas shows how', 'shows how words an', '', '', '', '', '', '', '', '', 'atlas shows ho', '', 'spots activated by fam', '', 'and related ter', '', 'spots activated by ', '', '', '', '', '', '', 'atlas shows how words and', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'atlas shows how words and', '', '', '', 'usbandâ€, â€œchi', 'shows how words a', '', '', 'shows how words and', 'spots activated by fa', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'atlas shows how w', 'shows how words and', 'exercise the same regi', '', 'example, on the left-', 'side of the br', 'ear, is one ', 'convictedâ€, â€œmurderedâ€ and â€œ', 'confessedâ€. On the ', 'side of the b', 'spots activated by fam', 'activated by family te', 'family terms: â€œwi', '', 'spots activated', '', 'example, on the left-hand', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'atlas shows how words a', '', '', '', 'shows how words ', '', '', 'atlas shows h', 'atlas shows how words and', '', '', '', '', '', '', '', 'arentsâ€.\\n', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'atlas shows how words a', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'atlas shows how wo', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', 'scientists have created ', 'created an interact', 'atlas.\\n', '', '', '', '', 'atlas.\\n', '', '', '', 'atlas.\\n', '', '', '', '', '', '', '', '', '', 'ach word is re', '', 'spot because words ten', '', 'and measurement', '', '', 'use words ten', '', '', '', '', '', 'atlas.\\n', '', '', '', '', '', '', '', '', '', '', '', 'use words tend to ', '', '', '', '', 'created an interactive ', 'atlas.\\n', 'scientists have created an ', '', '', 'use words ten', '', '', '', '', '', '', '', '', '', '', '', '', 'scientists have created', '', '', '', '', '', '', '', 'atlas.\\n', '', '', 'for example, reliabl', 'example, reliably res', '', '', '', '', '', '', '', '', '', 'spot because wo', 'several meanings. On', 'example, reliably respond', 'along with other wor', 'clothing. But the', 'activates many other r', 'another to buildings and', 'scientists have created an int', 'created an interactive webs', 'explore the brain a', '', '', '', '', 'scientists have created', '', '', '', '', '', '', '', '', '', '', 'atlas.\\n', '', '', '', '', '', '', 'ach word is r', 'atlas.\\n', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'created an interactive websi', '', '', 'atlas.\\n', '', '', '', '', 'use words ten', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'atlas.\\n', '', '', '', '', '', '', 'eated an inter', '', '', '', '', '', '', '', '', '', '', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', 'scientists only scanned ', '', 'atlases were simila', '', '', '', '', 'atlases were similar f', '', '', '', 'atlases were si', '', '', '', '', '', '', '', '', '', 'ain atlases we', '', '', '', 'and two women, ', '', '', '', '', '', 'all the participan', '', 'study published in Natur', 'atlases were similar for ', '', '', '', '', '', '', '', '', '', 'authors of the s', 'study published in ', '', '', '', '', '', '', 'atlases were similar for ', 'scientists only scanned fiv', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'scientists only scanned', '', 'scanned five men and t', '', '', '', '', '', 'atlases were simi', '', '', 'for all the particip', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'scientists only scanned five m', '', '', '', 'atlases were similar for all', 'similar for all the participant', 'suggesting that their brain', 'scientists only scanned', 'scanned five men', 'five men and', 'all the participan', '', 'speakers, and two ar', 'authors of the study pu', 'study published in Nat', 'cultures will have differen', 'semantic brain atlases', '', 'atlases were similar fo', '', '', 'atlases were simil', '', '', '', 'ain atlases w', 'atlases were similar for ', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'shed in Nature. ', '', '', '', '', '', '', 'semantic brain atlas', 'atlases were similar fo', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'atlases were simil', '', '', '', '', '', '', '', '', 'semantic brain atlases.\\n', '', '', '', '', '', '', '', '', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '', '', 'atlas, researchers ', '', '', '', '', 'atlas, researchers can', 'concepts, from numbers ', '', '', 'atlas, research', '', '', '', '', '', 'said.\\n', '', '', 'concepts, from n', 'atlas, researc', '', '', '', 'and religion. â€œ', '', '', '', '', '', 'allant said.\\n', '', '', 'atlas, researchers can no', '', '', '', '', '', '', 'said.\\n', '', '', '', '', '', '', '', '', '', '', 'atlas, researchers can no', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'atlas, researcher', '', '', '', '', '', 'earchers can', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'allant said.\\n', '', '', '', '', '', '', '', 'atlas, researchers can ', 'concepts, from numbers ', '', '', '', '', '', 'atlas, resear', 'atlas, researchers can no', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'said.\\n', '', 'said.\\n', '', '', '', 'atlas, researchers can ', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'said.\\n', '', '', 'atlas, researchers', '', '', '', '', '', '', '', '', '', '', '', '', '', 'said.\\n', '', '', '', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '', '', 'atlases that show h', '', '', '', '', 'atlases that show how ', '', '', '', 'atlases that sh', '', '', 'aspects of language, from p', '', '', 'said Gallant.\\n', '', '', '', 'ame haul of da', '', '', '', '', '', '', '', '', '', 'allant.\\n', '', '', 'atlases that show how the', '', '', '', '', '', '', 'said Gallant.\\n', '', '', '', '', '', '', '', '', '', '', 'atlases that show how the', '', '', '', 'usive, howeve', 'show how the brai', '', '', 'show how the brain ', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'atlases that show', '', '', 'formation on other a', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'atlases that show how the br', '', '', '', '', '', 'allant.\\n', '', '', '', '', '', '', '', 'atlases that show how t', '', '', 'atlases that show ', 'show how the bra', 'aspects of language, from', 'syntax. A brai', 'ame haul of d', 'atlases that show how the', 'structure has so far', 'far proved elusive', 'elusive, however. â€œEv', '', 'come up with a set', 'set of narrative featu', 'features, we get ', '', 'set of narrative featu', 'features, we get told', '', '', '', '', '', '', '', '', '', 'said Gallant.', '', 'said Gallant.\\n', '', '', '', 'atlases that show how t', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'said Gallant.\\n', '', '', 'atlases that show ', '', 'formation on othe', '', '', '', '', 'eatures, we ge', '', '', '', '', '', '', 'said Gallant.\\n', '', '', '', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '', '', 'atlas. Ultimately, ', '', '', '', '', 'atlas. Ultimately, Has', '', '', '', 'atlas. Ultimate', '', '', '', '', '', 'said. The next st', '', '', '', 'asson, a neuro', '', '', '', 'and precise sem', '', '', 'use would see', '', '', 'allantâ€™s team had ', '', '', 'atlas. Ultimately, Hasson', '', '', '', '', 'used to assess whether ', '', 'said. The next step', '', '', '', '', 'use would see brai', '', '', '', '', 'create a more comprehen', 'atlas. Ultimately, Hasson', '', 'activity when an isolated', '', 'us. One more ', '', '', 'activity when an i', '', '', 'used to assess w', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'atlas. Ultimately', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'allantâ€™s team had ', '', '', '', '', '', 'semantic brain atlas. ', '', 'atlas. Ultimately, Hass', '', '', '', '', '', '', 'asson, a neur', 'atlas. Ultimately, Hasson', '', '', '', '', '', '', '', '', '', '', '', '', '', 'studies that looked ', 'activity when an isola', 'sentence was spoken, Gal', 'spoken, Gallantâ€™s te', 'shed light on ho', 'scenario, he said', 'said. The nex', 'step, he said, w', 'said. The next step, he s', 'create a more comprehensive ', 'comprehensive and precise sema', 'semantic brain atlas', 'atlas. Ultimately, Hass', '', 'activity when an iso', 'ethical implications are enor', 'enormous. One more ', 'use would see', 'see brain activity', 'activity when an iso', 'used to assess whet', 'assess whether political', 'effectively communicated to the', 'communicated to the publi', '', '', '', '', '', 'said. The next ste', '', '', 'atlas. Ultimately,', '', '', '', '', '', '', 'eate a more co', '', 'semantic brain atlas. Ultima', '', 'used to assess whe', '', '', 'said. The next step', '', '', '', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '', '', 'atlas in its curren', '', '', '', '', 'atlas in its current f', '', '', '', 'atlas in its cu', '', '', '', '', 'semantics is represented', 'said the research', '', '', '', 'aine Tyler, a ', '', '', '', 'and head of the', '', '', 'used flexibly', '', '', '', 'force in its scope a', '', 'atlas in its current form', '', '', '', '', 'used flexibly is lost i', '', 'said the research w', '', '', '', '', 'used flexibly is l', '', '', '', '', '', 'atlas in its current form', '', '', '', 'used flexibly', '', '', '', '', '', 'used flexibly is', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'atlas in its curr', '', '', 'for Speech, Language', '', '', 'earch was a ', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'semantic information t', '', 'atlas in its current fo', '', '', '', '', '', '', 'aine Tyler, a', 'atlas in its current form', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'said the rese', '', 'said the research was a â€œ', '', '', 'semantic information', 'atlas in its current fo', '', '', '', '', 'used flexibly', '', '', 'used flexibly is lo', '', '', '', 'cognitive neuroscientist and ', '', '', '', '', 'said the research ', 'force in its scope ', 'scope and methods', 'atlas in its curre', 'current form does no', 'form does not cap', 'capture fine differences', 'fine differences in w', 'says Tyler. â€œ', 'something to eat ', 'eat off, thing', 'four legs, non-anima', 'semantic information that en', 'enables words to b', 'used flexibly is l', 'flexibly is lost in th', 'analysis,â€ she said', 'said the research w', 'scope and metho', 'still a lot to ', 'semantics is represented in']\n"
     ]
    }
   ],
   "source": [
    "with open(\"sciencearticle.txt\", \"r\") as testset:\n",
    "    word_list = ''\n",
    "    highlight_bucket = []\n",
    "    for line in testset:\n",
    "        print(line)\n",
    "        for trigram in metaphor_list:\n",
    "            trigram_list = trigram.split()\n",
    "            index = line.find(trigram_list[0])\n",
    "            index_len = len(trigram)\n",
    "            highlight_bucket.append(line[index:index+index_len])\n",
    "print(highlight_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n",
      "181\n",
      "5\n",
      "105\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "sample = ['the', 'methods', 'i', 'am', 'michelle']\n",
    "# word_list\n",
    "word_str = ' '.join(word_list)\n",
    "# print(word_str)\n",
    "for word in sample:\n",
    "    print(word_str.find(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(i|me|my|myself|we|our|ours|ourselves|you|your|yours|yourself|yourselves|he|him|his|himself|she|her|hers|herself|it|its|itself|they|them|their|theirs|themselves|what|which|who|whom|this|that|these|those|am|is|are|was|were|be|been|being|have|has|had|having|do|does|did|doing|a|an|the|and|but|if|or|because|as|until|while|of|at|by|for|with|about|against|between|into|through|during|before|after|above|below|to|from|up|down|in|out|on|off|over|under|again|further|then|once|here|there|when|where|why|how|all|any|both|each|few|more|most|other|some|such|no|nor|not|only|own|same|so|than|too|very|s|t|can|will|just|don|should|now|d|ll|m|o|re|ve|y|ain|aren|couldn|didn|doesn|hadn|hasn|haven|isn|ma|mightn|mustn|needn|shan|shouldn|wasn|weren|won|wouldn)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "stopwords_regex = \"(\"\n",
    "# print(stopwords.words('english'))\n",
    "for word in stopwords.words('english'):\n",
    "    stopwords_regex = stopwords_regex + word + \"|\"\n",
    "stopwords_regex = stopwords_regex[:-1] + \")\"\n",
    "print(stopwords_regex)\n",
    "punctuation_regex = r\"[!\\\"#$%&'\\(\\)*+,-./:;<=>?@^_`{}~]*\"\n",
    "\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scientists[!\\\"#$%&'\\(\\)*+,-./:;<=>?@^_`{}~]*(i|me|my|myself|we|our|ours|ourselves|you|your|yours|yourself|yourselves|he|him|his|himself|she|her|hers|herself|it|its|itself|they|them|their|theirs|themselves|what|which|who|whom|this|that|these|those|am|is|are|was|were|be|been|being|have|has|had|having|do|does|did|doing|a|an|the|and|but|if|or|because|as|until|while|of|at|by|for|with|about|against|between|into|through|during|before|after|above|below|to|from|up|down|in|out|on|off|over|under|again|further|then|once|here|there|when|where|why|how|all|any|both|each|few|more|most|other|some|such|no|nor|not|only|own|same|so|than|too|very|s|t|can|will|just|don|should|now|d|ll|m|o|re|ve|y|ain|aren|couldn|didn|doesn|hadn|hasn|haven|isn|ma|mightn|mustn|needn|shan|shouldn|wasn|weren|won|wouldn)[!\\\"#$%&'\\(\\)*+,-./:;<=>?@^_`{}~]*created[!\\\"#$%&'\\(\\)*+,-./:;<=>?@^_`{}~]*(i|me|my|myself|we|our|ours|ourselves|you|your|yours|yourself|yourselves|he|him|his|himself|she|her|hers|herself|it|its|itself|they|them|their|theirs|themselves|what|which|who|whom|this|that|these|those|am|is|are|was|were|be|been|being|have|has|had|having|do|does|did|doing|a|an|the|and|but|if|or|because|as|until|while|of|at|by|for|with|about|against|between|into|through|during|before|after|above|below|to|from|up|down|in|out|on|off|over|under|again|further|then|once|here|there|when|where|why|how|all|any|both|each|few|more|most|other|some|such|no|nor|not|only|own|same|so|than|too|very|s|t|can|will|just|don|should|now|d|ll|m|o|re|ve|y|ain|aren|couldn|didn|doesn|hadn|hasn|haven|isn|ma|mightn|mustn|needn|shan|shouldn|wasn|weren|won|wouldn)[!\\\"#$%&'\\(\\)*+,-./:;<=>?@^_`{}~]*atlas\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'group'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-512-40d41ef86c36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregex_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mmatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregex_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maritcle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mcomplete_trigram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrigram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomplete_trigram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maritcle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomplete_trigram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'group'"
     ]
    }
   ],
   "source": [
    "with open(\"sciencearticle.txt\", \"r\") as testset:\n",
    "    aritcle = testset.read().lower()\n",
    "    word_list = ''\n",
    "    highlight_bucket = []\n",
    "    tag_list = []\n",
    "    \n",
    "    for trigram in metaphor_list:\n",
    "#         print(trigram)\n",
    "        trigram_list = trigram.split(\" \")\n",
    "        regex_string = trigram_list[0] + punctuation_regex + stopwords_regex + punctuation_regex + trigram_list[1] + punctuation_regex + stopwords_regex + punctuation_regex + trigram_list[2] \n",
    "        print(regex_string)\n",
    "        match = re.search(regex_string, aritcle)\n",
    "        complete_trigram = match.group(0)\n",
    "        print(trigram, complete_trigram)\n",
    "        index = aritcle.find(complete_trigram)\n",
    "        print(index, trigram_list[0])\n",
    "        index_len = len(trigram)\n",
    "        highlight_bucket.append(aritcle[index:index+index_len])\n",
    "        tag_list.append((index, index_len))\n",
    "        aritcle = aritcle[index+index_len:]\n",
    "    \n",
    "#     for line in testset:\n",
    "# #         print(line)\n",
    "#         for trigram in metaphor_list:\n",
    "#             trigram_list = trigram.split()\n",
    "#             index = line.find(trigram_list[0])\n",
    "#             index_len = len(trigram)\n",
    "#             highlight_bucket.append(line[index:index+index_len])\n",
    "#             line = line[index+index_len:]\n",
    "print(highlight_bucket)\n",
    "len(highlight_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"sciencearticle.txt\", \"r\") as testset:\n",
    "    filtered_words = []\n",
    "    testmeta = []\n",
    "    word_list = ''\n",
    "    para_index = 0\n",
    "#     text = ''.join(testset.readlines())\n",
    "#     sentences = re.split(r' *[\\.\\?!][\\'\"\\)\\]]* *', text)\n",
    "#     sentences = [t.rstrip() for t in sentences]\n",
    "#     print(sentences)\n",
    "    for line in testset:\n",
    "        para_index += 1\n",
    "        word_list = line.split()\n",
    "        filtered_words = [re.sub(r'[^\\w\\s]','',word.lower()) for word in word_list if word not in stopwords.words('english')]\n",
    "        testmeta.append([' '.join(x) for x in trigrams(filtered_words)])\n",
    "        print(para_index)\n",
    "# print(testmeta)\n",
    "\n",
    "metaphor_count = 0\n",
    "nonmetaphor_count = 0\n",
    "metaphor_list = []\n",
    "for x in testmeta:\n",
    "    for y in x:\n",
    "        testsample = y\n",
    "        print (testsample + \": \" + cl.classify(metaphor_features(testsample)))\n",
    "        if cl.classify(metaphor_features(testsample)) == 'metaphor':\n",
    "            metaphor_count += 1\n",
    "            metaphor_list.append(testsample)\n",
    "        elif cl.classify(metaphor_features(testsample)) == 'NOT metaphor':\n",
    "            nonmetaphor_count += 1\n",
    "            \n",
    "print('metaphor_count:' + str(metaphor_count))\n",
    "print('nonmetaphor_count:' + str(nonmetaphor_count))\n",
    "print(metaphor_list)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
