{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "from sklearn.externals import joblib\n",
    "import pickle as pickle\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.externals import joblib\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from nltk import bigrams\n",
    "from nltk import collocations\n",
    "from nltk import trigrams\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_features(candidate):\n",
    "    features = Counter()\n",
    "#     print(candidate)\n",
    "    candidate = str(str(candidate).lower().encode(\"ascii\", \"ignore\"))\n",
    "    features  = get_letter_combinations(candidate, features, 1)\n",
    "    features  = get_letter_combinations(candidate, features, 2)\n",
    "    features  = get_letter_combinations(candidate, features, 3)\n",
    "    features  = get_letter_combinations(candidate, features, 4)\n",
    "#     features  = get_letter_combinations(candidate, features, 5)\n",
    "    features['first'] = candidate[:1]\n",
    "    features['last_three'] = candidate[-3:]\n",
    "    features['last_two'] = candidate[-2:]\n",
    "    features['first_three'] = candidate[:3]\n",
    "    features['name_len_id'] = len(candidate)\n",
    "    return dict(features)\n",
    "\n",
    "def get_letter_combinations(candidate, features, number):\n",
    "    candidate = candidate.replace(\" \", \"\")\n",
    "    if len(candidate) < number:\n",
    "        return features\n",
    "    else:\n",
    "        for index in range(0, len(candidate), number):\n",
    "            features[candidate[index:index + number]] += 1\n",
    "        return features\n",
    "\n",
    "def create_manual_test_set(manual_list, generate_features):\n",
    "    manual_set = [(generate_features(key), value, key) for (key, value) in manual_list]\n",
    "    test_set_features = np.asarray([item[0] for item in manual_set])\n",
    "    test_set_labels = np.asarray([item[1] for item in manual_set])\n",
    "    test_set_names = np.asarray([item[2] for item in manual_set])\n",
    "    manual_set_dict = {}\n",
    "    manual_set_dict[\"features\"] = test_set_features\n",
    "    manual_set_dict[\"names\"] = test_set_names\n",
    "    manual_set_dict[\"labels\"] = test_set_labels\n",
    "    return manual_set_dict\n",
    "\n",
    "def test_manual_predictions(manual_list, clf):\n",
    "    manual_test_dict = create_manual_test_set(manual_list, generate_features)\n",
    "    manual_predictions = clf.predict(manual_test_dict)\n",
    "    return manual_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"For data grouped by feature, select subset of data at a provided key.\n",
    "\n",
    "    The data is expected to be stored in a 2D data structure, where the first\n",
    "    index is over features and the second is over samples.  i.e.\n",
    "\n",
    "    >> len(data[key]) == n_samples\n",
    "\n",
    "    Please note that this is the opposite convention to scikit-learn feature\n",
    "    matrixes (where the first index corresponds to sample).\n",
    "\n",
    "    ItemSelector only requires that the collection implement getitem\n",
    "    (data[key]).  Examples include: a dict of lists, 2D numpy array, Pandas\n",
    "    DataFrame, numpy record array, etc.\n",
    "\n",
    "    >> data = {'a': [1, 5, 2, 5, 2, 8],\n",
    "               'b': [9, 4, 1, 4, 1, 3]}\n",
    "    >> ds = ItemSelector(key='a')\n",
    "    >> data['a'] == ds.transform(data)\n",
    "\n",
    "    ItemSelector is not designed to handle data grouped by sample.  (e.g. a\n",
    "    list of dicts).  If your data is structured this way, consider a\n",
    "    transformer along the lines of `sklearn.feature_extraction.DictVectorizer`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    key : hashable, required\n",
    "        The key corresponding to the desired value in a mappable.\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "#         print(data_dict)\n",
    "        return data_dict[self.key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Rule5(article):\n",
    "    clf = joblib.load('linear_jargon_classifier.pkl') \n",
    "    \n",
    "    markup_list = []\n",
    "    \n",
    "    sentences = nltk.sent_tokenize(article)\n",
    "    words = nltk.word_tokenize(article)\n",
    "    ngram_list = []\n",
    "    print(sentences)\n",
    "    for sent in sentences:\n",
    "        word_list = sent.split()\n",
    "        filtered_words = [word.lower() for word in word_list] # if word.lower() not in stopwords.words('english')\n",
    "        print(filtered_words)\n",
    "        ngram_list.append([' '.join(x) for x in trigrams(filtered_words)])\n",
    "    ngram_list = [ngram for sublist in ngram_list for ngram in sublist]\n",
    "    print(ngram_list)\n",
    "    manual_list = [(ngram.lower(), True) for ngram in words]# if ngram not in punctuation]\n",
    "#     manual_list = [(\"viz a viz\", True), (\"capablesomething\",False), (\"bottomline\", True), (\"ibuprofin\", True), (\"uninterested\", False)]\n",
    "    prediction_results = test_manual_predictions(manual_list , clf)\n",
    "    \n",
    "    draft_article = article\n",
    "    for word_index in range(len(words)):\n",
    "        if prediction_results[word_index]:\n",
    "            index = draft_article.find(words[word_index])\n",
    "            markup_list.append((index, len(words[word_index])))\n",
    "        else:\n",
    "            continue\n",
    "        draft_article = draft_article[len(words[word_index]):]\n",
    "    \n",
    "    return markup_list\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The increasing time of your life.', 'Is that allons-y or would you bonjour rather kill me too viz-a-viz']\n",
      "['the', 'increasing', 'time', 'of', 'your', 'life.']\n",
      "['is', 'that', 'allons-y', 'or', 'would', 'you', 'bonjour', 'rather', 'kill', 'me', 'too', 'viz-a-viz']\n",
      "['the increasing time', 'increasing time of', 'time of your', 'of your life.', 'is that allons-y', 'that allons-y or', 'allons-y or would', 'or would you', 'would you bonjour', 'you bonjour rather', 'bonjour rather kill', 'rather kill me', 'kill me too', 'me too viz-a-viz']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(32, 1), (41, 8), (82, 9)]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rule5(\"The increasing time of your life. Is that allons-y or would you bonjour rather kill me too viz-a-viz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clf = joblib.load('linear_jargon_classifier.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
